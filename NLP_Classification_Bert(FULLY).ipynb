{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3GmgGVYy8Sjs"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"H0bv4zaKQfcu"},"source":["#improts/setup/1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18217,"status":"ok","timestamp":1692212418027,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"KatNQUAMQfcu","outputId":"66284c63-0b4f-4eaf-e21b-6cf5d20cff3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from scipy.sparse import hstack\n","from scipy.sparse import csr_matrix\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20667,"status":"ok","timestamp":1692212438509,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"u_u3f8LiH3Xn","outputId":"7cb6932c-4ec1-4694-dd78-2f626f3136d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["# Installing required libraries\n","!pip install transformers\n","# Importing required libraries\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, BertForSequenceClassification\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7m2dHrYMTYLp"},"outputs":[],"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, BertForSequenceClassification\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13610,"status":"ok","timestamp":1692212452114,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"AC5he6bpQfcv","outputId":"159905b9-11f0-476d-dd48-9d5d8be79a33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.11.3-py3-none-any.whl (225 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes>=0.10.0 (from optuna)\n","  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.3 cmaes-0.10.0 colorlog-6.7.0 optuna-3.3.0\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}],"source":["\n","!pip install optuna\n","!pip install contractions textblob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyQsA2TCQfcw"},"outputs":[],"source":["from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl41bqZLQfcw"},"outputs":[],"source":["\n","import contractions\n","import spacy\n","import re\n","import pandas as pd\n","import numpy as np\n","import torch\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.compose import ColumnTransformer\n","from transformers import BertTokenizer, BertModel\n","from textblob import TextBlob\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukNZH6DICd8n"},"outputs":[],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'test123.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"id\": dfid[\"id\"],\n","        \"target\": predictions\n","    })\n","\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBEHW_eyQfcw"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n","from sklearn.svm import SVR\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","import statsmodels.api as sm\n","import spacy\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","import pandas as pd\n","from datetime import datetime, timedelta\n","from sklearn.preprocessing import FunctionTransformer\n","from google.colab import files\n","from sklearn.feature_selection import RFE\n","from sklearn.base import BaseEstimator, TransformerMixin\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import StratifiedKFold\n","from textblob import TextBlob\n","from sklearn.model_selection import cross_val_score\n","import contractions\n","import plotly.express as px\n","import nltk\n","from sklearn.feature_extraction import DictVectorizer\n","import seaborn as sns\n","import gc\n","from sklearn.experimental import enable_halving_search_cv\n","from sklearn.pipeline import make_pipeline\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import train_test_split\n","import re\n","import optuna\n","from nltk.tokenize import word_tokenize\n","import pandas_datareader as pdr\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from optuna.integration import OptunaSearchCV\n","import numpy as np\n","from sklearn.pipeline import Pipeline\n","import warnings\n","# Filter warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import make_scorer\n","from sklearn.ensemble import RandomForestClassifier\n","import os\n","import yfinance as yf\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.metrics import classification_report\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","from sklearn.linear_model import Lasso, Ridge, ElasticNet\n","from google.colab import drive\n","import xgboost as xgb\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.cross_decomposition import PLSRegression\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1483,"status":"ok","timestamp":1692212458526,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"RLFuiWuFQfcx","outputId":"c6f8485f-d3b2-43f8-a574-d5f7c85ac287"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}],"source":["\n","\n","# Setting display options\n","pd.set_option('display.max_columns', None)\n","pd.options.display.float_format = '{:.2f}'.format\n","\n","# Reading data\n","folder_path = '/content/drive/MyDrive/datasets/'\n","dfTrain = pd.read_csv(folder_path + 'train123.csv')\n","dfTest = pd.read_csv(folder_path + 'test123.csv')\n","\n","# Filter warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Downloading NLTK resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gyf9gLr2T55t"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4giRzM7NtHJ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbyLdfcfITX0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"b8xFcoHjKt2a"},"source":["##stuffy stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RZRRCg2DWkn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import BertTokenizerFast, BertModel, AdamW\n","import numpy as np\n","from imblearn.over_sampling import RandomOverSampler\n","from collections import Counter\n","# specify GPU\n","device = torch.device(\"cuda\")\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n"]},{"cell_type":"code","source":["import copy"],"metadata":{"id":"fo9PT23YbBiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtZPp2kptSD_"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def combine(predictions, text_stats):\n","    # Ensure predictions is a numpy array\n","    if not isinstance(predictions, np.ndarray):\n","        predictions = np.array(predictions)\n","\n","    # Reshape predictions to 2D so they can be concatenated with 2D text_stats\n","    predictions = predictions.reshape(-1, 1)\n","\n","    # Convert numpy array to dataframe\n","    pred_df = pd.DataFrame(predictions, columns=['predictions'])\n","\n","    # Concatenate the dataframes along axis 1 (columns)\n","    result = pd.concat([pred_df, text_stats], axis=1)\n","\n","    return result\n","\n","def prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len=50, batch_size=32, random_state=42,test_size=0.3):\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      random_state=random_state,\n","                                                                      test_size=test_size,\n","                                                                      stratify=dfTrain['target'])\n","\n","    tokens_train = tokenizer.batch_encode_plus(train_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    tokens_val = tokenizer.batch_encode_plus(val_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","\n","    train_seq = torch.tensor(tokens_train['input_ids'])\n","    train_mask = torch.tensor(tokens_train['attention_mask'])\n","    train_y = torch.tensor(train_labels.tolist())\n","\n","    val_seq = torch.tensor(tokens_val['input_ids'])\n","    val_mask = torch.tensor(tokens_val['attention_mask'])\n","    val_y = torch.tensor(val_labels.tolist())\n","\n","    train_data = TensorDataset(train_seq, train_mask, train_y)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    val_data = TensorDataset(val_seq, val_mask, val_y)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, val_dataloader, val_y, train_y\n","\n","\n","def prepare_test_loader(dfTest, tokenizer, max_seq_len=50, batch_size=32):\n","    test_text = dfTest['text']\n","    tokens_test = tokenizer.batch_encode_plus(test_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","\n","    test_seq = torch.tensor(tokens_test['input_ids'])\n","    test_mask = torch.tensor(tokens_test['attention_mask'])\n","\n","    test_data = TensorDataset(test_seq, test_mask)\n","    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","\n","    return test_dataloader\n"]},{"cell_type":"code","source":["def prepare_train_val_test_loaders(dfTrain, tokenizer, max_seq_len=50, batch_size=32, random_state=42, train_size=0.7, val_to_test_ratio=0.4):\n","    # Splitting the data into training and temporary validation sets\n","    train_text, temp_val_text, train_labels, temp_val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                                random_state=random_state,\n","                                                                                test_size=1 - train_size,\n","                                                                                stratify=dfTrain['target'])\n","\n","    # Splitting the temporary validation set into final validation and test sets\n","    val_text, test_text, val_labels, test_labels = train_test_split(temp_val_text, temp_val_labels,\n","                                                                    test_size=val_to_test_ratio,\n","                                                                    random_state=random_state,\n","                                                                    stratify=temp_val_labels)\n","\n","    # Tokenizing\n","    tokens_train = tokenizer.batch_encode_plus(train_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    tokens_val = tokenizer.batch_encode_plus(val_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    tokens_test = tokenizer.batch_encode_plus(test_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","\n","    # Converting tokens into PyTorch tensors\n","    train_seq = torch.tensor(tokens_train['input_ids'])\n","    train_mask = torch.tensor(tokens_train['attention_mask'])\n","    train_y = torch.tensor(train_labels.tolist())\n","\n","    val_seq = torch.tensor(tokens_val['input_ids'])\n","    val_mask = torch.tensor(tokens_val['attention_mask'])\n","    val_y = torch.tensor(val_labels.tolist())\n","\n","    test_seq = torch.tensor(tokens_test['input_ids'])\n","    test_mask = torch.tensor(tokens_test['attention_mask'])\n","    test_y = torch.tensor(test_labels.tolist())\n","\n","    # Creating DataLoaders for training, validation, and test sets\n","    train_data = TensorDataset(train_seq, train_mask, train_y)\n","    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n","\n","    val_data = TensorDataset(val_seq, val_mask, val_y)\n","    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n","\n","    test_data = TensorDataset(test_seq, test_mask, test_y)\n","    test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n","\n","    return train_dataloader, val_dataloader, test_dataloader, val_y, test_y, train_y\n"],"metadata":{"id":"P4803kkKZaxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGJbUC5CZfs9"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import BertTokenizerFast, BertModel, AdamW\n","import numpy as np\n","from imblearn.over_sampling import RandomOverSampler\n","from collections import Counter\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","def prepare_data_loaders(dfTrain, dfTest, tokenizer, max_seq_len=50, batch_size=32, random_state=42):\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      random_state=random_state,\n","                                                                      test_size=0.30,\n","                                                                      stratify=dfTrain['target'])\n","\n","    test_text = dfTest['text']\n","\n","    tokens_train = tokenizer.batch_encode_plus(train_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    tokens_val = tokenizer.batch_encode_plus(val_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    tokens_test = tokenizer.batch_encode_plus(test_text.tolist(), max_length=max_seq_len, pad_to_max_length=True, truncation=True, return_token_type_ids=False)\n","    train_seq = torch.tensor(tokens_train['input_ids'])\n","    train_mask = torch.tensor(tokens_train['attention_mask'])\n","    train_y = torch.tensor(train_labels.tolist())\n","    val_seq = torch.tensor(tokens_val['input_ids'])\n","    val_mask = torch.tensor(tokens_val['attention_mask'])\n","    val_y = torch.tensor(val_labels.tolist())\n","    test_seq = torch.tensor(tokens_test['input_ids'])\n","    test_mask = torch.tensor(tokens_test['attention_mask'])\n","    train_data = TensorDataset(train_seq, train_mask, train_y)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    val_data = TensorDataset(val_seq, val_mask, val_y)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","        # Convert test sequences and attention masks to tensors\n","    test_data = TensorDataset(test_seq, test_mask)\n","    # Create a DataLoader for the test data\n","    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","\n","    return train_dataloader, val_dataloader, val_y, train_y,test_dataloader\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXabvjJsDWkp"},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      random_state=42,\n","                                                                      test_size=0.30,\n","                                                                      stratify=dfTrain['target'])\n","# Compute class weights\n","class_weights = compute_class_weight(\n","    class_weight=\"balanced\",\n","    classes=np.unique(train_labels),\n","    y=train_labels\n",")\n","\n","# Convert class weights to tensor\n","weights = torch.tensor(class_weights, dtype=torch.float)\n","weights = weights.to(device)\n","\n","# Loss function\n","cross_entropy = nn.NLLLoss(weight=weights)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["import optuna\n","from optuna.trial import TrialState\n","import optuna\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from transformers import AdamW\n","import torch\n","\n","\n","def predict(model, dataloader):\n","    model.eval()\n","    total_preds = []\n","    for step, batch in enumerate(dataloader):\n","        batch = [t.to(device) for t in batch]\n","        if len(batch) == 3:\n","            sent_id, mask, labels = batch\n","        else:\n","            sent_id, mask = batch\n","        with torch.no_grad():\n","            preds = model(sent_id, mask)\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    return np.argmax(total_preds, axis=1)\n"],"metadata":{"id":"jnafNfm9ZIqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cnW80Pe0Doh-"},"source":["#V1/uncased"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["d6c76eabfba94b5691509c7bd9b7bb25","619573801d5f483d8e3d8947926ee61d","8f91036e684a44579515b12b2c26dcc4","181e5944864c4eb8b14197ab970a2175","ef9059ccf28d44b8a82ea581852b7ed9","b4a5a8b904ab4a8da9e5d12c71b7b396","0ee8672513ce457cb43072dacc8f0cde","9da37ff7a24f48aa884a67a304658f82","fec1473236bf4201a3a7ea13132bbcb5","ed1971c7a32d46a899d1dfa90c4399d8","61cd79e6be1345b6beec1ce47b489556","fb2e9dbdf56b4c9d82bab104c73657af","7f49146d67a447afa61741c9cf2689de","c6d190b83271400ebdb03de157afd8dd","2b2977ce27a3423c98ffebe15ada9f85","69e8240fb2144daf899acd96890b8257","1e601e48953d449dad18425f296e3480","9dcb4e0be0b04f68a146277d6d6cdb5d","80150b07b2d644c3aa48cc8055edca4b","0d7db229b5c445769f98a506c1aa0f93","70a0605ec2d64bd7a5722eb8b9f30c91","885dee8f5d874c36997174379d442b8e","f283bbaa28824a40b1e2270f741df99a","0ccbcc9551e04e95b6de2b668c1c753b","4b62f362c0c243528aab06ee5c098b0a","96011023ee5a457f91c04df48171dccb","512dacf3a9e54a478a8e402749e7c5f6","b32f8074bc2841d8bdf07428baf676c9","010608b326364dc18a737338f0299bd1","0c0f7874c7af4654ad97e1cffceba4dd","cd3146c37b084ca794e76b6c8456d3b3","e46a2fbf626a483e8a4bce1f981b3167","85a1b04791c64d4db58f2fa926d4205f","1390ae759f604b67975ee16a745f5c79","6addcfb19e354d7badbddf26a862594a","e07f390a032c43669cc24c708dda19af","0c65cb47d4274466a369478f13167b07","01d9ff85dc1c4c3bb987bc301d913c4a","7ea3e2ef454148c1bb80daf778c8b85e","6c20c9e454bf45dd97bec822dda0c4b6","693ccb1c34f2432c9209f5003e84dad0","c613e026785c46e3b3c9f41d36a2620e","98175044d1864441a4f5d9ebe6924cdd","0001fa6a3d284650b4b52fa2d43a4d24","1acea491a86e457aa3f5ab9396c8de79","b31ac08ae9d8411eb79566eb9a85f56a","a3448ae203c5436d900f9d01c2bb1e83","b9aaf2fc0a76405f8631b37b05fc82f7","9eb2ba35db074cc6ae181c9011341d2d","370de771db6d4bbaa8129998afb47ef0","a925712b828241ccacef1e95b3337aba","4d769bcfde7e422a993484924347ddf0","7adcbde30082482397ce6f48829ad2ee","68fbf12126844d2f913fa28c608bb896","e0a0033fc35d4610a3d0731527ad2256"]},"executionInfo":{"elapsed":4622,"status":"ok","timestamp":1692212472271,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"TLwpiX2XWcqC","outputId":"d99af058-6406-453c-f192-1c334e672742"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c76eabfba94b5691509c7bd9b7bb25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2e9dbdf56b4c9d82bab104c73657af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f283bbaa28824a40b1e2270f741df99a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1390ae759f604b67975ee16a745f5c79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acea491a86e457aa3f5ab9396c8de79"}},"metadata":{}}],"source":["bert = BertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","class BERT_Arch(nn.Module):\n","    def __init__(self, bert, activation='relu', dropout=0.3, hidden_size=768, fc1_out=512, fc2_out=2):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        self.fc1 = nn.Linear(hidden_size, fc1_out)\n","        self.fc2 = nn.Linear(fc1_out, fc2_out)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, sent_id, mask):\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        x = self.fc1(cls_hs)\n","        x = self.activation(x)  # Use chosen activation function\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n"]},{"cell_type":"markdown","source":["#v1.5??"],"metadata":{"id":"2O5YIPTCi68H"}},{"cell_type":"code","source":["class BERT_Arch(nn.Module):\n","    def __init__(self, bert, activation='relu', dropout=0.3, hidden_size=768, fc1_out=512):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        self.fc1 = nn.Linear(hidden_size, fc1_out)\n","        self.fc2 = nn.Linear(fc1_out, 1)  # Output size changed to 1\n","\n","    def forward(self, sent_id, mask):\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        x = self.fc1(cls_hs)\n","        x = self.activation(x)  # Use chosen activation function\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x  # Removed softmax, x now has shape [batch_size, 1]\n"],"metadata":{"id":"D00V9f4xi80i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#V1/bigboybert"],"metadata":{"id":"9_mOGcAXyHA0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RP-tfonyKsJ"},"outputs":[],"source":["bert = BertModel.from_pretrained('bert-large-uncased')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased')\n","class BERT_Arch(nn.Module):\n","    def __init__(self, bert, activation='relu', dropout=0.3, hidden_size=1024, fc1_out=512, fc2_out=2):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        self.fc1 = nn.Linear(hidden_size, fc1_out)\n","        self.fc2 = nn.Linear(fc1_out, fc2_out)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, sent_id, mask):\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        x = self.fc1(cls_hs)\n","        x = self.activation(x)  # Use chosen activation function\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MbqFPP9pRTMK"},"source":["#V1/cased"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3yZz_a8RTMK"},"outputs":[],"source":["bert = BertModel.from_pretrained('bert-base-cased')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n","class BERT_Arch(nn.Module):\n","    def __init__(self, bert, activation='relu', dropout=0.3, hidden_size=768, fc1_out=512, fc2_out=2):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        self.fc1 = nn.Linear(hidden_size, fc1_out)\n","        self.fc2 = nn.Linear(fc1_out, fc2_out)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, sent_id, mask):\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        x = self.fc1(cls_hs)\n","        x = self.activation(x)  # Use chosen activation function\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6w4jzeV9DmnR"},"source":["#v2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pF1UexQYDroc"},"outputs":[],"source":["bert = BertModel.from_pretrained('bert-large-cased')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased')\n","\n","\n","class BERT_Arch(nn.Module):\n","    def __init__(self, bert, activation='relu', dropout=0.3, hidden_size=1024, fc1_out=512, fc2_out=2): # hidden_size updated to 1024\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert # The BERT model is passed as an argument\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        self.fc1 = nn.Linear(hidden_size, fc1_out)\n","        self.fc2 = nn.Linear(fc1_out, fc2_out)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, sent_id, mask):\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","        x = self.fc1(cls_hs)\n","        x = self.activation(x)  # Use chosen activation function\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n"]},{"cell_type":"markdown","source":["#mom said im cool, twice!"],"metadata":{"id":"X0RVWGp4yZaU"}},{"cell_type":"markdown","metadata":{"id":"lmHjvG4qg7zA"},"source":["##pipeline loss objective"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-m-AZLWcg4Hj"},"outputs":[],"source":["import optuna\n","from optuna.trial import TrialState\n","import optuna\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from transformers import AdamW\n","import torch\n","\n","\n","def predict(model, dataloader):\n","    model.eval()\n","    total_preds = []\n","    for step, batch in enumerate(dataloader):\n","        batch = [t.to(device) for t in batch]\n","        if len(batch) == 3:\n","            sent_id, mask, labels = batch\n","        else:\n","            sent_id, mask = batch\n","        with torch.no_grad():\n","            preds = model(sent_id, mask)\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    return np.argmax(total_preds, axis=1)\n","\n","\n","\n","def train(model, optimizer, train_dataloader, device, cross_entropy):\n","    separator = '~' * 140\n","    print(' ' * 60 + 'TRAINING HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(train_dataloader):\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        model.zero_grad()\n","        preds = model(sent_id, mask)\n","        loss = cross_entropy(preds, labels)\n","        total_loss = total_loss + loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        preds = preds.detach().cpu().numpy()\n","        total_preds.append(preds)\n","\n","        if (step + 1) % 2 == 0:  # Adjust the n value as needed\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    print(\"Training Metrics:\")\n","    print(f\"Average Loss: {avg_loss}\")\n","    print(separator)\n","    print()\n","\n","    return avg_loss, total_preds\n","\n","def evaluate(model, val_dataloader, device, cross_entropy):\n","    separator = '~' * 140\n","    print()\n","    print(' ' * 60 + 'EVALUATION HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.eval()\n","    total_loss = 0\n","    total_preds = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(val_dataloader):\n","        batch = [t.to(device) for t in batch]\n","        sent_id, mask, labels = batch\n","        with torch.no_grad():\n","            preds = model(sent_id, mask)\n","            loss = cross_entropy(preds, labels)\n","            total_loss = total_loss + loss.item()\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","\n","        if (step + 1) % 2 == 0:  # Adjust the n value as needed\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","    print(\"Evaluation Metrics:\")\n","    print(f\"Average Loss: {avg_loss}\")\n","    print(separator)\n","    print()\n","\n","    return avg_loss, total_preds\n","\n","\n","\n","\n","\n","def objective(trial, dfTrain, tokenizer, device, bert):\n","    model_params = {\n","        \"lr\": [3e-8, 1e-6],\n","        \"weight_decay\": [1e-4, 1e-3],\n","\n","        \"dropout\": [0.1, 0.5],\n","\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","\n","        #\"eps_optimizer\": [1e-09, 1e-07],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 2],\n","        #\"scheduler_eps\": [1e-09, 1e-07],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 1],\n","\n","\n","        \"min_lr\": [0, 1e-3],\n","\n","        \"max_seq_len\": [45, 55],  # Example bounds\n","        \"batch_size\": [15, 20],   # Example bounds\n","        \"epochs\": [7,8],\n","\n","        \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'], # Added activation type\n","        #relu',\n","    }\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      random_state=42,\n","                                                                      test_size=0.30,\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    cross_entropy = nn.NLLLoss(weight=weights)\n","\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    epochs=params['epochs']\n","    print(epochs)\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('==================================================')\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('==================================================')\n","    print()\n","\n","    for epoch in range(epochs):\n","        print('-----------------------')\n","        print('|Epoch {:} / {:}|'.format(epoch + 1, epochs))\n","\n","        print('-----------------------')\n","        train_loss, _ = train(model, optimizer, train_dataloader, device, cross_entropy)  # Passing cross_entropy\n","        valid_loss, _ = evaluate(model, val_dataloader, device, cross_entropy)  # Passing cross_entropy\n","\n","\n","        if train_loss > best_train_loss or valid_loss > best_valid_loss:\n","            print()\n","            print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n","\n","            print(\"Early stopping due to increase in training or validation loss\")\n","\n","            print(f'Training Loss: {train_loss:.3f}')\n","            print(f'Validation Loss: {valid_loss:.3f}')\n","\n","            print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n","\n","            print()\n","            print()\n","\n","            raise optuna.TrialPruned()  # Prune trial\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","            print('-----------------------')\n","        best_train_loss = min(train_loss, best_train_loss)\n","\n","\n","        print(f'Training Loss: {train_loss:.3f}')\n","        print(f'Validation Loss: {valid_loss:.3f}')\n","\n","\n","    print('*************************************************************************************************************************************')\n","    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n","    print('yayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaay!!!!!')\n","    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n","    print('*************************************************************************************************************************************')\n","\n","\n","    print()\n","\n","    # Predict validation values\n","\n","    val_predictions = predict(model, val_dataloader)\n","    train_predictions = predict(model, train_dataloader)\n","\n","\n","\n","    print('TRAIN')\n","    # Print classification report for validation predictions\n","    print(classification_report(train_y, train_predictions))\n","\n","\n","    accuracy = accuracy_score(train_y, train_predictions)\n","    print(\"EXACT ACCURACY: {:.2f}%\".format(accuracy * 100))\n","\n","\n","\n","    print('VALIDATION')\n","    print(classification_report(val_y, val_predictions))\n","\n","\n","    accuracy = accuracy_score(val_y, val_predictions)\n","    print(\"EXACT ACCURACY: {:.2f}%\".format(accuracy * 100))\n","\n","\n","\n","\n","    return (valid_loss-train_loss)**2 *(valid_loss**2+train_loss**2)*train_loss\n","\n","\n","\n","\n","\n","def tune_model_with_optuna(train_dataloader, val_dataloader, device, bert):\n","    study = optuna.create_study(direction=\"minimize\") # Changed to maximize\n","    study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device, bert), n_trials=10)\n","\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}') # This is now the accuracy\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    best_model = BERT_Arch(bert)\n","    best_model.load_state_dict(torch.load('saved_weights.pt'))\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f5H075oDhGyN"},"source":["##pipeline accuracy objective"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36iVr-FDhGyN"},"outputs":[],"source":["import optuna\n","from optuna.trial import TrialState\n","import optuna\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from transformers import AdamW\n","import torch\n","\n","\n","def predict(model, dataloader):\n","    model.eval()\n","    total_preds = []\n","    for step, batch in enumerate(dataloader):\n","        batch = [t.to(device) for t in batch]\n","        if len(batch) == 3:\n","            sent_id, mask, labels = batch\n","        else:\n","            sent_id, mask = batch\n","        with torch.no_grad():\n","            preds = model(sent_id, mask)\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    return np.argmax(total_preds, axis=1)\n","\n","\n","\n","def train(model, optimizer, train_dataloader, device, cross_entropy):\n","    separator = '~' * 140\n","    print(' ' * 60 + 'TRAINING HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(train_dataloader):\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        model.zero_grad()\n","        preds = model(sent_id, mask)\n","        loss = cross_entropy(preds, labels)\n","        total_loss = total_loss + loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        preds = preds.detach().cpu().numpy()\n","        total_preds.append(preds)\n","\n","        if (step + 1) % 2 == 0:  # Adjust the n value as needed\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","\n","\n","    return avg_loss, total_preds\n","\n","def evaluate(model, val_dataloader, device, cross_entropy):\n","    separator = '~' * 140\n","    print()\n","    print(' ' * 60 + 'EVALUATION HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.eval()\n","    total_loss = 0\n","    total_preds = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(val_dataloader):\n","        batch = [t.to(device) for t in batch]\n","        sent_id, mask, labels = batch\n","        with torch.no_grad():\n","            preds = model(sent_id, mask)\n","            loss = cross_entropy(preds, labels)\n","            total_loss = total_loss + loss.item()\n","            preds = preds.detach().cpu().numpy()\n","            total_preds.append(preds)\n","\n","        if (step + 1) % 2 == 0:  # Adjust the n value as needed\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","\n","\n","    return avg_loss, total_preds\n","\n","\n","\n","\n","\n","def objective(trial, dfTrain, tokenizer, device, bert):\n","    model_params = {\n","        \"lr\": [3e-7, 1e-5],\n","        \"weight_decay\": [1e-4, 1e-3],\n","        \"dropout\": [0.1, 0.5],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        #\"eps_optimizer\": [1e-09, 1e-07],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 2],\n","        #\"scheduler_eps\": [1e-09, 1e-07],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-3],\n","        \"max_seq_len\": [45, 55],  # Example bounds\n","        \"batch_size\": [15,20],   # Example bounds\n","        \"epochs\": [7,8],\n","\n","        \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'], # Added activation type\n","        #relu',\n","    }\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      random_state=42,\n","                                                                      test_size=0.30,\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    cross_entropy = nn.NLLLoss(weight=weights)\n","\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    epochs=params['epochs']\n","    print(epochs)\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('==================================================')\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('==================================================')\n","    print()\n","\n","    for epoch in range(epochs):\n","        print('-----------------------')\n","        print('|Epoch {:} / {:}|'.format(epoch + 1, epochs))\n","\n","        print('-----------------------')\n","        train_loss, _ = train(model, optimizer, train_dataloader, device, cross_entropy)  # Passing cross_entropy\n","        valid_loss, _ = evaluate(model, val_dataloader, device, cross_entropy)  # Passing cross_entropy\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), 'saved_weights.pt')\n","\n","            print('-----------------------')\n","        best_train_loss = min(train_loss, best_train_loss)\n","\n","\n","        print(f'Training Loss: {train_loss:.3f}')\n","        print(f'Validation Loss: {valid_loss:.3f}')\n","\n","    print('***************************************************************************************************************************************************')\n","    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n","    print('yayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaay!!!!!')\n","    print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n","    print('***************************************************************************************************************************************************')\n","    print()\n","\n","\n","\n","    # Predict validation values\n","    val_predictions = predict(model, val_dataloader)\n","    train_predictions = predict(model, train_dataloader)\n","\n","\n","\n","    print('TRAIN')\n","    # Print classification report for validation predictions\n","    print(classification_report(train_y, train_predictions))\n","\n","\n","    accuracy = accuracy_score(train_y, train_predictions)\n","    print(\"EXACT ACCURACY: {:.2f}%\".format(accuracy * 100))\n","\n","\n","\n","    print('VALIDATION')\n","    print(classification_report(val_y, val_predictions))\n","\n","\n","    accuracy = accuracy_score(val_y, val_predictions)\n","    print(\"EXACT ACCURACY: {:.2f}%\".format(accuracy * 100))\n","\n","\n","\n","\n","    return accuracy\n","\n","\n","\n","\n","\n","def tune_model_with_optuna_accuracy(train_dataloader, val_dataloader, device, bert):\n","    study = optuna.create_study(direction=\"maximize\") # Changed to maximize\n","    study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device, bert), n_trials=10)\n","\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}') # This is now the accuracy\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    best_model = BERT_Arch(bert)\n","    best_model.load_state_dict(torch.load('saved_weights.pt'))\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"W4dNxQtf2TUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#INF RUNS"],"metadata":{"id":"2bgg1hc5kYCG"}},{"cell_type":"markdown","source":["##train/eval"],"metadata":{"id":"VGv27khY581-"}},{"cell_type":"code","source":["def train(model, optimizer, train_dataloader, device, loss_function):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_train = []\n","    predicted_labels_train = []\n","    print(\"~\"*45)\n","    print(f'Training has started. Data Loader size={len(train_dataloader)}, step',end='=')\n","\n","    for step, batch in enumerate(train_dataloader):\n","\n","        sent_id, mask, labels = [r.to(device) for r in batch]\n","        model.zero_grad()\n","        predictions = model(sent_id, mask)\n","        loss = loss_function(predictions, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predicted_labels = torch.max(predictions, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        true_labels_train.extend(labels.cpu().numpy())\n","        predicted_labels_train.extend(predicted_labels.cpu().numpy())\n","        if (step) % 20 == 0:\n","            print(step,end='=>')\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    accuracy = correct_predictions / len(train_dataloader.dataset)\n","    print()\n","    print(\"~\"*45)\n","\n","    return avg_loss, accuracy, true_labels_train, predicted_labels_train\n","\n","def evaluate(model, val_dataloader, device, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_eval = []\n","    predicted_labels_eval = []\n","\n","    print(f'Evaluation has started. Data Loader size={len(val_dataloader)}, step',end='=')\n","    with torch.no_grad():\n","        for step, batch in enumerate(val_dataloader):\n","            sent_id, mask, labels = [t.to(device) for t in batch]\n","            predictions = model(sent_id, mask)\n","            loss = loss_function(predictions, labels)\n","            total_loss += loss.item()\n","\n","            _, predicted_labels = torch.max(predictions, 1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            true_labels_eval.extend(labels.cpu().numpy())\n","            predicted_labels_eval.extend(predicted_labels.cpu().numpy())\n","            if (step) % 20 == 0:\n","                print(step,end='=>')\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    accuracy = correct_predictions / len(val_dataloader.dataset)\n","    print()\n","    print(\"~\"*45)\n","    return avg_loss, accuracy, true_labels_eval, predicted_labels_eval\n"],"metadata":{"id":"2kd9XRhm5_AE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##inf trial loss"],"metadata":{"id":"v1H-isIVfgyk"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","def train(model, optimizer, train_dataloader, device, loss_function):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_train = []\n","    predicted_labels_train = []\n","    print(\"~\"*45)\n","    print(f'Training has started. Data Loader size={len(train_dataloader)}, step',end='=')\n","\n","    for step, batch in enumerate(train_dataloader):\n","\n","        sent_id, mask, labels = [r.to(device) for r in batch]\n","        model.zero_grad()\n","        predictions = model(sent_id, mask)\n","        loss = loss_function(predictions, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predicted_labels = torch.max(predictions, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        true_labels_train.extend(labels.cpu().numpy())\n","        predicted_labels_train.extend(predicted_labels.cpu().numpy())\n","        if (step) % 20 == 0:\n","            print(step,end='=>')\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    accuracy = correct_predictions / len(train_dataloader.dataset)\n","    print()\n","    print(\"~\"*45)\n","\n","    return avg_loss, accuracy, true_labels_train, predicted_labels_train\n","\n","def evaluate(model, val_dataloader, device, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_eval = []\n","    predicted_labels_eval = []\n","\n","    print(f'Evaluation has started. Data Loader size={len(val_dataloader)}, step',end='=')\n","    with torch.no_grad():\n","        for step, batch in enumerate(val_dataloader):\n","            sent_id, mask, labels = [t.to(device) for t in batch]\n","            predictions = model(sent_id, mask)\n","            loss = loss_function(predictions, labels)\n","            total_loss += loss.item()\n","\n","            _, predicted_labels = torch.max(predictions, 1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            true_labels_eval.extend(labels.cpu().numpy())\n","            predicted_labels_eval.extend(predicted_labels.cpu().numpy())\n","            if (step) % 20 == 0:\n","                print(step,end='=>')\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    accuracy = correct_predictions / len(val_dataloader.dataset)\n","    print()\n","    print(\"~\"*45)\n","    return avg_loss, accuracy, true_labels_eval, predicted_labels_eval\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","        \"lr\": [5e-8, 5e-6],\n","        \"weight_decay\": [1e-5, 1e-3],\n","\n","\n","        \"activation\": [ 'leaky_relu','sigmoid', 'tanh', 'gelu'],\n","        #\"activation\": ['leaky_relu'],\n","        \"dropout\": [0.05, 0.1],\n","\n","        'train_size': [0.50, 0.51],\n","\n","\n","\n","        \"scheduler_patience\": [1, 3],\n","        \"cooldown\": [0, 1],\n","        \"threshold\": [1e-5, 1e-4],\n","\n","\n","        \"max_seq_len\": [45, 55],\n","        \"batch_size\": [10, 15],\n","\n","        'random_state': [42, 1024],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.01, 0.3],\n","        'test_size': [0.4, 0.41],\n","\n","        \"min_lr\": [0, 1e-9],\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print('='*45)\n","    k=0\n","    for param, value in params.items():\n","        k+=1\n","        print(f'{param:<20} = {value}')\n","        if k==2:\n","          print('='*45)\n","        if k==4:\n","          print('='*45)\n","        if k==5:\n","          print(f'{\"val_size\":<20} = {(1-params[\"test_size\"])*(1-params[\"train_size\"])}')\n","          print(f'{\"actual_test_size\":<20} = {(params[\"test_size\"])*(1-params[\"train_size\"])}')\n","          print('='*45)\n","        if k==8:\n","          print('='*45)\n","        if k==10:\n","          break\n","    #train_labels, val_labels = train_test_split(dfTrain['target'], test_size=1- params[\"train_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['target'])\n","    train_labels, val_labels = train_test_split(dfTrain['target'], test_size=1- params[\"train_size\"],random_state=params[\"random_state\"],stratify=dfTrain['target'])\n","\n","    test_dataloader=None\n","    train_dataloader, val_dataloader, test_dataloader, val_y, test_y, train_y = prepare_train_val_test_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"],train_size=params[\"train_size\"], val_to_test_ratio=params[\"test_size\"])\n","    #train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"], test_size=params[\"test_size\"])\n","\n","    #all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","\n","    loss_function = nn.CrossEntropyLoss(weight=weights)#best one so far\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        if test_dataloader is not None:\n","            test_preds = predict(model, test_dataloader)\n","            test_accuracy=accuracy_score(test_y, test_preds)\n","            print('-' * 105); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% / {100 * test_accuracy:.4f}% |'); print('-' * 105);\n","\n","        else:\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100);\n","        #metric = valid_loss\n","        metric=(valid_loss-train_loss)**2 *(valid_loss**2+train_loss**2)*train_loss\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print('~'*26);print(f'|Best Value Found {best_local_metric:.5f}|');print('~'*26); print(f'Params = {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54);\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(test_y, test_preds))\n","                    print('~' * 54)\n","\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print('>'*35); print('OH NO WE BE OVERFITTING'); print('<'*35);\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1692215861736,"user_tz":360,"elapsed":1132070,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"582288ba-a141-4ccd-acf4-237d7a18c96e","id":"mQ405i8yfgyt"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-16 19:38:49,594] A new study created in memory with name: no-name-f6119683-b2ad-43e3-b4c9-a2886688d21c\n"]},{"output_type":"stream","name":"stdout","text":["=============================================\n","lr                   = 1.7634535423582268e-07\n","weight_decay         = 9.342061384510143e-05\n","=============================================\n","activation           = leaky_relu\n","dropout              = 0.05028629906977902\n","=============================================\n","train_size           = 0.5087490045125537\n","val_size             = 0.2933803714343822\n","actual_test_size     = 0.1978706240530641\n","=============================================\n","scheduler_patience   = 3\n","cooldown             = 1\n","threshold            = 1.1763206163965135e-05\n","=============================================\n","max_seq_len          = 48\n","batch_size           = 11\n","=============================================\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 1 | Loss Train/Valid: 0.700405 / 0.631683 | Accuracy Train/Valid: 52.233% / 84.8634% / 83.0790% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00294|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.56      0.76      0.64      2209\n","           1       0.40      0.21      0.28      1664\n","\n","    accuracy                           0.52      3873\n","   macro avg       0.48      0.48      0.46      3873\n","weighted avg       0.49      0.52      0.49      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.95      0.88      1274\n","           1       0.91      0.72      0.80       959\n","\n","    accuracy                           0.85      2233\n","   macro avg       0.86      0.83      0.84      2233\n","weighted avg       0.86      0.85      0.85      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.94      0.86       859\n","           1       0.90      0.68      0.78       648\n","\n","    accuracy                           0.83      1507\n","   macro avg       0.85      0.81      0.82      1507\n","weighted avg       0.84      0.83      0.83      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 2 | Loss Train/Valid: 0.590209 / 0.535494 | Accuracy Train/Valid: 85.334% / 89.2969% / 87.9230% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00112|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.94      0.88      2209\n","           1       0.90      0.74      0.81      1664\n","\n","    accuracy                           0.85      3873\n","   macro avg       0.86      0.84      0.85      3873\n","weighted avg       0.86      0.85      0.85      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.93      0.91      1274\n","           1       0.90      0.84      0.87       959\n","\n","    accuracy                           0.89      2233\n","   macro avg       0.89      0.89      0.89      2233\n","weighted avg       0.89      0.89      0.89      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.93      0.90       859\n","           1       0.90      0.81      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.88      0.87      0.88      1507\n","weighted avg       0.88      0.88      0.88      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 3 | Loss Train/Valid: 0.513251 / 0.468287 | Accuracy Train/Valid: 87.168% / 88.8043% / 87.8567% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00050|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.93      0.89      2209\n","           1       0.89      0.80      0.84      1664\n","\n","    accuracy                           0.87      3873\n","   macro avg       0.88      0.86      0.87      3873\n","weighted avg       0.87      0.87      0.87      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.91      0.90      1274\n","           1       0.88      0.86      0.87       959\n","\n","    accuracy                           0.89      2233\n","   macro avg       0.89      0.88      0.89      2233\n","weighted avg       0.89      0.89      0.89      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.92      0.90       859\n","           1       0.89      0.82      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.88      0.87      0.87      1507\n","weighted avg       0.88      0.88      0.88      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 4 | Loss Train/Valid: 0.456654 / 0.422281 | Accuracy Train/Valid: 87.891% / 88.2221% / 87.6576% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00021|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.92      0.90      2209\n","           1       0.88      0.83      0.85      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.88      0.87      0.88      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.90      0.90      1274\n","           1       0.87      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.89       859\n","           1       0.87      0.83      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.88      0.87      0.87      1507\n","weighted avg       0.88      0.88      0.88      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 5 | Loss Train/Valid: 0.418920 / 0.392117 | Accuracy Train/Valid: 87.736% / 88.0430% / 87.5249% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00010|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.89      2209\n","           1       0.87      0.84      0.85      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.88      0.87      0.87      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.90      1274\n","           1       0.86      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.87      0.84      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.88      0.88      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 6 | Loss Train/Valid: 0.391710 / 0.371404 | Accuracy Train/Valid: 88.020% / 87.9534% / 87.1931% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00005|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.90      2209\n","           1       0.88      0.84      0.86      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.88      0.88      0.88      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.89      1274\n","           1       0.86      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.86      0.84      0.85       648\n","\n","    accuracy                           0.87      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.87      0.87      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 7 | Loss Train/Valid: 0.372650 / 0.358281 | Accuracy Train/Valid: 88.020% / 87.9086% / 87.1931% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00002|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.90      2209\n","           1       0.88      0.84      0.86      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.88      0.88      0.88      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.89      1274\n","           1       0.86      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.86      0.84      0.85       648\n","\n","    accuracy                           0.87      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.87      0.87      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 8 | Loss Train/Valid: 0.366891 / 0.346927 | Accuracy Train/Valid: 87.813% / 88.4908% / 87.7240% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 9 | Loss Train/Valid: 0.352665 / 0.341406 | Accuracy Train/Valid: 88.381% / 88.3565% / 87.6576% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00001|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.91      0.90      2209\n","           1       0.88      0.85      0.86      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.88      0.88      0.88      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.90      0.90      1274\n","           1       0.87      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.89       859\n","           1       0.87      0.83      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.88      0.87      0.87      1507\n","weighted avg       0.88      0.88      0.88      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 10 | Loss Train/Valid: 0.349114 / 0.337205 | Accuracy Train/Valid: 88.536% / 88.3565% / 87.7240% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 11 | Loss Train/Valid: 0.341470 / 0.336772 | Accuracy Train/Valid: 88.484% / 87.9086% / 87.5249% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00000|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.92      0.90      2209\n","           1       0.89      0.84      0.86      1664\n","\n","    accuracy                           0.88      3873\n","   macro avg       0.89      0.88      0.88      3873\n","weighted avg       0.88      0.88      0.88      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.89      1274\n","           1       0.86      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.87      0.84      0.85       648\n","\n","    accuracy                           0.88      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.88      0.88      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 12 | Loss Train/Valid: 0.339992 / 0.334998 | Accuracy Train/Valid: 88.562% / 87.9982% / 87.4585% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 13 | Loss Train/Valid: 0.335336 / 0.330520 | Accuracy Train/Valid: 88.897% / 88.2669% / 87.3922% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00000|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.92      0.90      2209\n","           1       0.89      0.84      0.87      1664\n","\n","    accuracy                           0.89      3873\n","   macro avg       0.89      0.88      0.89      3873\n","weighted avg       0.89      0.89      0.89      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.90      0.90      1274\n","           1       0.87      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.91      0.89       859\n","           1       0.87      0.83      0.85       648\n","\n","    accuracy                           0.87      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.87      0.87      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 14 | Loss Train/Valid: 0.333424 / 0.330959 | Accuracy Train/Valid: 88.923% / 88.1773% / 87.2595% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00000|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.93      0.91      2209\n","           1       0.90      0.84      0.87      1664\n","\n","    accuracy                           0.89      3873\n","   macro avg       0.89      0.88      0.89      3873\n","weighted avg       0.89      0.89      0.89      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.90      1274\n","           1       0.86      0.86      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.87      0.83      0.85       648\n","\n","    accuracy                           0.87      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.87      0.87      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 15 | Loss Train/Valid: 0.329031 / 0.330982 | Accuracy Train/Valid: 88.846% / 88.2221% / 87.3258% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 0.00000|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 1.7634535423582268e-07, 'weight_decay': 9.342061384510143e-05, 'activation': 'leaky_relu', 'dropout': 0.05028629906977902, 'train_size': 0.5087490045125537, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 1.1763206163965135e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 254, 'beta1': 0.9593666329513473, 'beta2': 0.9320818702029076, 'factor': 0.2340410338886762, 'test_size': 0.40278925818099554, 'min_lr': 0}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.92      0.90      2209\n","           1       0.89      0.85      0.87      1664\n","\n","    accuracy                           0.89      3873\n","   macro avg       0.89      0.88      0.89      3873\n","weighted avg       0.89      0.89      0.89      3873\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.89      0.90      1274\n","           1       0.86      0.87      0.86       959\n","\n","    accuracy                           0.88      2233\n","   macro avg       0.88      0.88      0.88      2233\n","weighted avg       0.88      0.88      0.88      2233\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       859\n","           1       0.87      0.83      0.85       648\n","\n","    accuracy                           0.87      1507\n","   macro avg       0.87      0.87      0.87      1507\n","weighted avg       0.87      0.87      0.87      1507\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 16 | Loss Train/Valid: 0.324657 / 0.333025 | Accuracy Train/Valid: 89.466% / 88.0878% / 87.2595% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=353, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=203, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-16 19:55:08,640] Trial 0 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------------------------------------------------------------\n","| Epoch 17 | Loss Train/Valid: 0.317241 / 0.332540 | Accuracy Train/Valid: 89.620% / 88.1773% / 87.3258% |\n","---------------------------------------------------------------------------------------------------------\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","OH NO WE BE OVERFITTING\n","<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","=============================================\n","lr                   = 7.125221352398662e-07\n","weight_decay         = 3.932553531619539e-05\n","=============================================\n","activation           = gelu\n","dropout              = 0.05199621721510476\n","=============================================\n","train_size           = 0.5046255868203666\n","val_size             = 0.296607888703394\n","actual_test_size     = 0.19876652447623935\n","=============================================\n","scheduler_patience   = 3\n","cooldown             = 1\n","threshold            = 3.344771203060101e-05\n","=============================================\n","max_seq_len          = 48\n","batch_size           = 11\n","=============================================\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=350, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=206, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 1 | Loss Train/Valid: 0.558897 / 0.410457 | Accuracy Train/Valid: 79.953% / 89.7254% / 88.0449% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=350, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=206, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","---------------------------------------------------------------------------------------------------------\n","| Epoch 2 | Loss Train/Valid: 0.379469 / 0.329634 | Accuracy Train/Valid: 88.623% / 89.7254% / 88.0449% |\n","---------------------------------------------------------------------------------------------------------\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training has started. Data Loader size=350, step=0=>20=>40=>60=>80=>100=>120=>140=>160=>180=>200=>220=>240=>260=>280=>300=>320=>340=>\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Evaluation has started. Data Loader size=206, step=0=>"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-16 19:57:41,415] Trial 1 failed with parameters: {'lr': 7.125221352398662e-07, 'weight_decay': 3.932553531619539e-05, 'activation': 'gelu', 'dropout': 0.05199621721510476, 'train_size': 0.5046255868203666, 'scheduler_patience': 3, 'cooldown': 1, 'threshold': 3.344771203060101e-05, 'max_seq_len': 48, 'batch_size': 11, 'random_state': 273, 'beta1': 0.94526145541389, 'beta2': 0.9927155996499561, 'factor': 0.019808425155866848, 'test_size': 0.40124503645722687, 'min_lr': 0} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-24-49ed366fe264>\", line 161, in <lambda>\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","  File \"<ipython-input-24-49ed366fe264>\", line 123, in objective_loss\n","    valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","  File \"<ipython-input-19-0ab04ee7fe37>\", line 47, in evaluate\n","    total_loss += loss.item()\n","KeyboardInterrupt\n","[W 2023-08-16 19:57:41,417] Trial 1 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-49ed366fe264>\u001b[0m in \u001b[0;36m<cell line: 177>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna_inf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-49ed366fe264>\u001b[0m in \u001b[0;36mtune_model_with_optuna_inf_loss\u001b[0;34m(train_dataloader, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-49ed366fe264>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-49ed366fe264>\u001b[0m in \u001b[0;36mobjective_loss\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-0ab04ee7fe37>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, val_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["tuned_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","tuned_model.load_state_dict(best_state_dict)\n","tuned_model = tuned_model.to(device)\n","# Prepare dataloaders\n","\n","train_dataloader, val_dataloader, val_y, train_y,test_dataloader = prepare_data_loaders(dfTrain, dfTest, tokenizer,random_state=(best_params[\"random_state\"]+random.randint(0, 100)))\n","\n","val_predictions = predict(tuned_model, val_dataloader)\n","print('='*40)\n","print(classification_report(val_y, val_predictions))\n","for param, value in best_params.items():\n","    print(f'{param:<20} = {value}')\n","\n","print('='*40); print()\n","\n","print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692214620216,"user_tz":360,"elapsed":6741,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"df0a2987-73c4-4f7b-977a-1216838e4b85","id":"YvH9DKQHFpVB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["========================================\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.93      0.90      1303\n","           1       0.89      0.83      0.86       981\n","\n","    accuracy                           0.88      2284\n","   macro avg       0.89      0.88      0.88      2284\n","weighted avg       0.88      0.88      0.88      2284\n","\n","lr                   = 2.355839853203695e-07\n","weight_decay         = 0.0002505904617679392\n","activation           = sigmoid\n","dropout              = 0.10561698343229392\n","train_size           = 0.5025846342299304\n","scheduler_patience   = 2\n","cooldown             = 1\n","threshold            = 3.0828121690288e-05\n","max_seq_len          = 53\n","batch_size           = 15\n","random_state         = 916\n","beta1                = 0.9092180875835741\n","beta2                = 0.9222786342012999\n","factor               = 0.27941832730191557\n","test_size            = 0.40999024684484664\n","min_lr               = 0\n","========================================\n","\n","{'lr': 2.355839853203695e-07, 'weight_decay': 0.0002505904617679392, 'activation': 'sigmoid', 'dropout': 0.10561698343229392, 'train_size': 0.5025846342299304, 'scheduler_patience': 2, 'cooldown': 1, 'threshold': 3.0828121690288e-05, 'max_seq_len': 53, 'batch_size': 15, 'random_state': 916, 'beta1': 0.9092180875835741, 'beta2': 0.9222786342012999, 'factor': 0.27941832730191557, 'test_size': 0.40999024684484664, 'min_lr': 0}\n"]}]},{"cell_type":"code","source":["test_dataloader=prepare_test_loader(dfTest, tokenizer, max_seq_len=51, batch_size=10)\n","test_predictions = predict(tuned_model, test_dataloader)\n","create_and_download_submission(test_predictions, '123123123123123123submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"wygBNaCDF9Bp","executionInfo":{"status":"ok","timestamp":1692214633815,"user_tz":360,"elapsed":9358,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"0149b6f8-a039-457f-b39f-3cf999fce001"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e11e9456-ff9f-43e3-866f-e290126f6bc6\", \"123123123123123123submission.csv\", 22746)"]},"metadata":{}}]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"],"metadata":{"id":"NrO-N3nufgyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS3FTiKffgyt"},"outputs":[],"source":["tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","source":["best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","best_model.load_state_dict(best_state_dict)\n","best_model = best_model.to(device)\n","# Prepare dataloaders\n","\n","train_dataloader, val_dataloader, val_y, train_y,test_dataloader = prepare_data_loaders(dfTrain, dfTest, tokenizer)\n","\n","val_predictions = predict(tuned_model, val_dataloader)\n","\n","# Print classification report for validation predictions\n","print(classification_report(val_y, val_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692141773014,"user_tz":360,"elapsed":2811,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"b918ec9f-e653-4e5a-a346-6416da9cfb0f","id":"wZcC2_e5kj7M"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.91      0.99      0.94      1303\n","           1       0.98      0.86      0.92       981\n","\n","    accuracy                           0.93      2284\n","   macro avg       0.94      0.93      0.93      2284\n","weighted avg       0.94      0.93      0.93      2284\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":2794,"status":"ok","timestamp":1692141780994,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"outputId":"813387b7-9f0b-4eac-c6d7-bbc6f4ba171a","id":"G4kL0KK6kj7N"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_592d1606-c2bd-49f4-8c4b-1961dd8f7f88\", \"submission.csv\", 22746)"]},"metadata":{}}],"source":["test_predictions = predict(tuned_model, test_dataloader)\n","create_and_download_submission(test_predictions, 'submission.csv')"]},{"cell_type":"markdown","source":["##inf trial prune=stop/accuracy"],"metadata":{"id":"4dTYQ0FEpWOP"}},{"cell_type":"code","source":["def objective_accuracy(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","    \"lr\": [5e-9, 1e-8],  # Lower range\n","    \"weight_decay\": [1e-5, 1e-3],  # Higher range\n","    \"dropout\": [0.4, 0.6],  # Higher range\n","    \"beta1\": [0.8, 0.99],  # Unchanged\n","    \"beta2\": [0.9, 0.9999], # Unchanged\n","    \"factor\": [0.01, 0.3], # Lower upper bound\n","    \"scheduler_patience\": [1,2], # Higher upper bound\n","    \"threshold\": [1e-5, 1e-4], # Higher range\n","    \"cooldown\": [0, 1],  # Unchanged\n","    \"min_lr\": [0, 1e-9], # Unchanged\n","    \"max_seq_len\": [45, 55],  # Unchanged\n","    \"batch_size\": [10, 15], # Higher range\n","    \"activation\": ['leaky_relu'],  # Unchanged\n","    'test_size': [0.3, 0.5],\n","    'train_size': [0.5, 0.7],# Higher upper bound\n","    'random_state': [42, 1024], # Unchanged\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print(); print('='*40)\n","    for param, value in params.items():\n","        print(f'{param:<20} = {value}')\n","    print('='*40); print()\n","##################################################################################################################################################################################################################################\n","    train_labels, val_labels = train_test_split( dfTrain['target'],\n","                                                test_size=params[\"test_size\"],\n","                                                random_state=params[\"random_state\"]+random.randint(0,300),\n","                                                 stratify=dfTrain['target'])\n","    test_dataloader=None\n","    #train_dataloader, val_dataloader, test_dataloader, val_y, test_y, train_y = prepare_train_val_test_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"],train_size=params[\"train_size\"], val_to_test_ratio=params[\"test_size\"])\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"], test_size=params[\"test_size\"])\n","\n","\n","    all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels = train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    #loss_function = nn.BCEWithLogitsLoss(weight=weights)\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)#best one so far\n","\n","    #loss_function = nn.NLLLoss()\n","    #loss_function = nn.CrossEntropyLoss()\n","    #loss_function = nn.BCEWithLogitsLoss()\n","\n","##################################################################################################################################################################################################################################\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","##################################################################################################################################################################################################################################\n","\n","    best_local_metric = 0\n","    separator = '=' * 180\n","    valid_losses = []\n","    metric = 0\n","    no_improvement_counter = 0\n","    for epoch in range(100000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        print(f'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100); print()\n","        metric = val_accuracy\n","        if metric > best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if best_local_metric > best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print(); print(f'| Best Value Found {100*best_local_metric:.5f}% |'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54)\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(test_y, test_preds))\n","                    print('~' * 54)\n","                    print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","\n","    return metric\n"],"metadata":{"id":"onW4ujpT8DEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","\n","def tune_model_with_optuna_inf_accuracy(train_dataloader, val_dataloader, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"maximize\") # Changed to maximize\n","    study.optimize(lambda trial: objective_accuracy(trial, train_dataloader, val_dataloader, device, bert), n_trials=2)\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_accuracy(dfTrain, tokenizer, device, bert)\n"],"metadata":{"executionInfo":{"status":"error","timestamp":1692140866840,"user_tz":360,"elapsed":109743,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Non2VeDqpX_i","outputId":"307060b7-cbdc-486a-fadb-efaab809c53e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-15 23:05:55,785] A new study created in memory with name: no-name-cb1a693b-721c-477d-9861-0d09f4bcba43\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================\n","lr                   = 5.1739432262398406e-09\n","weight_decay         = 1.1302228628382107e-05\n","dropout              = 0.44581824868679987\n","beta1                = 0.801514035943113\n","beta2                = 0.9973038003006123\n","factor               = 0.21373914604754643\n","scheduler_patience   = 2\n","threshold            = 1.5358019175191363e-05\n","cooldown             = 1\n","min_lr               = 0\n","max_seq_len          = 54\n","batch_size           = 12\n","activation           = leaky_relu\n","test_size            = 0.3156359023334788\n","train_size           = 0.6236954986881017\n","random_state         = 946\n","========================================\n","\n","| Epoch 1 | Loss Train/Valid:     0.702037 / 0.698747 | Accuracy Train/Valid: 45.739% / 42.6134% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 42.61340% |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.34      0.42      2971\n","           1       0.41      0.61      0.49      2239\n","\n","    accuracy                           0.46      5210\n","   macro avg       0.47      0.48      0.45      5210\n","weighted avg       0.48      0.46      0.45      5210\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.47      0.05      0.09      1371\n","           1       0.42      0.93      0.58      1032\n","\n","    accuracy                           0.43      2403\n","   macro avg       0.45      0.49      0.33      2403\n","weighted avg       0.45      0.43      0.30      2403\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","| Epoch 2 | Loss Train/Valid:     0.699013 / 0.698471 | Accuracy Train/Valid: 46.891% / 42.7799% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 42.77986% |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.37      0.44      2971\n","           1       0.42      0.61      0.49      2239\n","\n","    accuracy                           0.47      5210\n","   macro avg       0.49      0.49      0.47      5210\n","weighted avg       0.49      0.47      0.46      5210\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.49      0.05      0.09      1371\n","           1       0.42      0.93      0.58      1032\n","\n","    accuracy                           0.43      2403\n","   macro avg       0.45      0.49      0.34      2403\n","weighted avg       0.46      0.43      0.30      2403\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","| Epoch 3 | Loss Train/Valid:     0.699348 / 0.698195 | Accuracy Train/Valid: 46.219% / 43.1544% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 43.15439% |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.35      0.42      2971\n","           1       0.42      0.62      0.50      2239\n","\n","    accuracy                           0.46      5210\n","   macro avg       0.48      0.48      0.46      5210\n","weighted avg       0.49      0.46      0.45      5210\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.05      0.09      1371\n","           1       0.43      0.94      0.59      1032\n","\n","    accuracy                           0.43      2403\n","   macro avg       0.47      0.49      0.34      2403\n","weighted avg       0.48      0.43      0.30      2403\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","| Epoch 4 | Loss Train/Valid:     0.700467 / 0.697928 | Accuracy Train/Valid: 47.044% / 43.6122% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 43.61215% |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.37      0.44      2971\n","           1       0.42      0.60      0.49      2239\n","\n","    accuracy                           0.47      5210\n","   macro avg       0.49      0.49      0.47      5210\n","weighted avg       0.50      0.47      0.47      5210\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.56      0.05      0.10      1371\n","           1       0.43      0.95      0.59      1032\n","\n","    accuracy                           0.44      2403\n","   macro avg       0.50      0.50      0.34      2403\n","weighted avg       0.51      0.44      0.31      2403\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-15 23:07:45,485] Trial 0 failed with parameters: {'lr': 5.1739432262398406e-09, 'weight_decay': 1.1302228628382107e-05, 'dropout': 0.44581824868679987, 'beta1': 0.801514035943113, 'beta2': 0.9973038003006123, 'factor': 0.21373914604754643, 'scheduler_patience': 2, 'threshold': 1.5358019175191363e-05, 'cooldown': 1, 'min_lr': 0, 'max_seq_len': 54, 'batch_size': 12, 'activation': 'leaky_relu', 'test_size': 0.3156359023334788, 'train_size': 0.6236954986881017, 'random_state': 946} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-38-55a898c4ad0e>\", line 8, in <lambda>\n","    study.optimize(lambda trial: objective_accuracy(trial, train_dataloader, val_dataloader, device, bert), n_trials=2)\n","  File \"<ipython-input-37-c0ec07d9b995>\", line 72, in objective_accuracy\n","    train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","  File \"<ipython-input-25-c44c314b8aa9>\", line 14, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","[W 2023-08-15 23:07:45,487] Trial 0 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-55a898c4ad0e>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna_inf_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-55a898c4ad0e>\u001b[0m in \u001b[0;36mtune_model_with_optuna_inf_accuracy\u001b[0;34m(train_dataloader, val_dataloader, device, bert)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed to maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     best_model = BERT_Arch(\n\u001b[1;32m     10\u001b[0m         \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-55a898c4ad0e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed to maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     best_model = BERT_Arch(\n\u001b[1;32m     10\u001b[0m         \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-c0ec07d9b995>\u001b[0m in \u001b[0;36mobjective_accuracy\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mno_improvement_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-c44c314b8aa9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeHvMw49APxY","executionInfo":{"status":"ok","timestamp":1692049413026,"user_tz":360,"elapsed":4,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"60b7eca7-3edc-4894-e57c-8ab8ddba19ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'lr': 8.080887535626783e-09, 'weight_decay': 3.535542596809382e-05, 'dropout': 0.4628719151990931, 'beta1': 0.8790330454304485, 'beta2': 0.9352292228260416, 'factor': 0.0798848902008787, 'scheduler_patience': 2, 'threshold': 1.070002753425782e-05, 'cooldown': 1, 'min_lr': 0, 'max_seq_len': 45, 'batch_size': 10, 'activation': 'leaky_relu', 'test_size': 0.43472272912781407, 'random_state': 78}\n"]}]},{"cell_type":"markdown","source":["#INF+EXTRA STUFF"],"metadata":{"id":"GntzCaH6kUxF"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","        \"lr\": [5e-8, 5e-6],\n","        \"weight_decay\": [1e-5, 1e-3],\n","\n","\n","        \"dropout\": [0.2, 0.3],\n","        'train_size': [0.5, 0.7],\n","        'test_size': [0.2, 0.3],\n","\n","\n","        \"scheduler_patience\": [1, 3],\n","\n","        \"cooldown\": [0, 1],\n","        \"threshold\": [1e-5, 1e-4],\n","        \"min_lr\": [0, 1e-9],\n","\n","\n","\n","        \"activation\": ['leaky_relu'],\n","        'random_state': [42, 1024],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.01, 0.3],\n","\n","        \"max_seq_len\": [45, 55],\n","        \"batch_size\": [20, 25],\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print('='*45)\n","    for param, value in params.items():\n","        print(f'{param:<20} = {value}')\n","    print('='*45);\n","##################################################################################################################################################################################################################################\n","\n","    #train_labels, val_labels = train_test_split(dfTrain['target'], test_size=params[\"test_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['target'])\n","    train_labels, val_labels = train_test_split(dfTrain['target'], test_size=params[\"test_size\"],random_state=params[\"random_state\"],stratify=dfTrain['target'])\n","\n","    test_dataloader=None\n","    train_dataloader, val_dataloader, test_dataloader, val_y, test_y, train_y = prepare_train_val_test_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"],train_size=params[\"train_size\"], val_to_test_ratio=params[\"test_size\"])\n","    #train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"], test_size=params[\"test_size\"])\n","\n","    #all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    #loss_function = nn.BCEWithLogitsLoss(weight=weights)\n","    #loss_function = nn.BCELoss(weight=weights)\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)#best one so far\n","\n","\n","    #loss_function = nn.BCELoss()\n","    #loss_function = nn.NLLLoss()\n","    #loss_function = nn.BCEWithLogitsLoss()\n","    #loss_function = nn.CrossEntropyLoss()\n","    #loss_function = nn.HingeEmbeddingLoss()\n","\n","##################################################################################################################################################################################################################################\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","##################################################################################################################################################################################################################################\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        if test_dataloader is not None:\n","            test_preds = predict(model, test_dataloader)\n","            print(\"Testing Classification Report:\")\n","            test_accuracy=accuracy_score(test_y, test_preds)\n","            print('-' * 107); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% / {100 * test_accuracy:.4f}% |'); print('-' * 107);\n","\n","        else:\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100);\n","        metric = valid_loss\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print('~'*26);print(f'|Best Value Found {best_local_metric:.5f}|');print('~'*26); print(f'Params = {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54);\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(test_y, test_preds))\n","                    print('~' * 54)\n","\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50);\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)\n"],"metadata":{"id":"al1hGgPM-c2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##inf trial loss kfold"],"metadata":{"id":"7Y19i35fxQKU"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","        \"lr\": [5e-9, 5e-8],\n","        \"weight_decay\": [1e-5, 1e-3],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.01, 0.3],\n","\n","        \"scheduler_patience\": [1, 3],\n","        \"threshold\": [1e-5, 1e-4],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-9],\n","\n","        \"max_seq_len\": [45, 55],\n","        \"batch_size\": [10, 15],\n","\n","        \"dropout\": [0.4, 0.6],\n","        \"activation\": ['leaky_relu'],\n","\n","        'test_size': [0.3, 0.5],\n","        'random_state': [42, 1024]\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print(); print('='*40)\n","    for param, value in params.items():\n","        print(f'{param:<20} = {value}')\n","    print('='*40); print()\n","##################################################################################################################################################################################################################################\n","\n","    train_labels, val_labels = train_test_split( dfTrain['target'],\n","                                                test_size=params[\"test_size\"],\n","                                                random_state=params[\"random_state\"],\n","                                                 stratify=dfTrain['target'])\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,\n","                                                                                 random_state=params[\"random_state\"],\n","                                                                                 test_size=params[\"test_size\"]\n","                                                                                 )\n","\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)#best one so far\n","\n","    #loss_function = nn.NLLLoss()\n","    #loss_function = nn.CrossEntropyLoss()\n","##################################################################################################################################################################################################################################\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","##################################################################################################################################################################################################################################\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100); print()\n","        metric = valid_loss\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print(); print(f'| Best Value Found {best_local_metric:.5f} |'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54); print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GnNjG1AR8jOj","executionInfo":{"status":"error","timestamp":1692055768420,"user_tz":360,"elapsed":487209,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"f1c2e179-31f5-486d-cdf2-7fdd06a7a4c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 23:21:20,667] A new study created in memory with name: no-name-9d9c283f-404d-4405-93e5-9512dc9206e8\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================\n","lr                   = 8.495650670521902e-09\n","weight_decay         = 0.00011788127983061654\n","beta1                = 0.9450462163076663\n","beta2                = 0.9713897777803389\n","factor               = 0.047712613878744904\n","scheduler_patience   = 3\n","threshold            = 5.766610943528399e-05\n","cooldown             = 0\n","min_lr               = 0\n","max_seq_len          = 50\n","batch_size           = 10\n","dropout              = 0.42031054052433425\n","activation           = leaky_relu\n","test_size            = 0.3005615797471134\n","random_state         = 559\n","========================================\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 1 | Loss Train/Valid:     0.633214 / 0.622130 | Accuracy Train/Valid: 71.225% / 93.7090% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.62213 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.87      0.78      3036\n","           1       0.75      0.50      0.60      2288\n","\n","    accuracy                           0.71      5324\n","   macro avg       0.72      0.69      0.69      5324\n","weighted avg       0.72      0.71      0.70      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.97      0.95      1306\n","           1       0.96      0.89      0.92       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.93      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 2 | Loss Train/Valid:     0.621358 / 0.609870 | Accuracy Train/Valid: 74.380% / 94.4517% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.60987 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.88      0.80      3036\n","           1       0.78      0.56      0.65      2288\n","\n","    accuracy                           0.74      5324\n","   macro avg       0.76      0.72      0.72      5324\n","weighted avg       0.75      0.74      0.73      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95      1306\n","           1       0.96      0.91      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 3 | Loss Train/Valid:     0.608412 / 0.597929 | Accuracy Train/Valid: 78.268% / 94.5828% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.59793 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.91      0.83      3036\n","           1       0.83      0.62      0.71      2288\n","\n","    accuracy                           0.78      5324\n","   macro avg       0.80      0.76      0.77      5324\n","weighted avg       0.79      0.78      0.78      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.96      0.91      0.94       983\n","\n","    accuracy                           0.95      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.95      0.95      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 4 | Loss Train/Valid:     0.599566 / 0.586424 | Accuracy Train/Valid: 80.334% / 94.5828% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.58642 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.91      0.84      3036\n","           1       0.85      0.66      0.74      2288\n","\n","    accuracy                           0.80      5324\n","   macro avg       0.82      0.79      0.79      5324\n","weighted avg       0.81      0.80      0.80      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.96      0.91      0.94       983\n","\n","    accuracy                           0.95      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.95      0.95      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 5 | Loss Train/Valid:     0.587700 / 0.575299 | Accuracy Train/Valid: 82.870% / 94.5391% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.57530 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.92      0.86      3036\n","           1       0.87      0.71      0.78      2288\n","\n","    accuracy                           0.83      5324\n","   macro avg       0.84      0.81      0.82      5324\n","weighted avg       0.83      0.83      0.83      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.96      0.91      0.93       983\n","\n","    accuracy                           0.95      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.95      0.95      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 6 | Loss Train/Valid:     0.577362 / 0.564638 | Accuracy Train/Valid: 84.861% / 94.5391% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.56464 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.93      0.88      3036\n","           1       0.89      0.74      0.81      2288\n","\n","    accuracy                           0.85      5324\n","   macro avg       0.86      0.83      0.84      5324\n","weighted avg       0.85      0.85      0.85      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.96      0.91      0.93       983\n","\n","    accuracy                           0.95      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.95      0.95      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 7 | Loss Train/Valid:     0.567517 / 0.554425 | Accuracy Train/Valid: 86.983% / 94.5828% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.55443 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.95      0.89      3036\n","           1       0.92      0.77      0.83      2288\n","\n","    accuracy                           0.87      5324\n","   macro avg       0.88      0.86      0.86      5324\n","weighted avg       0.88      0.87      0.87      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.96      0.92      0.94       983\n","\n","    accuracy                           0.95      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.95      0.95      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 8 | Loss Train/Valid:     0.557778 / 0.544666 | Accuracy Train/Valid: 88.129% / 94.4954% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.54467 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.95      0.90      3036\n","           1       0.92      0.79      0.85      2288\n","\n","    accuracy                           0.88      5324\n","   macro avg       0.89      0.87      0.88      5324\n","weighted avg       0.89      0.88      0.88      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 9 | Loss Train/Valid:     0.547797 / 0.535228 | Accuracy Train/Valid: 90.270% / 94.4954% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.53523 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.96      0.92      3036\n","           1       0.94      0.82      0.88      2288\n","\n","    accuracy                           0.90      5324\n","   macro avg       0.91      0.89      0.90      5324\n","weighted avg       0.91      0.90      0.90      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.97      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.95      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 10 | Loss Train/Valid:     0.537951 / 0.526147 | Accuracy Train/Valid: 91.341% / 94.4517% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.52615 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.96      0.93      3036\n","           1       0.94      0.85      0.89      2288\n","\n","    accuracy                           0.91      5324\n","   macro avg       0.92      0.91      0.91      5324\n","weighted avg       0.92      0.91      0.91      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 11 | Loss Train/Valid:     0.525789 / 0.517314 | Accuracy Train/Valid: 92.149% / 94.4517% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.51731 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.96      0.93      3036\n","           1       0.95      0.86      0.90      2288\n","\n","    accuracy                           0.92      5324\n","   macro avg       0.93      0.91      0.92      5324\n","weighted avg       0.92      0.92      0.92      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 12 | Loss Train/Valid:     0.517608 / 0.508765 | Accuracy Train/Valid: 93.088% / 94.4517% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.50876 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.97      0.94      3036\n","           1       0.95      0.88      0.92      2288\n","\n","    accuracy                           0.93      5324\n","   macro avg       0.93      0.93      0.93      5324\n","weighted avg       0.93      0.93      0.93      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.95      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 13 | Loss Train/Valid:     0.508642 / 0.500447 | Accuracy Train/Valid: 93.576% / 94.4080% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.50045 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.97      0.95      3036\n","           1       0.96      0.89      0.92      2288\n","\n","    accuracy                           0.94      5324\n","   macro avg       0.94      0.93      0.93      5324\n","weighted avg       0.94      0.94      0.94      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 14 | Loss Train/Valid:     0.502907 / 0.492392 | Accuracy Train/Valid: 93.595% / 94.3207% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.49239 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.97      0.95      3036\n","           1       0.96      0.89      0.92      2288\n","\n","    accuracy                           0.94      5324\n","   macro avg       0.94      0.93      0.93      5324\n","weighted avg       0.94      0.94      0.94      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 15 | Loss Train/Valid:     0.493683 / 0.484526 | Accuracy Train/Valid: 94.102% / 94.2770% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.48453 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95      3036\n","           1       0.95      0.91      0.93      2288\n","\n","    accuracy                           0.94      5324\n","   macro avg       0.94      0.94      0.94      5324\n","weighted avg       0.94      0.94      0.94      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","----------------------------------------------------------------------------------------------------\n","| Epoch 16 | Loss Train/Valid:     0.487467 / 0.476903 | Accuracy Train/Valid: 94.140% / 94.2770% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.47690 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95      3036\n","           1       0.96      0.90      0.93      2288\n","\n","    accuracy                           0.94      5324\n","   macro avg       0.94      0.94      0.94      5324\n","weighted avg       0.94      0.94      0.94      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.95      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-14 23:29:27,662] Trial 0 failed with parameters: {'lr': 8.495650670521902e-09, 'weight_decay': 0.00011788127983061654, 'beta1': 0.9450462163076663, 'beta2': 0.9713897777803389, 'factor': 0.047712613878744904, 'scheduler_patience': 3, 'threshold': 5.766610943528399e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 50, 'batch_size': 10, 'dropout': 0.42031054052433425, 'activation': 'leaky_relu', 'test_size': 0.3005615797471134, 'random_state': 559} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-205-e1a23a146d9e>\", line 117, in <lambda>\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","  File \"<ipython-input-205-e1a23a146d9e>\", line 91, in objective_loss\n","    train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","  File \"<ipython-input-17-c44c314b8aa9>\", line 14, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","[W 2023-08-14 23:29:27,663] Trial 0 failed with value None.\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","| Epoch 17 | Loss Train/Valid:     0.477774 / 0.469481 | Accuracy Train/Valid: 94.421% / 94.1896% |\n","----------------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.46948 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95      3036\n","           1       0.96      0.91      0.93      2288\n","\n","    accuracy                           0.94      5324\n","   macro avg       0.95      0.94      0.94      5324\n","weighted avg       0.94      0.94      0.94      5324\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95      1306\n","           1       0.94      0.92      0.93       983\n","\n","    accuracy                           0.94      2289\n","   macro avg       0.94      0.94      0.94      2289\n","weighted avg       0.94      0.94      0.94      2289\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-205-e1a23a146d9e>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna_inf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-205-e1a23a146d9e>\u001b[0m in \u001b[0;36mtune_model_with_optuna_inf_loss\u001b[0;34m(train_dataloader, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-205-e1a23a146d9e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-205-e1a23a146d9e>\u001b[0m in \u001b[0;36mobjective_loss\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mno_improvement_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c44c314b8aa9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","from sklearn.model_selection import KFold\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","        \"lr\": [5e-7, 5e-5],\n","        \"weight_decay\": [1e-5, 1e-3],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.01, 0.3],\n","\n","        \"scheduler_patience\": [1, 3],\n","        \"threshold\": [1e-5, 1e-4],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-9],\n","\n","        \"max_seq_len\": [45, 55],\n","        \"batch_size\": [10, 15],\n","\n","        \"dropout\": [0.4, 0.6],\n","        \"activation\": ['leaky_relu'],\n","\n","        'test_size': [0.3, 0.5],\n","        'random_state': [42, 1024]\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print(); print('='*40)\n","    for param, value in params.items():\n","        print(f'{param:<20} = {value}')\n","    print('='*40); print()\n","##################################################################################################################################################################################################################################\n","\n","    train_labels, val_labels = train_test_split( dfTrain['target'],\n","                                                test_size=params[\"test_size\"],\n","                                                random_state=params[\"random_state\"],\n","                                                 stratify=dfTrain['target'])\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,\n","                                                                                 random_state=params[\"random_state\"],\n","                                                                                 test_size=params[\"test_size\"]\n","                                                                                 )\n","\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)\n","\n","    #loss_function = nn.NLLLoss()\n","    #loss_function = nn.CrossEntropyLoss()\n","##################################################################################################################################################################################################################################\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","##################################################################################################################################################################################################################################\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    n_splits = 5\n","    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=params[\"random_state\"])\n","    total_metric = 0  # Will store the sum of the validation metric across all folds\n","\n","    # Loop through each fold\n","    for fold, (train_index, val_index) in enumerate(kfold.split(dfTrain)):\n","        train_data, val_data = dfTrain.iloc[train_index], dfTrain.iloc[val_index]\n","        train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(\n","            train_data, tokenizer, max_seq_len, batch_size, random_state=params[\"random_state\"], test_size=params[\"test_size\"]\n","        )\n","\n","        # Initializing model and related objects for each fold\n","        model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","        model = model.to(device)\n","        optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","        best_local_metric = float('inf')\n","        no_improvement_counter = 0\n","\n","        for epoch in range(3):\n","            train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","            valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100); print()\n","            metric = valid_loss\n","            if metric < best_local_metric:\n","                best_local_metric = metric\n","                no_improvement_counter = 0\n","                if metric < best_valid_metric:\n","                    best_valid_metric = metric\n","                    best_state_dict = copy.deepcopy(model.state_dict())\n","                    best_params=params\n","                    print(); print(f'| Best Value Found {best_local_metric:.5f} |'); print('~' * 54)\n","                    print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                    print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                    print('~' * 54); print()\n","            else:\n","                no_improvement_counter += 1\n","                if no_improvement_counter >= 2:\n","                    print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                    break\n","\n","        total_metric += best_local_metric\n","\n","    return total_metric / n_splits\n","\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)\n"],"metadata":{"id":"QtthHp21VHsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"],"metadata":{"executionInfo":{"status":"error","timestamp":1692054760610,"user_tz":360,"elapsed":86121,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GFPAHwqg0YbB","outputId":"215f6e44-e684-4c00-a0df-d387dbf8599c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 23:11:13,949] A new study created in memory with name: no-name-91530647-700c-4e08-9ed2-1ad94078a65f\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================\n","lr                   = 8.68979317505054e-09\n","weight_decay         = 2.0457267211606944e-05\n","dropout              = 0.5221146340093656\n","beta1                = 0.841191843800257\n","beta2                = 0.9209744330696439\n","factor               = 0.28752580134054895\n","scheduler_patience   = 2\n","threshold            = 1.4405984292076701e-05\n","cooldown             = 0\n","min_lr               = 0\n","max_seq_len          = 54\n","batch_size           = 13\n","activation           = leaky_relu\n","test_size            = 0.3831825144113664\n","random_state         = 884\n","========================================\n","\n","| Epoch 1 | Loss Train/Valid:     0.732457 / 0.718290 | Accuracy Train/Valid: 43.046% / 5.0034% |\n","------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.71829 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.40      0.44      2678\n","           1       0.37      0.47      0.42      2017\n","\n","    accuracy                           0.43      4695\n","   macro avg       0.44      0.44      0.43      4695\n","weighted avg       0.45      0.43      0.43      4695\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.03      0.02      0.02      1664\n","           1       0.06      0.09      0.08      1254\n","\n","    accuracy                           0.05      2918\n","   macro avg       0.05      0.05      0.05      2918\n","weighted avg       0.04      0.05      0.05      2918\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","| Epoch 2 | Loss Train/Valid:     0.723786 / 0.709928 | Accuracy Train/Valid: 44.452% / 14.3592% |\n","------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.70993 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.41      0.46      2678\n","           1       0.39      0.49      0.43      2017\n","\n","    accuracy                           0.44      4695\n","   macro avg       0.45      0.45      0.44      4695\n","weighted avg       0.46      0.44      0.45      4695\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.02      0.01      0.01      1664\n","           1       0.20      0.32      0.24      1254\n","\n","    accuracy                           0.14      2918\n","   macro avg       0.11      0.17      0.13      2918\n","weighted avg       0.09      0.14      0.11      2918\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","| Epoch 3 | Loss Train/Valid:     0.715852 / 0.701662 | Accuracy Train/Valid: 46.390% / 41.6038% |\n","------------------------------------------------------------------------------------------\n","\n","\n","| Best Value Found 0.70166 |\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.42      0.47      2678\n","           1       0.40      0.52      0.46      2017\n","\n","    accuracy                           0.46      4695\n","   macro avg       0.47      0.47      0.46      4695\n","weighted avg       0.48      0.46      0.46      4695\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1664\n","           1       0.42      0.97      0.59      1254\n","\n","    accuracy                           0.42      2918\n","   macro avg       0.21      0.48      0.29      2918\n","weighted avg       0.18      0.42      0.25      2918\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-14 23:12:39,896] Trial 0 failed with parameters: {'lr': 8.68979317505054e-09, 'weight_decay': 2.0457267211606944e-05, 'dropout': 0.5221146340093656, 'beta1': 0.841191843800257, 'beta2': 0.9209744330696439, 'factor': 0.28752580134054895, 'scheduler_patience': 2, 'threshold': 1.4405984292076701e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 54, 'batch_size': 13, 'activation': 'leaky_relu', 'test_size': 0.3831825144113664, 'random_state': 884} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-202-128a21490522>\", line 8, in <lambda>\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","  File \"<ipython-input-196-8ccb2622bb00>\", line 87, in objective_loss\n","    train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","  File \"<ipython-input-17-c44c314b8aa9>\", line 18, in train\n","    correct_predictions += (predicted_labels == labels).sum().item()\n","KeyboardInterrupt\n","[W 2023-08-14 23:12:39,898] Trial 0 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-202-128a21490522>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna_inf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-202-128a21490522>\u001b[0m in \u001b[0;36mtune_model_with_optuna_inf_loss\u001b[0;34m(train_dataloader, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-128a21490522>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-196-8ccb2622bb00>\u001b[0m in \u001b[0;36mobjective_loss\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mno_improvement_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c44c314b8aa9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpredicted_labels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(best_valid_metric)\n","print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3eyBI0JT7lE","executionInfo":{"status":"ok","timestamp":1692055775402,"user_tz":360,"elapsed":154,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"63d7ecdc-a750-4c69-ca0d-75165545cea4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.46948113751203213\n","{'lr': 8.495650670521902e-09, 'weight_decay': 0.00011788127983061654, 'beta1': 0.9450462163076663, 'beta2': 0.9713897777803389, 'factor': 0.047712613878744904, 'scheduler_patience': 3, 'threshold': 5.766610943528399e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 50, 'batch_size': 10, 'dropout': 0.42031054052433425, 'activation': 'leaky_relu', 'test_size': 0.3005615797471134, 'random_state': 559}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKgEPm4P0YbB"},"outputs":[],"source":["tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"]},{"cell_type":"markdown","source":["##inf trial loss bce"],"metadata":{"id":"WA9c1T_NjCRn"}},{"cell_type":"code","source":["def train(model, optimizer, train_dataloader, device, loss_function):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_train = []\n","    predicted_labels_train = []\n","\n","    for step, batch in enumerate(train_dataloader):\n","        sent_id, mask, labels = [r.to(device) for r in batch]\n","        labels = labels.float().unsqueeze(1)  # Ensure correct shape\n","        model.zero_grad()\n","        predictions = model(sent_id, mask)\n","        predictions = torch.sigmoid(predictions)  # Convert logits to probabilities\n","        loss = loss_function(predictions, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        predicted_labels = (predictions > 0.5).float()\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        true_labels_train.extend(labels.cpu().numpy())\n","        predicted_labels_train.extend(predicted_labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    accuracy = correct_predictions / len(train_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_train, predicted_labels_train\n","\n","# Evaluate function will be similar to the train function with the appropriate changes\n","\n","def evaluate(model, val_dataloader, device, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_eval = []\n","    predicted_labels_eval = []\n","\n","    with torch.no_grad():\n","        for step, batch in enumerate(val_dataloader):\n","            sent_id, mask, labels = [t.to(device) for t in batch]\n","            labels = labels.float().unsqueeze(1)  # Making sure labels are in correct shape\n","            predictions = model(sent_id, mask)\n","            loss = loss_function(predictions, labels)\n","            total_loss += loss.item()\n","\n","            predicted_labels = (torch.sigmoid(predictions) > 0.5).float()  # Converting predictions to 0 or 1\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            true_labels_eval.extend(labels.cpu().numpy())\n","            predicted_labels_eval.extend(predicted_labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    accuracy = correct_predictions / len(val_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_eval, predicted_labels_eval\n"],"metadata":{"id":"b8XJWyutiFyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params, best_valid_metric\n","    model_params = {\n","        \"lr\": [5e-8, 5e-6],\n","        \"weight_decay\": [1e-5, 1e-3],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.01, 0.3],\n","\n","        \"scheduler_patience\": [1, 3],\n","        \"threshold\": [1e-5, 1e-4],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-9],\n","\n","        \"max_seq_len\": [45, 55],\n","        \"batch_size\": [10, 15],\n","\n","        \"dropout\": [0.2, 0.3],\n","        \"activation\": ['leaky_relu'],\n","\n","        'test_size': [0.2, 0.3],\n","        'train_size': [0.5, 0.7],\n","        'random_state': [42, 1024],\n","\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str):\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    print(); print('='*40)\n","    for param, value in params.items():\n","        print(f'{param:<20} = {value}')\n","    print('='*40); print()\n","##################################################################################################################################################################################################################################\n","\n","    train_labels, val_labels = train_test_split(dfTrain['target'], test_size=params[\"test_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['target'])\n","    test_dataloader=None\n","    train_dataloader, val_dataloader, test_dataloader, val_y, test_y, train_y = prepare_train_val_test_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"],train_size=params[\"train_size\"], val_to_test_ratio=params[\"test_size\"])\n","    #train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size,random_state=params[\"random_state\"], test_size=params[\"test_size\"])\n","\n","\n","    all_labels = np.concatenate([train_labels, val_labels])\n","    #all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    #loss_function = nn.BCEWithLogitsLoss(weight=weights)\n","    loss_function = nn.BCELoss(weight=weights)\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    #loss_function = nn.CrossEntropyLoss(weight=weights)#best one so far\n","\n","\n","    #loss_function = nn.BCELoss()\n","    #loss_function = nn.NLLLoss()\n","    #loss_function = nn.BCEWithLogitsLoss()\n","    #loss_function = nn.CrossEntropyLoss()\n","\n","\n","##################################################################################################################################################################################################################################\n","\n","\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","##################################################################################################################################################################################################################################\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        if test_dataloader is not None:\n","            test_preds = predict(model, test_dataloader)\n","            print(\"Testing Classification Report:\")\n","            test_accuracy=accuracy_score(test_y, test_preds)\n","            print('-' * 110); print(f'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% / {100 * test_accuracy:.4f}% |'); print('-' * 110); print()\n","            print()\n","        else:\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid:     {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100); print()\n","        metric = valid_loss\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print(); print(f'| Best Value Found {best_local_metric:.5f} | {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54); print()\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(test_y, test_preds))\n","                    print('~' * 54)\n","                    print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1692142541527,"user_tz":360,"elapsed":888,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"e92ed42d-6cad-4b0c-bd46-4006d1dfa351","id":"hFy8coajjCRo"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-15 23:35:39,490] A new study created in memory with name: no-name-db9affe0-03f4-4902-ace6-e3f943e1d9ac\n"]},{"output_type":"stream","name":"stdout","text":["\n","========================================\n","lr                   = 1.0122157070701595e-06\n","weight_decay         = 5.040253118492428e-05\n","beta1                = 0.8434482887336511\n","beta2                = 0.9015405573580578\n","factor               = 0.046447932100891105\n","scheduler_patience   = 3\n","threshold            = 9.024276392735006e-05\n","cooldown             = 0\n","min_lr               = 0\n","max_seq_len          = 53\n","batch_size           = 14\n","dropout              = 0.2946064278209388\n","activation           = leaky_relu\n","test_size            = 0.20216088378210928\n","train_size           = 0.5803655696495112\n","random_state         = 140\n","========================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-15 23:35:40,189] Trial 0 failed with parameters: {'lr': 1.0122157070701595e-06, 'weight_decay': 5.040253118492428e-05, 'beta1': 0.8434482887336511, 'beta2': 0.9015405573580578, 'factor': 0.046447932100891105, 'scheduler_patience': 3, 'threshold': 9.024276392735006e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 53, 'batch_size': 14, 'dropout': 0.2946064278209388, 'activation': 'leaky_relu', 'test_size': 0.20216088378210928, 'train_size': 0.5803655696495112, 'random_state': 140} because of the following error: RuntimeError('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-54-89797ae1fca7>\", line 140, in <lambda>\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","  File \"<ipython-input-54-89797ae1fca7>\", line 72, in objective_loss\n","    weights = weights.to(device)\n","RuntimeError: CUDA error: device-side assert triggered\n","CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n","For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","[W 2023-08-15 23:35:40,195] Trial 0 failed with value None.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-89797ae1fca7>\u001b[0m in \u001b[0;36m<cell line: 156>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna_inf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-54-89797ae1fca7>\u001b[0m in \u001b[0;36mtune_model_with_optuna_inf_loss\u001b[0;34m(train_dataloader, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-89797ae1fca7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-89797ae1fca7>\u001b[0m in \u001b[0;36mobjective_loss\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#loss_function = nn.BCEWithLogitsLoss(weight=weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","def tune_model_with_optuna_inf_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","\n","\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"],"metadata":{"id":"nw5hEAaZjCRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(best_valid_metric)\n","print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692055775402,"user_tz":360,"elapsed":154,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"63d7ecdc-a750-4c69-ca0d-75165545cea4","id":"s6IGXwW1jCRp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.46948113751203213\n","{'lr': 8.495650670521902e-09, 'weight_decay': 0.00011788127983061654, 'beta1': 0.9450462163076663, 'beta2': 0.9713897777803389, 'factor': 0.047712613878744904, 'scheduler_patience': 3, 'threshold': 5.766610943528399e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 50, 'batch_size': 10, 'dropout': 0.42031054052433425, 'activation': 'leaky_relu', 'test_size': 0.3005615797471134, 'random_state': 559}\n"]}]},{"cell_type":"code","source":["tuned_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","tuned_model.load_state_dict(best_state_dict)\n","tuned_model = tuned_model.to(device)\n","# Prepare dataloaders\n","\n","train_dataloader, val_dataloader, val_y, train_y,test_dataloader = prepare_data_loaders(dfTrain, dfTest, tokenizer,random_state=(best_params[\"random_state\"]+random.randint(0, 100)))\n","\n","val_predictions = predict(tuned_model, val_dataloader)\n","print('='*40)\n","print(classification_report(val_y, val_predictions))\n","for param, value in best_params.items():\n","    print(f'{param:<20} = {value}')\n","\n","print('='*40); print()\n","\n","print(best_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692135242144,"user_tz":360,"elapsed":3589,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"fedda361-d979-45b6-8335-d8c52ccbbf5c","id":"-oO9djEXjCRp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["========================================\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.90      0.88      1303\n","           1       0.86      0.79      0.82       981\n","\n","    accuracy                           0.85      2284\n","   macro avg       0.85      0.85      0.85      2284\n","weighted avg       0.85      0.85      0.85      2284\n","\n","lr                   = 1.4603446397626205e-07\n","weight_decay         = 0.00020599397194466426\n","beta1                = 0.9467071101506788\n","beta2                = 0.9841943945969813\n","factor               = 0.04069921713514273\n","scheduler_patience   = 1\n","threshold            = 2.990565088545137e-05\n","cooldown             = 0\n","min_lr               = 0\n","max_seq_len          = 51\n","batch_size           = 10\n","dropout              = 0.5310402013616752\n","activation           = leaky_relu\n","test_size            = 0.33337290801312347\n","random_state         = 246\n","========================================\n","\n","{'lr': 1.4603446397626205e-07, 'weight_decay': 0.00020599397194466426, 'beta1': 0.9467071101506788, 'beta2': 0.9841943945969813, 'factor': 0.04069921713514273, 'scheduler_patience': 1, 'threshold': 2.990565088545137e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 51, 'batch_size': 10, 'dropout': 0.5310402013616752, 'activation': 'leaky_relu', 'test_size': 0.33337290801312347, 'random_state': 246}\n"]}]},{"cell_type":"code","source":["test_dataloader=prepare_test_loader(dfTest, tokenizer, max_seq_len=51, batch_size=10)\n","test_predictions = predict(tuned_model, test_dataloader)\n","create_and_download_submission(test_predictions, 'submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1692134957373,"user_tz":360,"elapsed":5116,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"096bd728-0fcc-49ce-c12e-bc95de3edca4","id":"PcDmqkVljCRq"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_6d36add5-42c3-4601-8d30-32678ee43de4\", \"submission.csv\", 22746)"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P6lxirRjCRq"},"outputs":[],"source":["tuned_model= tune_model_with_optuna_inf_loss(dfTrain, tokenizer, device, bert)"]},{"cell_type":"markdown","source":["#set epoch's number"],"metadata":{"id":"-U4nlb1vkbpW"}},{"cell_type":"markdown","source":["##loss"],"metadata":{"id":"DO4b4myElrzx"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","model_params = {\n","    \"lr\": [5e-9, 5e-8],  # Lower range\n","    \"weight_decay\": [1e-5, 1e-3],  # Higher range\n","    \"dropout\": [0.4, 0.6],  # Higher range\n","    \"beta1\": [0.8, 0.99],  # Unchanged\n","    \"beta2\": [0.9, 0.9999], # Unchanged\n","    \"factor\": [0.01, 0.3], # Lower upper bound\n","    \"scheduler_patience\": [1, 3], # Higher upper bound\n","    \"threshold\": [1e-5, 1e-4], # Higher range\n","    \"cooldown\": [0, 1],  # Unchanged\n","    \"min_lr\": [0, 1e-9], # Unchanged\n","    \"max_seq_len\": [45, 55],  # Unchanged\n","    \"batch_size\": [10, 15], # Higher range\n","    \"activation\": ['leaky_relu'],  # Unchanged\n","    'test_size': [0.3, 0.5], # Higher upper bound\n","    'random_state': [42, 1024], # Unchanged\n","    }\n","\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [5e-9, 5e-8],  # Lower range\n","    \"weight_decay\": [1e-5, 1e-3],  # Higher range\n","    \"dropout\": [0.4, 0.6],  # Higher range\n","    \"beta1\": [0.8, 0.99],  # Unchanged\n","    \"beta2\": [0.9, 0.9999], # Unchanged\n","    \"factor\": [0.01, 0.3], # Lower upper bound\n","    \"scheduler_patience\": [1, 3], # Higher upper bound\n","    \"threshold\": [1e-5, 1e-4], # Higher range\n","    \"cooldown\": [0, 1],  # Unchanged\n","    \"min_lr\": [0, 1e-9], # Unchanged\n","    \"max_seq_len\": [45, 55],  # Unchanged\n","    \"batch_size\": [10, 15], # Higher range\n","    \"activation\": ['leaky_relu'],  # Unchanged\n","    'test_size': [0.3, 0.5], # Higher upper bound\n","    'random_state': [42, 1024], # Unchanged\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      test_size=params[\"test_size\"],\n","                                                                      random_state=params[\"random_state\"],\n","\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    loss_function = nn.NLLLoss(weight=weights)\n","\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('==================================================')\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('==================================================')\n","    print()\n","\n","    loss_function = nn.CrossEntropyLoss()\n","    best_local_metric=12312310\n","\n","    separator = '=' * 180\n","    valid_losses=[]\n","\n","\n","\n","    no_improvement_counter = 0\n","\n","    for epoch in range(1000000):\n","        print(' ' * 37,end='')\n","        print(f'|Epoch {epoch + 1}|')\n","        print('-' * 90)\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","\n","        print(f'|Loss Train/Valid:       {train_loss:.6f} / {valid_loss:.6f} |',end='')\n","        print(f' Accuracy Train/Valid:   {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |')\n","        print('-' * 90)\n","        print()\n","        metric = valid_loss\n","\n","\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print()\n","                print('~' * 25)\n","                print(f'|Best Value Found {best_local_metric:.5f}|')\n","                print('~' * 25)\n","                print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:  # If no improvement twice in a row\n","                print()\n","\n","                print('<'*50)\n","                print('OH NO WE BE OVERFITTING')\n","                print('<'*50)\n","\n","                print()\n","\n","                print(\"Training Classification Report:\")\n","                print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\")\n","                print(classification_report(true_labels_eval, predicted_labels_eval))\n","\n","\n","                raise optuna.TrialPruned()\n","\n","    return metric\n","\n","\n"],"metadata":{"id":"tRqUeph50YbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","\n","def objective_loss(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","        \"lr\": [3e-7, 1e-5],\n","        \"weight_decay\": [1e-4, 1e-3],\n","        \"dropout\": [0.1, 0.5],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        #\"eps_optimizer\": [1e-09, 1e-07],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 2],\n","        #\"scheduler_eps\": [1e-09, 1e-07],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-3],\n","        \"max_seq_len\": [45, 55],  # Example bounds\n","        \"batch_size\": [5,10],   # Example bounds\n","        \"epochs\": [7,8],\n","\n","        \"activation\": ['leaky_relu'], # Added activation type\n","            #relu', 'sigmoid', 'tanh', 'gelu'\n","        'test_size':[0.25,0.35],\n","        'random_state':[42,1024],\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      test_size=params[\"test_size\"],\n","                                                                      random_state=params[\"random_state\"],\n","\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    loss_function = nn.NLLLoss(weight=weights)\n","\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    epochs=params['epochs']\n","    print(epochs)\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('==================================================')\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('==================================================')\n","    print()\n","\n","\n","\n","\n","\n","    loss_function = nn.CrossEntropyLoss()\n","    best_local_metric=12312310\n","\n","    separator = '=' * 180\n","    valid_losses=[]\n","\n","\n","\n","    for epoch in range(params[\"epochs\"]):\n","        print('-'*24)\n","        print(f'|Epoch {epoch + 1} / {params[\"epochs\"]}|')\n","        print('-'*31)\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","\n","        print(f'|Loss Train/Valid:     {train_loss:.5f} / {valid_loss:.5f}  |')\n","        print(f'|Accuracy Train/Valid:   {100*train_accuracy:.3f}%/{100*val_accuracy:.4f}%  |')\n","        print('-'*47)\n","        #metric=(1/abs(train_accuracy-val_accuracy))*abs(train_accuracy+val_accuracy)*val_accuracy\n","        metric=valid_loss\n","        #metric=val_accuracy\n","\n","        if metric <    best_local_metric:\n","            best_local_metric=metric\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print()\n","                print('^'*25)\n","                print(f'|Best Value Found {best_local_metric:.5f}|')\n","                print('^'*25)\n","                print()\n","        valid_losses.append(valid_loss)\n","\n","    print(\"Training Classification Report:\")\n","    print(classification_report(true_labels_train, predicted_labels_train))\n","    print(\"Validation Classification Report:\")\n","    print(classification_report(true_labels_eval, predicted_labels_eval))\n","\n","    print('*' * 180)\n","    print('                                                                                    TRIAL=GUD')\n","    print('*' * 180)\n","    print()\n","\n","\n","    torch.save(model.state_dict(), f'saved_weights_trial_{trial.number}.pt')\n","    avg_valid_loss = sum(valid_losses) / len(valid_losses)\n","\n","    print(('-----------------------'))\n","\n","\n","\n","    return metric\n","    #return (1/abs(train_accuracy-val_accuracy))*abs(train_accuracy+val_accuracy)*val_accuracy\n","\n","\n"],"metadata":{"id":"BFQUnzj4mHmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 121310\n","\n","def tune_model_with_optuna2_loss(train_dataloader, tokenizer, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}') # This is now the accuracy\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n"],"metadata":{"id":"zYQXud3lahaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","best_model.load_state_dict(best_state_dict)\n","best_model = best_model.to(device)\n"],"metadata":{"id":"3w5dio2aAVmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"82999394-a16b-40de-88de-c47c0fdd4b33","id":"7DD2XLJgy7iu","executionInfo":{"status":"error","timestamp":1692046350235,"user_tz":360,"elapsed":177588,"user":{"displayName":"Irek","userId":"02915086061816371557"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 20:49:32,103] A new study created in memory with name: no-name-82173cb8-eeb6-4963-9277-9c1d25a24100\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","lr = 5.48435834781086e-07\n","weight_decay = 0.0003425438457112304\n","dropout = 0.19955229648538778\n","beta1 = 0.8375633259989812\n","beta2 = 0.9791692632877389\n","factor = 0.41185710485245935\n","scheduler_patience = 2\n","threshold = 0.0006151660798547851\n","cooldown = 1\n","min_lr = 0\n","max_seq_len = 53\n","batch_size = 5\n","epochs = 7\n","activation = leaky_relu\n","test_size = 0.3337537666634927\n","random_state = 784\n","==================================================\n","\n","                                     |Epoch 1|\n","------------------------------------------------------------------------------------------\n","|Loss Train/Valid:       0.381092 / 0.469721 | Accuracy Train/Valid:   86.433% / 82.1366% |\n","------------------------------------------------------------------------------------------\n","\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^\n","|Best Value Found 0.46972|\n","^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","                                     |Epoch 2|\n","------------------------------------------------------------------------------------------\n","|Loss Train/Valid:       0.056996 / 0.685685 | Accuracy Train/Valid:   99.531% / 81.7426% |\n","------------------------------------------------------------------------------------------\n","\n","                                     |Epoch 3|\n","------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-14 20:52:29,336] Trial 0 failed with parameters: {'lr': 5.48435834781086e-07, 'weight_decay': 0.0003425438457112304, 'dropout': 0.19955229648538778, 'beta1': 0.8375633259989812, 'beta2': 0.9791692632877389, 'factor': 0.41185710485245935, 'scheduler_patience': 2, 'threshold': 0.0006151660798547851, 'cooldown': 1, 'min_lr': 0, 'max_seq_len': 53, 'batch_size': 5, 'epochs': 7, 'activation': 'leaky_relu', 'test_size': 0.3337537666634927, 'random_state': 784} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-111-ffc303bb779b>\", line 8, in <lambda>\n","    study.optimize(lambda trial: objective_loss(trial, train_dataloader, tokenizer, device, bert), n_trials=3)\n","  File \"<ipython-input-110-89eae4ada159>\", line 129, in objective_loss\n","    train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","  File \"<ipython-input-17-c44c314b8aa9>\", line 18, in train\n","    correct_predictions += (predicted_labels == labels).sum().item()\n","KeyboardInterrupt\n","[W 2023-08-14 20:52:29,338] Trial 0 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-fdf6a2babfd9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-111-ffc303bb779b>\u001b[0m in \u001b[0;36mtune_model_with_optuna2_loss\u001b[0;34m(train_dataloader, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-111-ffc303bb779b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-110-89eae4ada159>\u001b[0m in \u001b[0;36mobjective_loss\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'|Epoch {epoch + 1}|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c44c314b8aa9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpredicted_labels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["tuned_model= tune_model_with_optuna2_loss(dfTrain, tokenizer, device, bert)"]},{"cell_type":"markdown","source":["##accuracy"],"metadata":{"id":"9fDLPEBAluBs"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","\n","\n","def objective_accuracy(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [5e-9, 1e-8],  # Lower range\n","    \"weight_decay\": [1e-5, 1e-3],  # Higher range\n","    \"dropout\": [0.4, 0.6],  # Higher range\n","    \"beta1\": [0.8, 0.99],  # Unchanged\n","    \"beta2\": [0.9, 0.9999], # Unchanged\n","    \"factor\": [0.01, 0.3], # Lower upper bound\n","    \"scheduler_patience\": [1,2], # Higher upper bound\n","    \"threshold\": [1e-5, 1e-4], # Higher range\n","    \"cooldown\": [0, 1],  # Unchanged\n","    \"min_lr\": [0, 1e-9], # Unchanged\n","    \"max_seq_len\": [45, 55],  # Unchanged\n","    \"batch_size\": [10, 15], # Higher range\n","    \"activation\": ['leaky_relu'],  # Unchanged\n","    'test_size': [0.3, 0.5], # Higher upper bound\n","    'random_state': [42, 1024], # Unchanged\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      test_size=params[\"test_size\"],\n","                                                                      random_state=params[\"random_state\"],\n","\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    loss_function = nn.NLLLoss(weight=weights)\n","\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('='*40)\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('='*40)\n","    print()\n","\n","\n","\n","\n","\n","    loss_function = nn.CrossEntropyLoss()\n","    best_local_metric=0\n","\n","    separator = '=' * 180\n","    valid_losses=[]\n","    metric=0\n","\n","\n","    no_improvement_counter = 0\n","\n","    for epoch in range(100000000):\n","        print(' ' * 40,end='')\n","        print(f'|Epoch {epoch + 1}|')\n","        print('-' * 90)\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","\n","        print(f'|Loss Train/Valid:       {train_loss:.6f} / {valid_loss:.6f} |',end='')\n","        print(f' Accuracy Train/Valid:   {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |')\n","        print('-' * 90)\n","        print()\n","        metric = val_accuracy\n","\n","        if metric > best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if best_local_metric > best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print()\n","                print(f'|Best Value Found {100*best_local_metric:.5f}% |')\n","                print('~' * 54)\n","                print(\"Training Classification Report:\")\n","                print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\")\n","                print(classification_report(true_labels_eval, predicted_labels_eval))\n","\n","                print('~' * 54)\n","                print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:  # If no improvement twice in a row\n","                print()\n","\n","                print('<'*50)\n","                print('OH NO WE BE OVERFITTING')\n","                print('<'*50)\n","\n","                print()\n","\n","                raise optuna.TrialPruned()\n","\n","\n","\n","\n","    return metric\n"],"metadata":{"id":"RVDRCILSELQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_params = {\n","    \"lr\": [5e-9, 5e-8],  # Unchanged from previous\n","    \"weight_decay\": [1e-5, 1e-3],  # Unchanged from previous\n","    \"dropout\": [0.4, 0.6],  # Unchanged from previous\n","    \"beta1\": [0.8, 0.99],  # Unchanged from previous\n","    \"beta2\": [0.9, 0.9999], # Unchanged from previous\n","    \"factor\": [0.95, 0.99], # Close to 1 for smaller change\n","    \"scheduler_patience\": [0, 1], # Lower for higher frequency\n","    \"threshold\": [1e-6, 1e-5], # Lower for higher frequency\n","    \"cooldown\": [0, 1],  # Unchanged from previous\n","    \"min_lr\": [0, 1e-9], # Unchanged from previous\n","    \"max_seq_len\": [45, 55],  # Unchanged from previous\n","    \"batch_size\": [10, 15], # Unchanged from previous\n","    \"activation\": ['leaky_relu'],  # Unchanged from previous\n","    'test_size': [0.3, 0.5], # Unchanged from previous\n","    'random_state': [42, 1024], # Unchanged from previous\n","}\n","model_params = {\n","        \"lr\": [3e-7, 1e-5],\n","        \"weight_decay\": [1e-4, 1e-3],\n","        \"dropout\": [0.2, 0.5],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        #\"eps_optimizer\": [1e-09, 1e-07],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 2],\n","        #\"scheduler_eps\": [1e-09, 1e-07],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-3],\n","        \"max_seq_len\": [45, 55],  # Example bounds\n","        \"batch_size\": [7,12],   # Example bounds\n","        \"epochs\": [7,8],\n","\n","        \"activation\": ['leaky_relu'], # Added activation type\n","            #relu', 'sigmoid', 'tanh', 'gelu'\n","        'test_size':[0.25,0.35],\n","        'random_state':[42,1024],\n","    }\n","\n","model_params = {\n","    \"lr\": [5e-9, 5e-8],  # Unchanged from previous\n","    \"weight_decay\": [1e-5, 1e-3],  # Unchanged from previous\n","    \"dropout\": [0.4, 0.6],  # Unchanged from previous\n","    \"beta1\": [0.8, 0.99],  # Unchanged from previous\n","    \"beta2\": [0.9, 0.9999], # Unchanged from previous\n","    \"factor\": [0.95, 0.99], # Close to 1 for smaller change\n","    \"scheduler_patience\": [0, 1], # Lower for higher frequency\n","    \"threshold\": [1e-6, 1e-5], # Lower for higher frequency\n","    \"cooldown\": [0, 1],  # Unchanged from previous\n","    \"min_lr\": [0, 1e-9], # Unchanged from previous\n","    \"max_seq_len\": [45, 55],  # Unchanged from previous\n","    \"batch_size\": [10, 15], # Unchanged from previous\n","    \"activation\": ['leaky_relu'],  # Unchanged from previous\n","    'test_size': [0.3, 0.5], # Unchanged from previous\n","    'random_state': [42, 1024], # Unchanged from previous\n","}\n"],"metadata":{"id":"hQ3JVg4o4z17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","\n","\n","def objective_accuracy(trial, dfTrain, tokenizer, device, bert):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = model_params = {\n","    \"lr\": [5e-9, 5e-8],  # Lower range\n","    \"weight_decay\": [1e-5, 1e-3],  # Higher range\n","    \"dropout\": [0.4, 0.6],  # Higher range\n","    \"beta1\": [0.8, 0.99],  # Unchanged\n","    \"beta2\": [0.9, 0.9999], # Unchanged\n","    \"factor\": [0.01, 0.3], # Lower upper bound\n","    \"scheduler_patience\": [1, 3], # Higher upper bound\n","    \"threshold\": [1e-5, 1e-4], # Higher range\n","    \"cooldown\": [0, 1],  # Unchanged\n","    \"min_lr\": [0, 1e-9], # Unchanged\n","    \"max_seq_len\": [45, 55],  # Unchanged\n","    \"batch_size\": [10, 15], # Higher range\n","    \"activation\": ['leaky_relu'],  # Unchanged\n","    'test_size': [0.3, 0.5], # Higher upper bound\n","    'random_state': [42, 1024], # Unchanged\n","    }\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    train_text, val_text, train_labels, val_labels = train_test_split(dfTrain['text'], dfTrain['target'],\n","                                                                      test_size=params[\"test_size\"],\n","                                                                      random_state=params[\"random_state\"],\n","\n","                                                                      stratify=dfTrain['target'])\n","    # Compute class weights\n","    class_weights = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.unique(train_labels),\n","        y=train_labels\n","    )\n","\n","    # Convert class weights to tensor\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","    # Loss function\n","    loss_function = nn.NLLLoss(weight=weights)\n","\n","\n","\n","    # Extract max_seq_len and batch_size from the params\n","    max_seq_len = params[\"max_seq_len\"]\n","    batch_size = params[\"batch_size\"]\n","    epochs=params['epochs']\n","    print(epochs)\n","    # Prepare the data loaders using the extracted values\n","    train_dataloader, val_dataloader, val_y, train_y = prepare_train_val_loaders(dfTrain, tokenizer, max_seq_len, batch_size)\n","\n","    model = BERT_Arch(bert, activation=params[\"activation\"], dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    best_train_loss = float('inf')\n","    best_valid_loss = float('inf')\n","    print()\n","    print('==================================================')\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","    print('==================================================')\n","    print()\n","\n","\n","\n","\n","\n","    loss_function = nn.CrossEntropyLoss()\n","    best_local_metric=0\n","\n","    separator = '=' * 180\n","    valid_losses=[]\n","    metric=0\n","\n","\n","    no_improvement_counter = 0\n","\n","    for epoch in range(params[\"epochs\"]):\n","        print('-' * 14)\n","        print(f'|Epoch {epoch + 1} / {params[\"epochs\"]}|')\n","        print('-' * 44)\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","\n","        print(f'|Loss Train/Valid:     {train_loss:.5f} / {valid_loss:.5f}  |')\n","        print(f'|Accuracy Train/Valid:   {100 * train_accuracy:.3f}%/{100 * val_accuracy:.4f}%  |')\n","        print('-' * 47)\n","        metric = val_accuracy\n","\n","        if metric > best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if best_local_metric > best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print()\n","                print('^' * 25)\n","                print(f'|Best Value Found {best_local_metric:.5f}|')\n","                print('^' * 25)\n","                print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:  # If no improvement twice in a row\n","                print()\n","\n","                print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n","\n","                print()\n","\n","                raise optuna.TrialPruned()\n","\n","        valid_losses.append(valid_loss)\n","\n","    print(\"Training Classification Report:\")\n","    print(classification_report(true_labels_train, predicted_labels_train))\n","    print(\"Validation Classification Report:\")\n","    print(classification_report(true_labels_eval, predicted_labels_eval))\n","\n","    print('*' * 180)\n","    print('                                                                                    TRIAL=GUD')\n","    print('*' * 180)\n","    print()\n","\n","\n","    torch.save(model.state_dict(), f'saved_weights_trial_{trial.number}.pt')\n","    avg_valid_loss = sum(valid_losses) / len(valid_losses)\n","\n","    print(('-----------------------'))\n","\n","\n","\n","    return metric\n"],"metadata":{"id":"kpLqp-sumLJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","\n","def tune_model_with_optuna2_accuracy(dfTrain, val_dataloader, device, bert):\n","    global best_state_dict, best_params\n","    study = optuna.create_study(direction=\"maximize\") # Changed to maximize\n","    study.optimize(lambda trial: objective_accuracy(trial, dfTrain, val_dataloader, device, bert), n_trials=10)\n","\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}') # This is now the accuracy\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n"],"metadata":{"id":"zl8KvSRi7C15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KKWz8uOQ3Dy"},"source":["##here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QewYFCgmb-C"},"outputs":[],"source":["tuned_model= tune_model_with_optuna2_(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoUfiSxV7MxH"},"outputs":[],"source":["tuned_model= tune_model_with_optuna2_loss(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpkISYqilJrM"},"outputs":[],"source":["tuned_model= tune_model_with_optuna_accuracy(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ki5EsA1kKv6K"},"outputs":[],"source":["tuned_model= tune_model_with_optuna(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","source":["best_model = BERT_Arch(\n","        bert,\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","best_model.load_state_dict(best_state_dict)\n","best_model = best_model.to(device)\n","# Prepare dataloaders\n","\n","train_dataloader, val_dataloader, val_y, train_y,test_dataloader = prepare_data_loaders(dfTrain, dfTest, tokenizer)\n","\n","val_predictions = predict(tuned_model, val_dataloader)\n","\n","# Print classification report for validation predictions\n","print(classification_report(val_y, val_predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I87-BCkGHzLq","executionInfo":{"status":"ok","timestamp":1692141773014,"user_tz":360,"elapsed":2811,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"b918ec9f-e653-4e5a-a346-6416da9cfb0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.91      0.99      0.94      1303\n","           1       0.98      0.86      0.92       981\n","\n","    accuracy                           0.93      2284\n","   macro avg       0.94      0.93      0.93      2284\n","weighted avg       0.94      0.93      0.93      2284\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUMNQbQag4Hk"},"outputs":[],"source":["# Prepare dataloaders\n","\n","train_dataloader, val_dataloader, val_y, train_y,test_dataloader = prepare_data_loaders(dfTrain, dfTest, tokenizer)\n","\n","cool_params = {\n","        \"lr\": [3e-7, 1e-6],\n","        \"weight_decay\": [1e-4, 1e-2],\n","        \"dropout\": [0.1, 0.5],\n","\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"eps_optimizer\": [1e-09, 1e-07],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 10],\n","        \"scheduler_eps\": [1e-09, 1e-07],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 10],\n","        \"min_lr\": [0, 0.01],\n","        \"epochs\": [1, 2]  # added epochs here\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":554,"status":"ok","timestamp":1691283612669,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"AVgPv6aMW6Yu","outputId":"9754aaaa-d162-4f82-d419-35157e77488b"},"outputs":[{"data":{"text/plain":["torch.Size([4187])"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["train_y.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2041,"status":"ok","timestamp":1692051399492,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"CZH58duHd62P","outputId":"bfa692ae-1246-4f96-b798-fcb78f744a55"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.92      0.87      1303\n","           1       0.87      0.74      0.80       981\n","\n","    accuracy                           0.84      2284\n","   macro avg       0.85      0.83      0.83      2284\n","weighted avg       0.84      0.84      0.84      2284\n","\n"]}],"source":["\n","val_predictions = predict(tuned_model, val_dataloader)\n","\n","# Print classification report for validation predictions\n","print(classification_report(val_y, val_predictions))\n","\n","#test_predictions = predict(tuned_model, test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":788,"status":"ok","timestamp":1691270783856,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"VKDh7vBBl_u9","outputId":"c9b77ce3-519b-4e6b-e946-2dd374f09335"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 1, 1,  ..., 1, 0, 0])\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbcQ3sZamBah"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":2794,"status":"ok","timestamp":1692141780994,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"Bl0OwEyteCce","outputId":"813387b7-9f0b-4eac-c6d7-bbc6f4ba171a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_592d1606-c2bd-49f4-8c4b-1961dd8f7f88\", \"submission.csv\", 22746)"]},"metadata":{}}],"source":["test_predictions = predict(tuned_model, test_dataloader)\n","create_and_download_submission(test_predictions, 'submission.csv')"]},{"cell_type":"markdown","source":["# stuff"],"metadata":{"id":"3JnVvugnb6FO"}},{"cell_type":"code","source":[],"metadata":{"id":"UUr_z-9bb7fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"xRzAv55zSZmz","outputId":"40b9c964-1174-46b6-ef40-e7f08a6b3c46","executionInfo":{"status":"error","timestamp":1691883493805,"user_tz":360,"elapsed":856,"user":{"displayName":"Irek","userId":"02915086061816371557"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-ae3a6d4094ad>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create and download submission for final predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcreate_and_download_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'he_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'test123.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"id\": dfid[\"id\"],\n","        \"target\": predictions\n","    })\n","\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)\n","# Create and download submission for final predictions\n","create_and_download_submission(predictions, 'he_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"outputId":"ebe1fe71-53f9-4674-8c53-df7001392363","id":"Yj2JymBBcC__"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-08-10 03:54:23,623] A new study created in memory with name: no-name-ceebde8f-251e-4142-9def-dfc8ec214f35\n"]},{"name":"stdout","output_type":"stream","text":["7\n","\n","==================================================\n","lr = 9.302097388424848e-07\n","weight_decay = 0.00013791671922193954\n","dropout = 0.17453683286128835\n","beta1 = 0.9210256191989711\n","beta2 = 0.9201666680763443\n","factor = 0.1353644965826617\n","scheduler_patience = 1\n","threshold = 9.336043816286001e-05\n","cooldown = 0\n","min_lr = 0\n","max_seq_len = 50\n","batch_size = 30\n","epochs = 7\n","activation = gelu\n","==================================================\n","\n","-----------------------\n","|Epoch 1 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5409253465660503\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.4784427401307341\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.541\n","Validation Loss: 0.478\n","-----------------------\n","|Epoch 2 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.3067349784829643\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.4467666185908503\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.307\n","Validation Loss: 0.447\n","-----------------------\n","|Epoch 3 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.23350481270404344\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.48837761116492284\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.234\n","Validation Loss: 0.488\n","-----------------------\n","|Epoch 4 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.21683103386103436\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5190437613176061\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.217\n","Validation Loss: 0.519\n","-----------------------\n","|Epoch 5 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.21480843232253963\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5199149227761602\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.215\n","Validation Loss: 0.520\n","-----------------------\n","|Epoch 6 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.20819511143176744\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5317507829371985\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.208\n","Validation Loss: 0.532\n","-----------------------\n","|Epoch 7 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.20246929970433872\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.531449584508097\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.202\n","Validation Loss: 0.531\n","*************************************************************************************************************************************\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","yayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaay!!!!!\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","*************************************************************************************************************************************\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-08-10 03:56:34,927] Trial 0 finished with value: 0.824430823117338 and parameters: {'lr': 9.302097388424848e-07, 'weight_decay': 0.00013791671922193954, 'dropout': 0.17453683286128835, 'beta1': 0.9210256191989711, 'beta2': 0.9201666680763443, 'factor': 0.1353644965826617, 'scheduler_patience': 1, 'threshold': 9.336043816286001e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 50, 'batch_size': 30, 'epochs': 7, 'activation': 'gelu'}. Best is trial 0 with value: 0.824430823117338.\n"]},{"name":"stdout","output_type":"stream","text":["TRAIN\n","              precision    recall  f1-score   support\n","\n","           0       0.57      0.59      0.58      3039\n","           1       0.43      0.41      0.42      2290\n","\n","    accuracy                           0.51      5329\n","   macro avg       0.50      0.50      0.50      5329\n","weighted avg       0.51      0.51      0.51      5329\n","\n","EXACT ACCURACY: 51.49%\n","VALIDATION\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.86      0.85      1303\n","           1       0.81      0.78      0.79       981\n","\n","    accuracy                           0.82      2284\n","   macro avg       0.82      0.82      0.82      2284\n","weighted avg       0.82      0.82      0.82      2284\n","\n","EXACT ACCURACY: 82.44%\n","7\n","\n","==================================================\n","lr = 9.994934257120996e-08\n","weight_decay = 0.00010311419788530171\n","dropout = 0.3677665344039642\n","beta1 = 0.9192430563905795\n","beta2 = 0.9603332280835974\n","factor = 0.1529361017339175\n","scheduler_patience = 2\n","threshold = 4.714679297075664e-05\n","cooldown = 0\n","min_lr = 0\n","max_seq_len = 50\n","batch_size = 31\n","epochs = 7\n","activation = tanh\n","==================================================\n","\n","-----------------------\n","|Epoch 1 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.6913019120693207\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.657882617937552\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.691\n","Validation Loss: 0.658\n","-----------------------\n","|Epoch 2 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.6241109443958416\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.6145331585729444\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.624\n","Validation Loss: 0.615\n","-----------------------\n","|Epoch 3 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5648065138348314\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5773159507158641\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.565\n","Validation Loss: 0.577\n","-----------------------\n","|Epoch 4 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5102212986973829\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5447952860110515\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.510\n","Validation Loss: 0.545\n","-----------------------\n","|Epoch 5 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.46294394291417545\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5180420114381893\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.463\n","Validation Loss: 0.518\n","-----------------------\n","|Epoch 6 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.41969908184783405\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.4978778708625484\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.420\n","Validation Loss: 0.498\n","-----------------------\n","|Epoch 7 / 7|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.3810319018571876\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.48159491331190674\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.381\n","Validation Loss: 0.482\n","*************************************************************************************************************************************\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","yayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaay!!!!!\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","*************************************************************************************************************************************\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-08-10 03:58:47,132] Trial 1 finished with value: 0.8209281961471103 and parameters: {'lr': 9.994934257120996e-08, 'weight_decay': 0.00010311419788530171, 'dropout': 0.3677665344039642, 'beta1': 0.9192430563905795, 'beta2': 0.9603332280835974, 'factor': 0.1529361017339175, 'scheduler_patience': 2, 'threshold': 4.714679297075664e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 50, 'batch_size': 31, 'epochs': 7, 'activation': 'tanh'}. Best is trial 0 with value: 0.824430823117338.\n"]},{"name":"stdout","output_type":"stream","text":["TRAIN\n","              precision    recall  f1-score   support\n","\n","           0       0.57      0.59      0.58      3039\n","           1       0.44      0.42      0.43      2290\n","\n","    accuracy                           0.52      5329\n","   macro avg       0.50      0.50      0.50      5329\n","weighted avg       0.51      0.52      0.52      5329\n","\n","EXACT ACCURACY: 51.68%\n","VALIDATION\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.85      0.84      1303\n","           1       0.80      0.78      0.79       981\n","\n","    accuracy                           0.82      2284\n","   macro avg       0.82      0.82      0.82      2284\n","weighted avg       0.82      0.82      0.82      2284\n","\n","EXACT ACCURACY: 82.09%\n","8\n","\n","==================================================\n","lr = 4.759723151173136e-07\n","weight_decay = 0.0001980345942631461\n","dropout = 0.2161726541259594\n","beta1 = 0.8551075774798119\n","beta2 = 0.9862558407084394\n","factor = 0.11531547932051993\n","scheduler_patience = 1\n","threshold = 6.802548166017495e-05\n","cooldown = 1\n","min_lr = 0\n","max_seq_len = 45\n","batch_size = 30\n","epochs = 8\n","activation = gelu\n","==================================================\n","\n","-----------------------\n","|Epoch 1 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5277404143904032\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5244226811768172\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.528\n","Validation Loss: 0.524\n","-----------------------\n","|Epoch 2 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.3905009300856108\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.47076524929566815\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.391\n","Validation Loss: 0.471\n","-----------------------\n","|Epoch 3 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.3077548084299216\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.4525204271852196\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.308\n","Validation Loss: 0.453\n","-----------------------\n","|Epoch 4 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.257430763559395\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.46171149824346813\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.257\n","Validation Loss: 0.462\n","-----------------------\n","|Epoch 5 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.2349843497524101\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.46969921093482475\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.235\n","Validation Loss: 0.470\n","-----------------------\n","|Epoch 6 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.21425671724790937\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.4884811384337289\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.214\n","Validation Loss: 0.488\n","-----------------------\n","|Epoch 7 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.2032822301883376\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5181202206325222\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.203\n","Validation Loss: 0.518\n","-----------------------\n","|Epoch 8 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|164|166|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","168|170|172|174|176|178|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.19856505229901733\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.5146219714895471\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Loss: 0.199\n","Validation Loss: 0.515\n","*************************************************************************************************************************************\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","yayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaayyayayayayayayaay!!!!!\n",">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n","*************************************************************************************************************************************\n","\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-08-10 04:01:10,818] Trial 2 finished with value: 0.8213660245183888 and parameters: {'lr': 4.759723151173136e-07, 'weight_decay': 0.0001980345942631461, 'dropout': 0.2161726541259594, 'beta1': 0.8551075774798119, 'beta2': 0.9862558407084394, 'factor': 0.11531547932051993, 'scheduler_patience': 1, 'threshold': 6.802548166017495e-05, 'cooldown': 1, 'min_lr': 0, 'max_seq_len': 45, 'batch_size': 30, 'epochs': 8, 'activation': 'gelu'}. Best is trial 0 with value: 0.824430823117338.\n"]},{"name":"stdout","output_type":"stream","text":["TRAIN\n","              precision    recall  f1-score   support\n","\n","           0       0.57      0.59      0.58      3039\n","           1       0.43      0.41      0.42      2290\n","\n","    accuracy                           0.51      5329\n","   macro avg       0.50      0.50      0.50      5329\n","weighted avg       0.51      0.51      0.51      5329\n","\n","EXACT ACCURACY: 51.00%\n","VALIDATION\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.85      0.84      1303\n","           1       0.80      0.78      0.79       981\n","\n","    accuracy                           0.82      2284\n","   macro avg       0.82      0.82      0.82      2284\n","weighted avg       0.82      0.82      0.82      2284\n","\n","EXACT ACCURACY: 82.14%\n","8\n","\n","==================================================\n","lr = 6.132583788602921e-08\n","weight_decay = 0.0004099325877802864\n","dropout = 0.3269552137665457\n","beta1 = 0.9194335626924197\n","beta2 = 0.9140563788258107\n","factor = 0.4558728548228317\n","scheduler_patience = 1\n","threshold = 0.0001487171904249414\n","cooldown = 0\n","min_lr = 0\n","max_seq_len = 54\n","batch_size = 33\n","epochs = 8\n","activation = leaky_relu\n","==================================================\n","\n","-----------------------\n","|Epoch 1 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.6700740416108826\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.6638866697038923\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.670\n","Validation Loss: 0.664\n","-----------------------\n","|Epoch 2 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.6419788852885917\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.6441603899002075\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.642\n","Validation Loss: 0.644\n","-----------------------\n","|Epoch 3 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.6148091249259902\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.625357529095241\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.615\n","Validation Loss: 0.625\n","-----------------------\n","|Epoch 4 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5892038621284343\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Evaluation Metrics:\n","Average Loss: 0.6076160260609218\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","-----------------------\n","Training Loss: 0.589\n","Validation Loss: 0.608\n","-----------------------\n","|Epoch 5 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|84|86|88|90|92|94|96|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","98|100|102|104|106|108|110|112|114|116|118|120|122|124|126|128|130|132|134|136|138|140|142|144|146|148|150|152|154|156|158|160|162|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Training Metrics:\n","Average Loss: 0.5655424300535226\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","\n","                                                            EVALUATION HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|"]}],"source":["tuned_model= tune_model_with_optuna_accuracy(dfTrain, tokenizer, device, bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9078,"status":"error","timestamp":1691639631710,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"outputId":"29958c46-a5d1-4c2b-c251-4a9ca963fc9d","id":"lxZwidngcDAA"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-08-10 03:53:42,998] A new study created in memory with name: no-name-d6e34821-b905-4067-8933-1ac65cca928a\n"]},{"name":"stdout","output_type":"stream","text":["8\n","\n","==================================================\n","lr = 4.356891820578161e-08\n","weight_decay = 0.00037643539181739336\n","dropout = 0.33918137922431635\n","beta1 = 0.9058788147352017\n","beta2 = 0.9212058651822946\n","factor = 0.33930565814703967\n","scheduler_patience = 1\n","threshold = 2.8033397266812092e-05\n","cooldown = 0\n","min_lr = 0\n","max_seq_len = 45\n","batch_size = 32\n","epochs = 8\n","activation = tanh\n","==================================================\n","\n","-----------------------\n","|Epoch 1 / 8|\n","-----------------------\n","                                                            TRAINING HAS STARTED\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","2|4|6|8|10|12|14|16|18|20|22|24|26|28|30|32|34|36|38|40|42|44|46|48|50|52|54|56|58|60|62|64|66|68|70|72|74|76|78|80|82|"]},{"name":"stderr","output_type":"stream","text":["[W 2023-08-10 03:53:51,262] Trial 0 failed with parameters: {'lr': 4.356891820578161e-08, 'weight_decay': 0.00037643539181739336, 'dropout': 0.33918137922431635, 'beta1': 0.9058788147352017, 'beta2': 0.9212058651822946, 'factor': 0.33930565814703967, 'scheduler_patience': 1, 'threshold': 2.8033397266812092e-05, 'cooldown': 0, 'min_lr': 0, 'max_seq_len': 45, 'batch_size': 32, 'epochs': 8, 'activation': 'tanh'} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-37-bf545917125e>\", line 268, in <lambda>\n","    study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device, bert), n_trials=10)\n","  File \"<ipython-input-37-bf545917125e>\", line 211, in objective\n","    train_loss, _ = train(model, optimizer, train_dataloader, device, cross_entropy)  # Passing cross_entropy\n","  File \"<ipython-input-37-bf545917125e>\", line 44, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","[W 2023-08-10 03:53:51,268] Trial 0 failed with value None.\n"]},{"name":"stdout","output_type":"stream","text":["84|"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-6c943e130224>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtune_model_with_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-bf545917125e>\u001b[0m in \u001b[0;36mtune_model_with_optuna\u001b[0;34m(train_dataloader, val_dataloader, device, bert)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_model_with_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed to maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-bf545917125e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_model_with_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed to maximize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-bf545917125e>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, dfTrain, tokenizer, device, bert)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Passing cross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Passing cross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-bf545917125e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, cross_entropy)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["tuned_model= tune_model_with_optuna(dfTrain, tokenizer, device, bert)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","toc_visible":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNLfhhEWZB0V9RsCikcywfd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d6c76eabfba94b5691509c7bd9b7bb25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_619573801d5f483d8e3d8947926ee61d","IPY_MODEL_8f91036e684a44579515b12b2c26dcc4","IPY_MODEL_181e5944864c4eb8b14197ab970a2175"],"layout":"IPY_MODEL_ef9059ccf28d44b8a82ea581852b7ed9"}},"619573801d5f483d8e3d8947926ee61d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a5a8b904ab4a8da9e5d12c71b7b396","placeholder":"​","style":"IPY_MODEL_0ee8672513ce457cb43072dacc8f0cde","value":"Downloading (…)lve/main/config.json: 100%"}},"8f91036e684a44579515b12b2c26dcc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9da37ff7a24f48aa884a67a304658f82","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fec1473236bf4201a3a7ea13132bbcb5","value":570}},"181e5944864c4eb8b14197ab970a2175":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed1971c7a32d46a899d1dfa90c4399d8","placeholder":"​","style":"IPY_MODEL_61cd79e6be1345b6beec1ce47b489556","value":" 570/570 [00:00&lt;00:00, 27.7kB/s]"}},"ef9059ccf28d44b8a82ea581852b7ed9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4a5a8b904ab4a8da9e5d12c71b7b396":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ee8672513ce457cb43072dacc8f0cde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9da37ff7a24f48aa884a67a304658f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fec1473236bf4201a3a7ea13132bbcb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed1971c7a32d46a899d1dfa90c4399d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61cd79e6be1345b6beec1ce47b489556":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb2e9dbdf56b4c9d82bab104c73657af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f49146d67a447afa61741c9cf2689de","IPY_MODEL_c6d190b83271400ebdb03de157afd8dd","IPY_MODEL_2b2977ce27a3423c98ffebe15ada9f85"],"layout":"IPY_MODEL_69e8240fb2144daf899acd96890b8257"}},"7f49146d67a447afa61741c9cf2689de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e601e48953d449dad18425f296e3480","placeholder":"​","style":"IPY_MODEL_9dcb4e0be0b04f68a146277d6d6cdb5d","value":"Downloading model.safetensors: 100%"}},"c6d190b83271400ebdb03de157afd8dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80150b07b2d644c3aa48cc8055edca4b","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d7db229b5c445769f98a506c1aa0f93","value":440449768}},"2b2977ce27a3423c98ffebe15ada9f85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70a0605ec2d64bd7a5722eb8b9f30c91","placeholder":"​","style":"IPY_MODEL_885dee8f5d874c36997174379d442b8e","value":" 440M/440M [00:01&lt;00:00, 237MB/s]"}},"69e8240fb2144daf899acd96890b8257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e601e48953d449dad18425f296e3480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dcb4e0be0b04f68a146277d6d6cdb5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80150b07b2d644c3aa48cc8055edca4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d7db229b5c445769f98a506c1aa0f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70a0605ec2d64bd7a5722eb8b9f30c91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"885dee8f5d874c36997174379d442b8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f283bbaa28824a40b1e2270f741df99a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ccbcc9551e04e95b6de2b668c1c753b","IPY_MODEL_4b62f362c0c243528aab06ee5c098b0a","IPY_MODEL_96011023ee5a457f91c04df48171dccb"],"layout":"IPY_MODEL_512dacf3a9e54a478a8e402749e7c5f6"}},"0ccbcc9551e04e95b6de2b668c1c753b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32f8074bc2841d8bdf07428baf676c9","placeholder":"​","style":"IPY_MODEL_010608b326364dc18a737338f0299bd1","value":"Downloading (…)okenizer_config.json: 100%"}},"4b62f362c0c243528aab06ee5c098b0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c0f7874c7af4654ad97e1cffceba4dd","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd3146c37b084ca794e76b6c8456d3b3","value":28}},"96011023ee5a457f91c04df48171dccb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e46a2fbf626a483e8a4bce1f981b3167","placeholder":"​","style":"IPY_MODEL_85a1b04791c64d4db58f2fa926d4205f","value":" 28.0/28.0 [00:00&lt;00:00, 1.33kB/s]"}},"512dacf3a9e54a478a8e402749e7c5f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b32f8074bc2841d8bdf07428baf676c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010608b326364dc18a737338f0299bd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c0f7874c7af4654ad97e1cffceba4dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd3146c37b084ca794e76b6c8456d3b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e46a2fbf626a483e8a4bce1f981b3167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85a1b04791c64d4db58f2fa926d4205f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1390ae759f604b67975ee16a745f5c79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6addcfb19e354d7badbddf26a862594a","IPY_MODEL_e07f390a032c43669cc24c708dda19af","IPY_MODEL_0c65cb47d4274466a369478f13167b07"],"layout":"IPY_MODEL_01d9ff85dc1c4c3bb987bc301d913c4a"}},"6addcfb19e354d7badbddf26a862594a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ea3e2ef454148c1bb80daf778c8b85e","placeholder":"​","style":"IPY_MODEL_6c20c9e454bf45dd97bec822dda0c4b6","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"e07f390a032c43669cc24c708dda19af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_693ccb1c34f2432c9209f5003e84dad0","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c613e026785c46e3b3c9f41d36a2620e","value":231508}},"0c65cb47d4274466a369478f13167b07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98175044d1864441a4f5d9ebe6924cdd","placeholder":"​","style":"IPY_MODEL_0001fa6a3d284650b4b52fa2d43a4d24","value":" 232k/232k [00:00&lt;00:00, 3.21MB/s]"}},"01d9ff85dc1c4c3bb987bc301d913c4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ea3e2ef454148c1bb80daf778c8b85e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c20c9e454bf45dd97bec822dda0c4b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"693ccb1c34f2432c9209f5003e84dad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c613e026785c46e3b3c9f41d36a2620e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98175044d1864441a4f5d9ebe6924cdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0001fa6a3d284650b4b52fa2d43a4d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1acea491a86e457aa3f5ab9396c8de79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b31ac08ae9d8411eb79566eb9a85f56a","IPY_MODEL_a3448ae203c5436d900f9d01c2bb1e83","IPY_MODEL_b9aaf2fc0a76405f8631b37b05fc82f7"],"layout":"IPY_MODEL_9eb2ba35db074cc6ae181c9011341d2d"}},"b31ac08ae9d8411eb79566eb9a85f56a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_370de771db6d4bbaa8129998afb47ef0","placeholder":"​","style":"IPY_MODEL_a925712b828241ccacef1e95b3337aba","value":"Downloading (…)/main/tokenizer.json: 100%"}},"a3448ae203c5436d900f9d01c2bb1e83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d769bcfde7e422a993484924347ddf0","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7adcbde30082482397ce6f48829ad2ee","value":466062}},"b9aaf2fc0a76405f8631b37b05fc82f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68fbf12126844d2f913fa28c608bb896","placeholder":"​","style":"IPY_MODEL_e0a0033fc35d4610a3d0731527ad2256","value":" 466k/466k [00:00&lt;00:00, 3.58MB/s]"}},"9eb2ba35db074cc6ae181c9011341d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"370de771db6d4bbaa8129998afb47ef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a925712b828241ccacef1e95b3337aba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d769bcfde7e422a993484924347ddf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7adcbde30082482397ce6f48829ad2ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68fbf12126844d2f913fa28c608bb896":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0a0033fc35d4610a3d0731527ad2256":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}