{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qnEbOIfl4E3U"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sNwjfm0KzDM0"},"source":["#setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9423,"status":"ok","timestamp":1692038585883,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"2sPQJvvKn9wh","outputId":"bfe1d343-717c-4c5e-f20a-769b74b33407"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install transformers\n","\n","# Imports\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","import pandas as pd\n","\n","\n","from scipy.sparse import hstack\n","from scipy.sparse import csr_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXWODo9-0hdE"},"outputs":[],"source":["import warnings\n","# Filter warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5340,"status":"ok","timestamp":1692038591219,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"NgsGDmoPyK0V","outputId":"d6a31052-cbdc-47c3-d80b-195c13dc35b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.3.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.2)\n","Requirement already satisfied: cmaes>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.10.0)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a4vdrsFMBYj"},"outputs":[],"source":["import optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdXjWS97oUyW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import BertTokenizerFast, BertModel, AdamW\n","import numpy as np\n","from imblearn.over_sampling import RandomOverSampler\n","from collections import Counter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iGOehQE_V3o"},"outputs":[],"source":["torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wIIVdl_orcb"},"outputs":[],"source":["# Reading data\n","folder_path = '/content/drive/MyDrive/datasets/'\n","dfTrain = pd.read_csv(folder_path + 'trainR.csv')\n","dfTest = pd.read_csv(folder_path + 'testR.csv')\n","\n","dfTrain.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)\n","\n","dfTest.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1692038596052,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"bVmI0xjXGv4i","outputId":"ea3e91f8-fae2-491c-e797-2e7faba2d864"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 79023 entries, 0 to 79022\n","Data columns (total 75 columns):\n"," #   Column                                                    Non-Null Count  Dtype  \n","---  ------                                                    --------------  -----  \n"," 0   latitude                                                  79023 non-null  float64\n"," 1   longitude                                                 79023 non-null  float64\n"," 2   year                                                      79023 non-null  int64  \n"," 3   week_no                                                   79023 non-null  int64  \n"," 4   SulphurDioxide_SO2_column_number_density                  64414 non-null  float64\n"," 5   SulphurDioxide_SO2_column_number_density_amf              64414 non-null  float64\n"," 6   SulphurDioxide_SO2_slant_column_number_density            64414 non-null  float64\n"," 7   SulphurDioxide_cloud_fraction                             64414 non-null  float64\n"," 8   SulphurDioxide_sensor_azimuth_angle                       64414 non-null  float64\n"," 9   SulphurDioxide_sensor_zenith_angle                        64414 non-null  float64\n"," 10  SulphurDioxide_solar_azimuth_angle                        64414 non-null  float64\n"," 11  SulphurDioxide_solar_zenith_angle                         64414 non-null  float64\n"," 12  SulphurDioxide_SO2_column_number_density_15km             64414 non-null  float64\n"," 13  CarbonMonoxide_CO_column_number_density                   76901 non-null  float64\n"," 14  CarbonMonoxide_H2O_column_number_density                  76901 non-null  float64\n"," 15  CarbonMonoxide_cloud_height                               76901 non-null  float64\n"," 16  CarbonMonoxide_sensor_altitude                            76901 non-null  float64\n"," 17  CarbonMonoxide_sensor_azimuth_angle                       76901 non-null  float64\n"," 18  CarbonMonoxide_sensor_zenith_angle                        76901 non-null  float64\n"," 19  CarbonMonoxide_solar_azimuth_angle                        76901 non-null  float64\n"," 20  CarbonMonoxide_solar_zenith_angle                         76901 non-null  float64\n"," 21  NitrogenDioxide_NO2_column_number_density                 60703 non-null  float64\n"," 22  NitrogenDioxide_tropospheric_NO2_column_number_density    60703 non-null  float64\n"," 23  NitrogenDioxide_stratospheric_NO2_column_number_density   60703 non-null  float64\n"," 24  NitrogenDioxide_NO2_slant_column_number_density           60703 non-null  float64\n"," 25  NitrogenDioxide_tropopause_pressure                       60703 non-null  float64\n"," 26  NitrogenDioxide_absorbing_aerosol_index                   60703 non-null  float64\n"," 27  NitrogenDioxide_cloud_fraction                            60703 non-null  float64\n"," 28  NitrogenDioxide_sensor_altitude                           60703 non-null  float64\n"," 29  NitrogenDioxide_sensor_azimuth_angle                      60703 non-null  float64\n"," 30  NitrogenDioxide_sensor_zenith_angle                       60703 non-null  float64\n"," 31  NitrogenDioxide_solar_azimuth_angle                       60703 non-null  float64\n"," 32  NitrogenDioxide_solar_zenith_angle                        60703 non-null  float64\n"," 33  Formaldehyde_tropospheric_HCHO_column_number_density      71746 non-null  float64\n"," 34  Formaldehyde_tropospheric_HCHO_column_number_density_amf  71746 non-null  float64\n"," 35  Formaldehyde_HCHO_slant_column_number_density             71746 non-null  float64\n"," 36  Formaldehyde_cloud_fraction                               71746 non-null  float64\n"," 37  Formaldehyde_solar_zenith_angle                           71746 non-null  float64\n"," 38  Formaldehyde_solar_azimuth_angle                          71746 non-null  float64\n"," 39  Formaldehyde_sensor_zenith_angle                          71746 non-null  float64\n"," 40  Formaldehyde_sensor_azimuth_angle                         71746 non-null  float64\n"," 41  UvAerosolIndex_absorbing_aerosol_index                    78484 non-null  float64\n"," 42  UvAerosolIndex_sensor_altitude                            78484 non-null  float64\n"," 43  UvAerosolIndex_sensor_azimuth_angle                       78484 non-null  float64\n"," 44  UvAerosolIndex_sensor_zenith_angle                        78484 non-null  float64\n"," 45  UvAerosolIndex_solar_azimuth_angle                        78484 non-null  float64\n"," 46  UvAerosolIndex_solar_zenith_angle                         78484 non-null  float64\n"," 47  Ozone_O3_column_number_density                            78475 non-null  float64\n"," 48  Ozone_O3_column_number_density_amf                        78475 non-null  float64\n"," 49  Ozone_O3_slant_column_number_density                      78475 non-null  float64\n"," 50  Ozone_O3_effective_temperature                            78475 non-null  float64\n"," 51  Ozone_cloud_fraction                                      78475 non-null  float64\n"," 52  Ozone_sensor_azimuth_angle                                78475 non-null  float64\n"," 53  Ozone_sensor_zenith_angle                                 78475 non-null  float64\n"," 54  Ozone_solar_azimuth_angle                                 78475 non-null  float64\n"," 55  Ozone_solar_zenith_angle                                  78475 non-null  float64\n"," 56  UvAerosolLayerHeight_aerosol_height                       439 non-null    float64\n"," 57  UvAerosolLayerHeight_aerosol_pressure                     439 non-null    float64\n"," 58  UvAerosolLayerHeight_aerosol_optical_depth                439 non-null    float64\n"," 59  UvAerosolLayerHeight_sensor_zenith_angle                  439 non-null    float64\n"," 60  UvAerosolLayerHeight_sensor_azimuth_angle                 439 non-null    float64\n"," 61  UvAerosolLayerHeight_solar_azimuth_angle                  439 non-null    float64\n"," 62  UvAerosolLayerHeight_solar_zenith_angle                   439 non-null    float64\n"," 63  Cloud_cloud_fraction                                      78539 non-null  float64\n"," 64  Cloud_cloud_top_pressure                                  78539 non-null  float64\n"," 65  Cloud_cloud_top_height                                    78539 non-null  float64\n"," 66  Cloud_cloud_base_pressure                                 78539 non-null  float64\n"," 67  Cloud_cloud_base_height                                   78539 non-null  float64\n"," 68  Cloud_cloud_optical_depth                                 78539 non-null  float64\n"," 69  Cloud_surface_albedo                                      78539 non-null  float64\n"," 70  Cloud_sensor_azimuth_angle                                78539 non-null  float64\n"," 71  Cloud_sensor_zenith_angle                                 78539 non-null  float64\n"," 72  Cloud_solar_azimuth_angle                                 78539 non-null  float64\n"," 73  Cloud_solar_zenith_angle                                  78539 non-null  float64\n"," 74  emission                                                  79023 non-null  float64\n","dtypes: float64(73), int64(2)\n","memory usage: 45.2 MB\n"]}],"source":["dfTrain.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1692038596052,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"tRnp27hgboe4","outputId":"b0c1ae4b-412e-4739-ff3f-90a9a6d6bb15"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       latitude  longitude  year  week_no  \\\n","0        -0.510     29.290  2022        0   \n","1        -0.510     29.290  2022        1   \n","2        -0.510     29.290  2022        2   \n","3        -0.510     29.290  2022        3   \n","4        -0.510     29.290  2022        4   \n","...         ...        ...   ...      ...   \n","24348    -3.299     30.301  2022       44   \n","24349    -3.299     30.301  2022       45   \n","24350    -3.299     30.301  2022       46   \n","24351    -3.299     30.301  2022       47   \n","24352    -3.299     30.301  2022       48   \n","\n","       SulphurDioxide_SO2_column_number_density  \\\n","0                                           NaN   \n","1                                      0.000456   \n","2                                      0.000161   \n","3                                      0.000350   \n","4                                     -0.000317   \n","...                                         ...   \n","24348                                 -0.000618   \n","24349                                       NaN   \n","24350                                       NaN   \n","24351                                  0.000071   \n","24352                                       NaN   \n","\n","       SulphurDioxide_SO2_column_number_density_amf  \\\n","0                                               NaN   \n","1                                          0.691164   \n","2                                          0.605107   \n","3                                          0.696917   \n","4                                          0.580527   \n","...                                             ...   \n","24348                                      0.745549   \n","24349                                           NaN   \n","24350                                           NaN   \n","24351                                      1.003805   \n","24352                                           NaN   \n","\n","       SulphurDioxide_SO2_slant_column_number_density  \\\n","0                                                 NaN   \n","1                                            0.000316   \n","2                                            0.000106   \n","3                                            0.000243   \n","4                                           -0.000184   \n","...                                               ...   \n","24348                                       -0.000461   \n","24349                                             NaN   \n","24350                                             NaN   \n","24351                                        0.000077   \n","24352                                             NaN   \n","\n","       SulphurDioxide_cloud_fraction  SulphurDioxide_sensor_azimuth_angle  \\\n","0                                NaN                                  NaN   \n","1                           0.000000                            76.239196   \n","2                           0.079870                           -42.055341   \n","3                           0.201028                            72.169566   \n","4                           0.204352                            76.190865   \n","...                              ...                                  ...   \n","24348                       0.234492                            72.306198   \n","24349                            NaN                                  NaN   \n","24350                            NaN                                  NaN   \n","24351                       0.205077                            74.327427   \n","24352                            NaN                                  NaN   \n","\n","       SulphurDioxide_sensor_zenith_angle  ...  Cloud_cloud_top_pressure  \\\n","0                                     NaN  ...              36022.027344   \n","1                               15.600607  ...              48539.737242   \n","2                               39.889060  ...              34133.080469   \n","3                               58.862543  ...              50854.991076   \n","4                               15.646016  ...              46594.685145   \n","...                                   ...  ...                       ...   \n","24348                           61.114494  ...              48839.430415   \n","24349                                 NaN  ...              47042.694849   \n","24350                                 NaN  ...              55337.148173   \n","24351                           38.215228  ...              44813.691428   \n","24352                                 NaN  ...              40528.702057   \n","\n","       Cloud_cloud_top_height  Cloud_cloud_base_pressure  \\\n","0                 8472.313477               41047.937500   \n","1                 6476.147323               54915.708579   \n","2                 8984.795703               39006.093750   \n","3                 6014.724059               57646.368368   \n","4                 6849.280477               52896.541873   \n","...                       ...                        ...   \n","24348             6260.120033               55483.459980   \n","24349             6678.843299               53589.917383   \n","24350             5336.282475               62646.761340   \n","24351             7188.578533               50728.313991   \n","24352             7777.863638               46260.039092   \n","\n","       Cloud_cloud_base_height  Cloud_cloud_optical_depth  \\\n","0                  7472.313477                   7.935617   \n","1                  5476.147161                  11.448437   \n","2                  7984.795703                  10.753179   \n","3                  5014.724115                  11.764556   \n","4                  5849.280394                  13.065317   \n","...                        ...                        ...   \n","24348              5260.120056                  30.398508   \n","24349              5678.951521                  19.223844   \n","24350              4336.282491                  13.801194   \n","24351              6188.578464                  27.887489   \n","24352              6777.863819                  23.771269   \n","\n","       Cloud_surface_albedo  Cloud_sensor_azimuth_angle  \\\n","0                  0.240773                 -100.113792   \n","1                  0.293119                  -30.510319   \n","2                  0.267130                   39.087361   \n","3                  0.304679                  -24.465127   \n","4                  0.284221                  -12.907850   \n","...                     ...                         ...   \n","24348              0.180046                  -25.528588   \n","24349              0.177833                  -13.380005   \n","24350              0.219471                   -5.072065   \n","24351              0.247275                   -0.668714   \n","24352              0.239684                  -40.826139   \n","\n","       Cloud_sensor_zenith_angle  Cloud_solar_azimuth_angle  \\\n","0                      33.697044                -133.047546   \n","1                      42.402593                -138.632822   \n","2                      45.936480                -144.784988   \n","3                      42.140419                -135.027891   \n","4                      30.122641                -135.500119   \n","...                          ...                        ...   \n","24348                  45.284576                -116.521412   \n","24349                  43.770351                -122.405759   \n","24350                  33.226455                -124.530639   \n","24351                  45.885617                -129.006797   \n","24352                  30.680056                -124.895473   \n","\n","       Cloud_solar_zenith_angle  \n","0                     33.779583  \n","1                     31.012380  \n","2                     26.743361  \n","3                     29.604774  \n","4                     26.276807  \n","...                         ...  \n","24348                 29.992562  \n","24349                 29.017975  \n","24350                 30.187472  \n","24351                 30.427455  \n","24352                 34.457720  \n","\n","[24353 rows x 74 columns]"],"text/html":["\n","\n","  <div id=\"df-a14953e3-70ce-45dc-a4db-66a46efccfce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>year</th>\n","      <th>week_no</th>\n","      <th>SulphurDioxide_SO2_column_number_density</th>\n","      <th>SulphurDioxide_SO2_column_number_density_amf</th>\n","      <th>SulphurDioxide_SO2_slant_column_number_density</th>\n","      <th>SulphurDioxide_cloud_fraction</th>\n","      <th>SulphurDioxide_sensor_azimuth_angle</th>\n","      <th>SulphurDioxide_sensor_zenith_angle</th>\n","      <th>...</th>\n","      <th>Cloud_cloud_top_pressure</th>\n","      <th>Cloud_cloud_top_height</th>\n","      <th>Cloud_cloud_base_pressure</th>\n","      <th>Cloud_cloud_base_height</th>\n","      <th>Cloud_cloud_optical_depth</th>\n","      <th>Cloud_surface_albedo</th>\n","      <th>Cloud_sensor_azimuth_angle</th>\n","      <th>Cloud_sensor_zenith_angle</th>\n","      <th>Cloud_solar_azimuth_angle</th>\n","      <th>Cloud_solar_zenith_angle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.510</td>\n","      <td>29.290</td>\n","      <td>2022</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>36022.027344</td>\n","      <td>8472.313477</td>\n","      <td>41047.937500</td>\n","      <td>7472.313477</td>\n","      <td>7.935617</td>\n","      <td>0.240773</td>\n","      <td>-100.113792</td>\n","      <td>33.697044</td>\n","      <td>-133.047546</td>\n","      <td>33.779583</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.510</td>\n","      <td>29.290</td>\n","      <td>2022</td>\n","      <td>1</td>\n","      <td>0.000456</td>\n","      <td>0.691164</td>\n","      <td>0.000316</td>\n","      <td>0.000000</td>\n","      <td>76.239196</td>\n","      <td>15.600607</td>\n","      <td>...</td>\n","      <td>48539.737242</td>\n","      <td>6476.147323</td>\n","      <td>54915.708579</td>\n","      <td>5476.147161</td>\n","      <td>11.448437</td>\n","      <td>0.293119</td>\n","      <td>-30.510319</td>\n","      <td>42.402593</td>\n","      <td>-138.632822</td>\n","      <td>31.012380</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.510</td>\n","      <td>29.290</td>\n","      <td>2022</td>\n","      <td>2</td>\n","      <td>0.000161</td>\n","      <td>0.605107</td>\n","      <td>0.000106</td>\n","      <td>0.079870</td>\n","      <td>-42.055341</td>\n","      <td>39.889060</td>\n","      <td>...</td>\n","      <td>34133.080469</td>\n","      <td>8984.795703</td>\n","      <td>39006.093750</td>\n","      <td>7984.795703</td>\n","      <td>10.753179</td>\n","      <td>0.267130</td>\n","      <td>39.087361</td>\n","      <td>45.936480</td>\n","      <td>-144.784988</td>\n","      <td>26.743361</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.510</td>\n","      <td>29.290</td>\n","      <td>2022</td>\n","      <td>3</td>\n","      <td>0.000350</td>\n","      <td>0.696917</td>\n","      <td>0.000243</td>\n","      <td>0.201028</td>\n","      <td>72.169566</td>\n","      <td>58.862543</td>\n","      <td>...</td>\n","      <td>50854.991076</td>\n","      <td>6014.724059</td>\n","      <td>57646.368368</td>\n","      <td>5014.724115</td>\n","      <td>11.764556</td>\n","      <td>0.304679</td>\n","      <td>-24.465127</td>\n","      <td>42.140419</td>\n","      <td>-135.027891</td>\n","      <td>29.604774</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.510</td>\n","      <td>29.290</td>\n","      <td>2022</td>\n","      <td>4</td>\n","      <td>-0.000317</td>\n","      <td>0.580527</td>\n","      <td>-0.000184</td>\n","      <td>0.204352</td>\n","      <td>76.190865</td>\n","      <td>15.646016</td>\n","      <td>...</td>\n","      <td>46594.685145</td>\n","      <td>6849.280477</td>\n","      <td>52896.541873</td>\n","      <td>5849.280394</td>\n","      <td>13.065317</td>\n","      <td>0.284221</td>\n","      <td>-12.907850</td>\n","      <td>30.122641</td>\n","      <td>-135.500119</td>\n","      <td>26.276807</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24348</th>\n","      <td>-3.299</td>\n","      <td>30.301</td>\n","      <td>2022</td>\n","      <td>44</td>\n","      <td>-0.000618</td>\n","      <td>0.745549</td>\n","      <td>-0.000461</td>\n","      <td>0.234492</td>\n","      <td>72.306198</td>\n","      <td>61.114494</td>\n","      <td>...</td>\n","      <td>48839.430415</td>\n","      <td>6260.120033</td>\n","      <td>55483.459980</td>\n","      <td>5260.120056</td>\n","      <td>30.398508</td>\n","      <td>0.180046</td>\n","      <td>-25.528588</td>\n","      <td>45.284576</td>\n","      <td>-116.521412</td>\n","      <td>29.992562</td>\n","    </tr>\n","    <tr>\n","      <th>24349</th>\n","      <td>-3.299</td>\n","      <td>30.301</td>\n","      <td>2022</td>\n","      <td>45</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>47042.694849</td>\n","      <td>6678.843299</td>\n","      <td>53589.917383</td>\n","      <td>5678.951521</td>\n","      <td>19.223844</td>\n","      <td>0.177833</td>\n","      <td>-13.380005</td>\n","      <td>43.770351</td>\n","      <td>-122.405759</td>\n","      <td>29.017975</td>\n","    </tr>\n","    <tr>\n","      <th>24350</th>\n","      <td>-3.299</td>\n","      <td>30.301</td>\n","      <td>2022</td>\n","      <td>46</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>55337.148173</td>\n","      <td>5336.282475</td>\n","      <td>62646.761340</td>\n","      <td>4336.282491</td>\n","      <td>13.801194</td>\n","      <td>0.219471</td>\n","      <td>-5.072065</td>\n","      <td>33.226455</td>\n","      <td>-124.530639</td>\n","      <td>30.187472</td>\n","    </tr>\n","    <tr>\n","      <th>24351</th>\n","      <td>-3.299</td>\n","      <td>30.301</td>\n","      <td>2022</td>\n","      <td>47</td>\n","      <td>0.000071</td>\n","      <td>1.003805</td>\n","      <td>0.000077</td>\n","      <td>0.205077</td>\n","      <td>74.327427</td>\n","      <td>38.215228</td>\n","      <td>...</td>\n","      <td>44813.691428</td>\n","      <td>7188.578533</td>\n","      <td>50728.313991</td>\n","      <td>6188.578464</td>\n","      <td>27.887489</td>\n","      <td>0.247275</td>\n","      <td>-0.668714</td>\n","      <td>45.885617</td>\n","      <td>-129.006797</td>\n","      <td>30.427455</td>\n","    </tr>\n","    <tr>\n","      <th>24352</th>\n","      <td>-3.299</td>\n","      <td>30.301</td>\n","      <td>2022</td>\n","      <td>48</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>40528.702057</td>\n","      <td>7777.863638</td>\n","      <td>46260.039092</td>\n","      <td>6777.863819</td>\n","      <td>23.771269</td>\n","      <td>0.239684</td>\n","      <td>-40.826139</td>\n","      <td>30.680056</td>\n","      <td>-124.895473</td>\n","      <td>34.457720</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>24353 rows × 74 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a14953e3-70ce-45dc-a4db-66a46efccfce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-e323aedd-6ebc-4cb0-b83d-54be7a8a692a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e323aedd-6ebc-4cb0-b83d-54be7a8a692a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-e323aedd-6ebc-4cb0-b83d-54be7a8a692a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a14953e3-70ce-45dc-a4db-66a46efccfce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a14953e3-70ce-45dc-a4db-66a46efccfce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":9}],"source":["dfTest"]},{"cell_type":"markdown","metadata":{"id":"bivpXVcHqhzh"},"source":["#funcers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njqX-CP8qid4"},"outputs":[],"source":["\n","def get_first_non_nan_sorted(dataset):\n","    total_rows = len(dataset)\n","\n","    # Initialize a list to store the first non-NaN values and their indices\n","    first_non_nan_values = []\n","\n","    for column in dataset.columns:\n","        # Get the column values excluding NaNs\n","        column_values = dataset[column].values\n","        non_nan_indices = np.where(~np.isnan(column_values))[0]\n","\n","        # Check if there are any non-NaN values\n","        if len(non_nan_indices) > 0:\n","            # Retrieve the first non-NaN value and its index\n","            first_non_nan_value = column_values[non_nan_indices[0]]\n","            index_type = type(dataset.index[0])\n","            first_non_nan_index = index_type(dataset.index[non_nan_indices[0]])\n","\n","            # Add the first non-NaN value and its index to the list\n","            first_non_nan_values.append((column, first_non_nan_index, first_non_nan_value))\n","\n","    # Sort the first non-NaN values based on the sorted indices\n","    sorted_first_non_nan_values = sorted(first_non_nan_values, key=lambda x: x[1])\n","\n","    return sorted_first_non_nan_values\n","\n","\n","\n","def plot_columns(dataframe, n=10, figsize=(20, 16)):\n","    # Apply min-max scaling to the selected columns\n","    scaler = MinMaxScaler()\n","    scaled_columns = pd.DataFrame(scaler.fit_transform(dataframe),\n","                                  columns=dataframe.columns,\n","                                  index=dataframe.index)\n","\n","    # Number of graphs\n","    num_graphs = len(dataframe.columns) // n if len(dataframe.columns) % n == 0 else len(dataframe.columns) // n + 1\n","\n","\n","    for k in range(num_graphs):\n","        start = k * n\n","        end = (k + 1) * n if (k + 1) * n < len(dataframe.columns) else len(dataframe.columns)\n","\n","        fig, ax = plt.subplots(figsize=figsize)\n","\n","        # Plot each column as a line\n","        for column in dataframe.columns[start:end]:\n","            ax.plot(scaled_columns.index, scaled_columns[column], label=column)\n","\n","        # Set the x-axis label\n","        ax.set_xlabel('Index')\n","\n","        # Set the y-axis label\n","        ax.set_ylabel('Scaled Value')\n","\n","        # Set the title\n","        ax.set_title(f'Columns Plot {k+1}')\n","\n","        # Add a legend\n","        ax.legend()\n","\n","        # Display the plot\n","        plt.show()\n","def print_nan_zero_percentage(dataset):\n","    total_rows = len(dataset)\n","\n","    # Calculate percentage of NaN and 0 values for each column\n","    nan_zero_percentage = []\n","    for column in dataset.columns:\n","        nan_percentage = dataset[column].isna().sum() / total_rows * 100\n","        zero_percentage = (dataset[column] == 0).sum() / total_rows * 100\n","\n","        nan_zero_percentage.append((column, nan_percentage, zero_percentage))\n","\n","    # Sort the percentages in descending order based on NaN percentage\n","    sorted_nan_zero_percentage = sorted(nan_zero_percentage, key=lambda x: x[1], reverse=True)\n","\n","    # Print the sorted percentages\n","    for column, nan_percentage, zero_percentage in sorted_nan_zero_percentage:\n","        print(f\"Column: {column}\")\n","        print(f\"NaN Percentage: {nan_percentage:.2f}%\")\n","        print(f\"Zero Percentage: {zero_percentage:.2f}%\")\n","        print(\"------------------------\")\n","\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","import numpy as np\n","\n","def plot_yearly(df, start, end, start_date, end_date):\n","    # Plot each column\n","    for column in df.columns[start:end+1]:\n","        # Create a new figure for this column\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","\n","        # For each year\n","        for year in range(pd.to_datetime(start_date).year, pd.to_datetime(end_date).year):\n","            # Get data for this year\n","            start_of_year = max(pd.to_datetime(f\"{year}-01-01\"), pd.to_datetime(start_date))\n","            end_of_year = min(pd.to_datetime(f\"{year+1}-01-01\") - pd.Timedelta(days=1), pd.to_datetime(end_date))\n","            yearly_data = df[(df.index >= start_of_year) & (df.index <= end_of_year)][column]\n","\n","            # Apply min-max scaling to the selected columns\n","            scaler = MinMaxScaler()\n","            yearly_data = pd.Series(scaler.fit_transform(yearly_data.values.reshape(-1, 1)).flatten(),\n","                                    index=yearly_data.index)\n","\n","            # Create a continuous date index and interpolate missing data\n","            all_days = pd.date_range(start=start_of_year, end=end_of_year, freq='D')\n","            yearly_data = yearly_data.reindex(all_days).interpolate(method='time')\n","\n","            # Plot data\n","            ax.plot(yearly_data.index.dayofyear, yearly_data.values, label=year)\n","\n","        # Set labels and title\n","        ax.set_xlabel('Day of Year')\n","        ax.set_ylabel('Scaled Value')\n","        ax.set_title(f'{column} - Yearly Comparison')\n","\n","        # Simplify x-axis to 12 ticks\n","        ax.set_xticks(np.linspace(1, 365, num=12))\n","        ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n","\n","        # Show legend\n","        ax.legend()\n","\n","        # Show the plot\n","        plt.show()\n","\n","import matplotlib.patches as mpatches\n","\n","def visualize_data(df, figsize=(15, 10)):\n","    n = 10  # Number of columns per plot\n","    num_plots = np.ceil(df.shape[1] / n).astype(int)  # Calculate the number of plots needed\n","\n","    for i in range(num_plots):\n","        fig, ax = plt.subplots(figsize=figsize)  # Set the figure size\n","\n","        # Select the columns for this plot\n","        columns = df.columns[i * n: (i + 1) * n]\n","\n","        for col_num, col_name in enumerate(columns):\n","            col_data = df[col_name]\n","\n","            # Iterate over rows and stack them in the bar plot\n","            for row_num, value in enumerate(col_data):\n","                if pd.isna(value):\n","                    color = 'red'\n","                elif value == 0:\n","                    color = 'blue'\n","                else:\n","                    color = 'green'\n","                ax.bar(col_num, 1, bottom=row_num, color=color)\n","\n","        # Set Y-axis labels as row indices for selected rows only\n","        selected_ticks = range(0, len(df.index), len(df.index)//10)  # Select every 10% of data for labeling\n","        ax.set_yticks(selected_ticks)\n","        ax.set_yticklabels(df.index[selected_ticks])\n","\n","        # Set X-axis labels as column names\n","        ax.set_xticks(range(len(columns)))\n","        ax.set_xticklabels(columns)\n","\n","        # Create legend\n","        red_patch = mpatches.Patch(color='red', label='NaN')\n","        blue_patch = mpatches.Patch(color='blue', label='Zero')\n","        green_patch = mpatches.Patch(color='green', label='Non-Zero')\n","        ax.legend(handles=[red_patch, blue_patch, green_patch])\n","\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkRSPbyUmHqI"},"outputs":[],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'testR.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"ID_LAT_LON_YEAR_WEEK\": dfid[\"ID_LAT_LON_YEAR_WEEK\"],\n","        \"emission\": predictions\n","    })\n","\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y58F9EsKEimN"},"outputs":[],"source":["def print_column_summary(df):\n","    for col in df.columns:\n","        unique_values = df[col].nunique()\n","        top_values = df[col].value_counts(dropna=False).head(5).to_dict()\n","        top_values_str = \"///\".join([f'[{str(k)} = {v}]' for k, v in top_values.items()])\n","\n","        print(f'Column: {col} - number of unique values = {unique_values}/ Top 5 {top_values_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rgjBRmW_EPj"},"outputs":[],"source":["import pandas as pd\n","\n","def fill_nans_with_previous(df):\n","    for column in df.columns:\n","        # Fill NaNs with the previous non-NaN values\n","        df[column].fillna(method='ffill', inplace=True)\n","\n","        # If the column starts with NaN values, fill with the next non-NaN value\n","        if pd.isna(df[column].iloc[0]):\n","            df[column].fillna(method='bfill', inplace=True)\n","\n","    return df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5wnkQaaqk-f"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def eda_function(df, target_column,\n","                 figsize_heatmap=(12, 8),\n","                 figsize_corr=(20, 20),\n","                 figsize_hist=(20, 20),\n","                 bins_hist=20,\n","                 figsize_scatter=(20, 20),\n","                 selected_features=None,\n","                 num_hist_per_fig=9,\n","                 num_scatter_per_fig=6,\n","                 num_features_per_heatmap=10,\n","                 num_features_per_pairplot=6):\n","\n","\n","    print('*'*180)\n","    print_nan_zero_percentage(df)\n","    print('*'*180)\n","    # Summary statistics\n","    print(df.describe())\n","\n","    print('*'*180)\n","    print('*'*180)\n","    print()\n","\n","    # Histograms for numerical features\n","    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","    for i in range(0, len(numerical_cols), num_hist_per_fig):\n","        df[numerical_cols[i:i + num_hist_per_fig]].hist(figsize=figsize_hist, bins=bins_hist)\n","        plt.show()\n","        print('-'*180)\n","\n","\n","\n","\n","    print('*'*180)\n","    print('*'*180)\n","    print()\n","\n","    # Scatter plots to observe relationships\n","    for i in range(0, len(numerical_cols) - 1, num_scatter_per_fig):\n","        fig, axes = plt.subplots(1, num_scatter_per_fig, figsize=figsize_scatter, sharey=True)\n","        for j in range(num_scatter_per_fig):\n","            if i + j < len(numerical_cols) - 1:\n","                column = numerical_cols[i + j]\n","                axes[j].scatter(df[column], df[target_column])\n","                axes[j].set_xlabel(column)\n","                axes[j].set_ylabel(target_column)\n","                axes[j].set_title(f\"{column} vs {target_column}\")\n","        plt.show()\n","        print('-'*180)\n","\n","    print('*'*180)\n","    print('*'*180)\n","    print()\n","\n","    # Pairplot for selected features (avoiding all for heavy computational load)\n","\n","    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.drop(target_column)\n","    for i in range(0, len(numerical_columns), num_features_per_pairplot):\n","        selected_features = numerical_columns[i:i+num_features_per_pairplot].to_list()\n","        selected_features.append(target_column)\n","        sns.pairplot(df[selected_features], y_vars=target_column, x_vars=selected_features)\n","        plt.show()\n","\n","def analyze_nans(df):\n","    results = {}\n","    for col in df.columns:\n","        # Count consecutive NaNs\n","        max_consecutive_nans = 0\n","        current_consecutive_nans = 0\n","        # Count total NaNs for average calculation\n","        total_nans = 0\n","\n","        for value in df[col]:\n","            if pd.isna(value):\n","                current_consecutive_nans += 1\n","                total_nans += 1\n","            else:\n","                if current_consecutive_nans > max_consecutive_nans:\n","                    max_consecutive_nans = current_consecutive_nans\n","                current_consecutive_nans = 0\n","\n","        if current_consecutive_nans > max_consecutive_nans:\n","            max_consecutive_nans = current_consecutive_nans\n","\n","        # Calculate average number of NaNs per 365 elements\n","        average_nans_per_365 = total_nans / (len(df[col]) / 365) if len(df[col]) > 0 else 0\n","\n","        results[col] = {\n","            'largest_number_of_nans_in_a_row': max_consecutive_nans,\n","            'average_number_of_nans_per_365_elements': average_nans_per_365\n","        }\n","\n","    return results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pd-6O7A3eg2"},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_top_correlations(df, target_column, num_features_per_heatmap):\n","    # Drop columns with over 70% NaN values\n","    df = df.dropna(thresh=len(df) * 0.2, axis=1)\n","\n","    # Drop remaining NaN values\n","    df = df.dropna()\n","\n","    # Compute the correlation matrix\n","    corrmat = df.corr()\n","\n","    # Get the correlations with the target column, sorted by absolute value\n","    sorted_correlations = corrmat[target_column].apply(abs).sort_values(ascending=False)\n","    for i in range(1, len(sorted_correlations), num_features_per_heatmap):\n","        # Get the next set of correlated features including the target column\n","        cols = [target_column] + sorted_correlations.index[i:i + num_features_per_heatmap].tolist()\n","        cm = np.corrcoef(df[cols].values.T)\n","        sns.set(font_scale=0.8) # Adjusting the font scale for smaller labels\n","        plt.figure(figsize=(20,20)) # Making the figure size larger\n","        hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.3f', annot_kws={'size': 8}, # Smaller annotation size\n","                        yticklabels=cols, xticklabels=cols, cmap='coolwarm')\n","        plt.title(f'Heatmap of Top {num_features_per_heatmap} Correlations with {target_column}')\n","        plt.show()\n","        print(\"\\n\" * 3 + \"=\" * 180 + \"\\n\" * 3)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kO3nezL1IHrQ"},"source":["# 10 layers you crazey!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJ5r6C8nks7T"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","# Loss function for regression\n","mse_loss = nn.MSELoss()\n","\n","\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","\n","def create_train_dataloader(dfTrain, batch_size=64,time_steps=60):\n","    # Number of features (excluding the target column \"emission\")\n","    num_features = 74\n","\n","    # Extracting features and target\n","    features = dfTrain.drop(columns=['emission']).values\n","    target = dfTrain['emission'].values\n","\n","    # Creating sequences of 60 timesteps\n","    sequences = []\n","    labels = []\n","    for i in range(len(features) - time_steps):\n","        sequences.append(features[i:i+time_steps])\n","        labels.append(target[i+time_steps])\n","\n","    # Converting to PyTorch tensors\n","    sequences_tensor = torch.tensor(sequences, dtype=torch.float32).view(-1, time_steps, num_features)\n","    labels_tensor = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","\n","    # Creating a TensorDataset and DataLoader\n","    train_dataset = TensorDataset(sequences_tensor, labels_tensor)\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    return train_dataloader\n","\n","def create_test_dataloader(dfTest, batch_size=64):\n","    # Number of features\n","    num_features = 74\n","\n","    # Extracting features\n","    features = dfTest.values\n","\n","    # Creating sequences of 60 timesteps\n","    sequences = [features[i:i+60] for i in range(len(features) - 60)]\n","\n","    # Converting to PyTorch tensors\n","    sequences_tensor = torch.tensor(sequences, dtype=torch.float32).view(-1, 60, num_features)\n","\n","    # Creating a TensorDataset and DataLoader\n","    test_dataset = TensorDataset(sequences_tensor)\n","    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return test_dataloader\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztpfmhGLks7h"},"outputs":[],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'testR.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"ID_LAT_LON_YEAR_WEEK\": dfid[\"ID_LAT_LON_YEAR_WEEK\"],\n","        \"emission\": predictions\n","    })\n","\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdjOpln2IJpZ"},"outputs":[],"source":["class LSTM_Arch(nn.Module):\n","    def __init__(self, input_size,\n","                 hidden_sizes,  # List of hidden sizes\n","                 dropouts,      # List of dropouts\n","                 attention_size):\n","        super(LSTM_Arch, self).__init__()\n","        num_layers = len(hidden_sizes)  # Determine the number of layers from the hidden_sizes list\n","        self.lstms = nn.ModuleList([nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n","                                            hidden_sizes[i],\n","                                            num_layers=1,\n","                                            batch_first=True,\n","                                            dropout=0)  # Set dropout to 0 since num_layers is 1\n","                                    for i in range(num_layers)])\n","        self.attentions = nn.ModuleList([nn.Linear(hidden_sizes[i], attention_size) for i in range(num_layers)])\n","        self.attention_combines = nn.ModuleList([nn.Linear(attention_size, 1) for i in range(num_layers)])\n","        self.fc = nn.Linear(hidden_sizes[-1], 1)\n","\n","\n","\n","    def apply_attention(self, out, attention_layer, attention_combine_layer):\n","        attention_weights = F.softmax(attention_combine_layer(F.tanh(attention_layer(out))), dim=1)\n","        out = torch.bmm(attention_weights.transpose(1, 2), out)\n","        return out.squeeze(1)\n","\n","    def forward(self, x):\n","        for i in range(len(self.lstms)):3\n","            out, _ = self.lstms[i](x if i == 0 else out.unsqueeze(1))\n","            out = self.apply_attention(out, self.attentions[i], self.attention_combines[i])\n","        out = self.fc(out)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7btLpYwFkfvj"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n","\n","def train(model, optimizer, train_dataloader, device, mse_loss, n=10):\n","    separator = '~' * 180\n","    print(' ' * 67 + 'TRAINING HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","    total_labels = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(train_dataloader):\n","        features, labels = batch[0].to(device), batch[1].to(device)\n","        model.zero_grad()\n","        preds = model(features)\n","        loss = mse_loss(preds, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        total_preds.append(preds.detach().cpu().numpy())\n","        total_labels.append(labels.detach().cpu().numpy())\n","\n","        if (step + 1) % n == 0:\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    total_labels = np.concatenate(total_labels, axis=0)\n","\n","    rmse = np.sqrt(mean_squared_error(total_labels, total_preds))\n","    mae = mean_absolute_error(total_labels, total_preds)\n","    mape = mean_absolute_percentage_error(total_labels, total_preds)\n","    r2 = r2_score(total_labels, total_preds)\n","\n","\n","    return avg_loss, total_preds, rmse, mae, mape, r2\n","\n","\n","def evaluate(model, val_dataloader, device, mse_loss, n=10):\n","    separator = '~' * 180\n","    print()\n","    print(' ' * 67 + 'EVALUATION HAS STARTED')\n","    print()\n","    print(separator)\n","\n","    model.eval()\n","    total_loss = 0\n","    total_preds = []\n","    total_labels = []\n","    chars_on_line = 0\n","    for step, batch in enumerate(val_dataloader):\n","        features, labels = batch[0].to(device), batch[1].to(device)\n","        with torch.no_grad():\n","            preds = model(features)\n","            loss = mse_loss(preds, labels)\n","            total_loss += loss.item()\n","            total_preds.append(preds.detach().cpu().numpy())\n","            total_labels.append(labels.detach().cpu().numpy())\n","\n","        if (step + 1) % n == 0:\n","            step_chars = len(str(step + 1)) + 1\n","            chars_on_line += step_chars\n","            print(step + 1, end='|')\n","            if chars_on_line + step_chars > len(separator):\n","                print()\n","                print(separator)\n","                chars_on_line = 0\n","    print()\n","    print(separator)\n","    print()\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    total_labels = np.concatenate(total_labels, axis=0)\n","\n","    rmse = np.sqrt(mean_squared_error(total_labels, total_preds))\n","    mae = mean_absolute_error(total_labels, total_preds)\n","    mape = mean_absolute_percentage_error(total_labels, total_preds)\n","    r2 = r2_score(total_labels, total_preds)\n","\n","\n","\n","    return avg_loss, total_preds, rmse, mae, mape, r2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jnGkvWGI_dL"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","class LSTM_Arch(nn.Module):\n","    def __init__(self, input_size,\n","                 hidden_sizes,  # List of hidden sizes\n","                 dropouts,      # List of dropouts\n","                 attention_size):\n","        super(LSTM_Arch, self).__init__()\n","        num_layers = len(hidden_sizes)  # Determine the number of layers from the hidden_sizes list\n","        self.lstms = nn.ModuleList([nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n","                                            hidden_sizes[i],\n","                                            num_layers=1,\n","                                            batch_first=True,\n","                                            dropout=0)  # Set dropout to 0 since num_layers is 1\n","                                    for i in range(num_layers)])\n","        self.attentions = nn.ModuleList([nn.Linear(hidden_sizes[i], attention_size) for i in range(num_layers)])\n","        self.attention_combines = nn.ModuleList([nn.Linear(attention_size, 1) for i in range(num_layers)])\n","        self.fc = nn.Linear(hidden_sizes[-1], 1)\n","\n","\n","\n","    def apply_attention(self, out, attention_layer, attention_combine_layer):\n","        attention_weights = F.softmax(attention_combine_layer(F.tanh(attention_layer(out))), dim=1)\n","        out = torch.bmm(attention_weights.transpose(1, 2), out)\n","        return out.squeeze(1)\n","\n","    def forward(self, x):\n","        for i in range(len(self.lstms)):\n","            out, _ = self.lstms[i](x if i == 0 else out.unsqueeze(1))\n","            out = self.apply_attention(out, self.attentions[i], self.attention_combines[i])\n","        out = self.fc(out)\n","        return out\n","\n","best_state_dict = None\n","best_params = None\n","best_global_metric=float('inf')\n","\n","def objective(trial, dfTrain, device,layers=10):\n","    global best_state_dict, best_params,best_global_metric\n","    separator = '~' * 180\n","    model_params = {\n","        \"lr\": [3e-8, 1e-2],\n","        \"weight_decay\": [1e-6, 1e-2],\n","        \"dropout\": [0.1, 0.5],\n","        \"beta1\": [0.8, 0.99],\n","        \"beta2\": [0.9, 0.9999],\n","        \"factor\": [0.05, 0.5],\n","        \"scheduler_patience\": [1, 2],\n","        \"threshold\": [1e-5, 1e-3],\n","        \"cooldown\": [0, 1],\n","        \"min_lr\": [0, 1e-3],\n","\n","        \"time_steps\":[50,5000],\n","        \"batch_size\": [128, 256],\n","        \"epochs\": [10, 15],\n","        \"attention_size\": [128, 512],\n","        'test_size':[0.15,0.45],\n","        'random_state':[42,1024],\n","}\n","\n","    for i in range(1, layers+1):\n","        model_params[f\"hidden_size{i}\"] = [256, 512]\n","        model_params[f\"dropout{i}\"] = [0.1, 0.5]\n","\n","\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds))\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","\n","    train_data, val_data = train_test_split(dfTrain, test_size=params[\"test_size\"], random_state=params[\"random_state\"])\n","\n","    # Create data loaders for training and validation data\n","    train_dataloader = create_train_dataloader(train_data, batch_size=params[\"batch_size\"],time_steps=params[\"time_steps\"])\n","    val_dataloader = create_train_dataloader(val_data, batch_size=params[\"batch_size\"],time_steps=params[\"time_steps\"])  # Using the same function for validation data\n","\n","    # Define input_size for the RNN model\n","    input_size = 74\n","\n","\n","\n","    hidden_sizes = [params[f\"hidden_size{i}\"] for i in range(1, layers+1)]\n","    dropouts = [params[f\"dropout{i}\"] for i in range(1, layers+1)]\n","\n","    # Instantiate the model using the new LSTM_Arch class\n","    model = LSTM_Arch(\n","        input_size=input_size,\n","        hidden_sizes=hidden_sizes,\n","        dropouts=dropouts,\n","        attention_size=params[\"attention_size\"]\n","    )\n","\n","    model = model.to(device)\n","\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","\n","    mse_loss = nn.MSELoss()\n","\n","    best_train_loss = float('inf')\n","    best_local_metric = float('inf')\n","    separator = '=' * 180\n","    metric=0\n","\n","\n","    for epoch in range(params[\"epochs\"]):\n","        print('-----------------------')\n","        print(f'    |Epoch {epoch + 1} / {params[\"epochs\"]}|')\n","        print('-----------------------')\n","\n","        train_loss, train_preds,train_rmse, train_mae, train_mape, train_r2 = train(model, optimizer, train_dataloader, device, mse_loss)\n","        valid_loss, val_preds,val_rmse, val_mae, val_mape, val_r2 = evaluate(model, val_dataloader, device, mse_loss)\n","\n","\n","        print(\"Training Metrics:\")\n","        print(f\"Average Loss: {train_loss}\")\n","        print(f\"RMSE: {train_rmse}\")\n","        print(f\"MAE: {train_mae}\")\n","        print(f\"MAPE: {train_mape}\")\n","        print(f\"R² Score: {train_r2}\")\n","        print(separator)\n","        print()\n","\n","\n","        print(\"Training Metrics:\")\n","        print(f\"Average Loss: {val_loss}\")\n","        print(f\"RMSE: {val_rmse}\")\n","        print(f\"MAE: {val_mae}\")\n","        print(f\"MAPE: {val_mape}\")\n","        print(f\"R² Score: {val_r2}\")\n","        print(separator)\n","        print()\n","\n","\n","\n","        metric = (valid_loss - train_loss)**2 * (valid_loss**2 + train_loss**2) * train_loss\n","\n","\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            if best_local_metric < best_global_metric :\n","                best_local_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params  # Save the parameters used to create the best model\n","                print('new best found')\n","            print('-----------------------')\n","        best_train_loss = min(train_loss, best_train_loss)\n","        print(f'Training Loss: {train_loss:.3f}')\n","        print(f'Validation Loss: {valid_loss:.3f}')\n","\n","    print('*' * 180)\n","    print('                                                                                    TRIAL=GUD')\n","    print('*' * 180)\n","    print()\n","\n","\n","\n","\n","    torch.save(model.state_dict(), f'saved_weights_trial_{trial.number}.pt')\n","\n","    print(('-----------------------'))\n","\n","    return best_local_metric\n","\n","\n","\n","\n","def tune_model_with_optuna(dfTrain, device, layers=10):\n","    global best_state_dict, best_params,best_global_metric\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=10)\n","\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    input_size = dfTrain.drop(columns=['emission']).shape[1]\n","\n","    hidden_sizes = [best_params[f\"hidden_size{i}\"] for i in range(1, layers + 1)]\n","    dropouts = [best_params[f\"dropout{i}\"] for i in range(1, layers + 1)]\n","\n","    best_model = LSTM_Arch(\n","        input_size=input_size,\n","        hidden_sizes=hidden_sizes,\n","        dropouts=dropouts,\n","        attention_size=best_params[\"attention_size\"]\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","\n","    return best_model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zykdcigB682y"},"outputs":[],"source":["\n","folder_path = '/content/drive/MyDrive/datasets/'\n","dfTrain = pd.read_csv(folder_path + 'trainR.csv')\n","dfTest = pd.read_csv(folder_path + 'testR.csv')\n","dfTrain.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)\n","\n","dfTest.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)\n","dfTrain=fill_nans_with_previous(dfTrain)\n","dfTest=fill_nans_with_previous(dfTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQv7fmdAKClI","outputId":"3c7ba435-2c80-4e1a-dcae-351f0c3d8466"},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 18:43:27,684] A new study created in memory with name: no-name-1173d2f9-a8f4-4d65-af37-d8cb38f9fa0a\n"]}],"source":["best_state_dict = None\n","best_params = None\n","best_global_metric=float('inf')\n","\n","tune_model_with_optuna(dfTrain, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4iq8GpPKXbv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"s2g4FR766jAg"},"source":["#using just models(nornn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXf1tTjj43xw"},"outputs":[],"source":["from sklearn.linear_model import Lasso, Ridge, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","\n","# Gradient Boosting Regressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","\n","\n","\n","# AdaBoost Regressor\n","from sklearn.ensemble import AdaBoostRegressor\n","\n","\n","\n","# XGBoost Regressor\n","from xgboost import XGBRegressor\n","\n","\n","\n","# Linear Regression\n","from sklearn.linear_model import LinearRegression\n","\n","# Lasso Regression\n","# Lasso Regression\n","lasso = Lasso()\n","lasso_params = {\n","    'alpha': [0.01, 100],\n","    'fit_intercept': [True, False],\n","    'copy_X': [True, False],\n","    'max_iter': [500, 10000],\n","    'tol': [0.00001, 0.01],\n","    'warm_start': [True, False],\n","    'positive': [True, False],\n","    'selection': ['cyclic', 'random']\n","}\n","\n","# Ridge Regression\n","ridge = Ridge()\n","ridge_params = {\n","    'alpha': [0.01, 100],\n","    'fit_intercept': [True, False],\n","    'copy_X': [True, False],\n","    'max_iter': [500, 2000],\n","    'tol': [0.00001, 0.01],\n","    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n","}\n","\n","# Random Forest Regressor\n","random_forest = RandomForestRegressor()\n","rf_params = {\n","    'n_estimators': [25, 100],\n","    'max_depth': [3, 20],\n","    'min_samples_split': [2, 10],\n","    'min_samples_leaf': [1, 10],\n","    'max_features': ['sqrt'],\n","    #'bootstrap': [True, False],\n","    #'oob_score': [False, True],\n","    #'warm_start': [False, True]\n","}\n","\n","# Gradient Boosting Regressor\n","gradient_boosting = GradientBoostingRegressor()\n","gbr_params = {\n","    'loss': ['ls', 'lad', 'huber', 'quantile'],\n","    'learning_rate': [0.001, 0.5],\n","    'n_estimators': [25, 100],\n","    'subsample': [0.1, 1.0],\n","    'warm_start': [False, True],\n","    'validation_fraction': [0.05, 0.5],\n","    'tol': [1e-5, 1e-3]\n","}\n","\n","# AdaBoost Regressor\n","adaboost = AdaBoostRegressor()\n","ada_params = {\n","    'n_estimators': [25, 100],\n","    'learning_rate': [0.001, 0.5],\n","    'loss': ['linear', 'square', 'exponential']\n","}\n","xgboost = XGBRegressor()\n","xgboost_params ={\n","    'n_estimators': (50, 200),\n","    'max_depth': (3, 10),\n","    'learning_rate': (0.01, 0.5),\n","    'gamma': (0, 0.5),\n","    'reg_lambda': (0.1, 1.0),\n","    'reg_alpha': (0.1, 1.0),\n","    'subsample': (0.5, 1.0)\n","    }\n","\n","\n","\n","# Linear Regression\n","linear_regression = LinearRegression()\n","lr_params = {\n","    'fit_intercept': [True, False],\n","    'copy_X': [True, False],\n","    'positive': [False, True]\n","}\n","\n","\n","# Linear Regression\n","linear_regression = LinearRegression()\n","lr_params = {\n","    'fit_intercept': [True, False],\n","    'copy_X': [True, False],\n","    'positive': [False, True]\n","}\n","\n","lgbm=LGBMRegressor()\n","\n","lgbm_params = {\n","    'n_estimators': (200, 300),\n","    'learning_rate': (0.01, 0.5),\n","    'boosting_type': ['gbdt', 'dart', 'rf'],\n","    'num_leaves': (51, 200),\n","    'max_depth': (3, 10),\n","    'subsample_for_bin': (20000, 300000),\n","    'min_split_gain': (0.0, 1.0),\n","    'min_child_weight': (1e-3, 1e-1),\n","    'min_child_samples': (5, 50),\n","    'subsample': (0.5, 1.0),\n","    'subsample_freq': (0, 10),\n","    'colsample_bytree': (0.5, 1.0),\n","    'reg_alpha': (0.0, 1.0),\n","    'reg_lambda': (0.0, 1.0)\n","}\n","xgboost = XGBRegressor()\n","xgboost_params ={\n","    'n_estimators': (50, 200),\n","    'max_depth': (3, 10),\n","    'learning_rate': (0.01, 0.5),\n","    'gamma': (0, 0.5),\n","    'reg_lambda': (0.1, 1.0),\n","    'reg_alpha': (0.1, 1.0),\n","    'subsample': (0.5, 1.0)\n","    }\n","\n"]},{"cell_type":"code","source":["lgbm=LGBMRegressor()\n","\n","lgbm_params = {\n","    'n_estimators': (216, 300),\n","    'learning_rate': (0.1086, 0.4925),\n","    'boosting_type': ['dart', 'gbdt'],\n","    'num_leaves': (101, 200),\n","    'max_depth': (8, 12),\n","    'subsample_for_bin': (21762, 57851),\n","    'min_split_gain': (0.00299, 0.0997),\n","    'min_child_weight': (0.00533, 0.0195),\n","    'min_child_samples': (10, 20),\n","    'subsample': (0.8077, 0.9976),\n","    'subsample_freq': (5, 10),\n","    'colsample_bytree': (0.5112, 0.9971),\n","    'reg_alpha': (0.00042, 0.9982),\n","    'reg_lambda': (0.0527, 0.9851)\n","}\n","\n"],"metadata":{"id":"LrLpwwmeWE04"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fLjVie66krt"},"outputs":[],"source":["import optuna\n","import numpy as np\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","from lightgbm import LGBMRegressor\n","lgbm=LGBMRegressor()\n","\n","lgbm_params = {\n","    'n_estimators': (216, 300),\n","    'learning_rate': (0.1086, 0.4925),\n","    'boosting_type': ['dart', 'gbdt'],\n","    'num_leaves': (101, 200),\n","    'max_depth': (8, 12),\n","    'subsample_for_bin': (21762, 57851),\n","    'min_split_gain': (0.00299, 0.0997),\n","    'min_child_weight': (0.00533, 0.0195),\n","    'min_child_samples': (10, 20),\n","    'subsample': (0.8077, 0.9976),\n","    'subsample_freq': (5, 10),\n","    'colsample_bytree': (0.5112, 0.9971),\n","    'reg_alpha': (0.00042, 0.9982),\n","    'reg_lambda': (0.0527, 0.9851)\n","}\n","\n","class ModelSelectionPipeline:\n","    def __init__(self, model, model_params, n_jobs=-1, n_trials=15):\n","        self.model = model\n","        self.model_params = model_params\n","        self.n_jobs = n_jobs\n","        self.best_model = None\n","        self.best_predictions = None\n","        self.n_trials=n_trials\n","\n","    def fit_and_predict(self, X_train, y_train, X_test):\n","\n","\n","\n","        print(f\"Evaluating model: {self.model.__class__.__name__}\")\n","        study = optuna.create_study(direction='minimize')\n","        study.optimize(lambda trial: self.objective(trial, X_train, y_train),  n_trials=self.n_trials)\n","        print(f\"Best trial for model {self.model.__class__.__name__}: score {study.best_value}, params {study.best_params}\")\n","\n","\n","        self.best_model = self.model.set_params(**study.best_params)\n","        self.best_model.fit(X_train, y_train)\n","        self.best_predictions = self.best_model.predict(X_test)\n","\n","    def objective(self, trial, X, y):\n","        params = self.model.get_params(deep=True)  # Start with the model's default parameters\n","        for param, bounds in self.model_params.items():\n","            if isinstance(bounds[0], int):\n","                params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","            elif isinstance(bounds[0], float):\n","                params[param] = trial.suggest_float(param, min(bounds), max(bounds))\n","            else:\n","                params[param] = trial.suggest_categorical(param, bounds)\n","        self.model.set_params(**params)\n","\n","        # Use MAE as the scoring metric\n","        scores = cross_val_score(self.model, X, y, cv=10, scoring='neg_mean_absolute_error', n_jobs=self.n_jobs)\n","\n","        return -scores.mean()  # Minimize the negative mean absolute error\n","\n","\n","\n","def best_preds_comb(models, param_grids, X_train_combined, X_test_combined, y, n_trials=15):\n","    # Split the full training data into train, meta_train and validation sets\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","        X_train_combined, y, test_size=0.30, random_state=42)\n","    X_meta_train, X_val, y_meta_train, y_val = train_test_split(\n","        X_temp, y_temp, test_size=(6/30), random_state=42)\n","\n","    train_predictions = []\n","    meta_train_predictions = []\n","    val_predictions = []\n","    test_predictions = []\n","    model_selections = []\n","\n","    for model, param_grid in zip(models, param_grids):\n","        pipeline = ModelSelectionPipeline(model, param_grid, n_trials=n_trials)\n","        pipeline.fit_and_predict(X_train, y_train, X_meta_train)\n","\n","        train_predictions.append(pipeline.best_model.predict(X_train))\n","        meta_train_predictions.append(pipeline.best_model.predict(X_meta_train))\n","        val_predictions.append(pipeline.best_model.predict(X_val))\n","        test_predictions.append(pipeline.best_model.predict(X_test_combined))\n","        model_selections.append(pipeline.best_model)\n","\n","\n","    meta_param_grid  = {\n","    'n_estimators': (150, 300),\n","    'learning_rate': (0.1, 0.5), # Narrowing down based on best value\n","    'boosting_type': ['gbdt', 'dart'], # Removing 'rf' as 'dart' performed best\n","    'num_leaves': (100, 200), # Narrowing down based on best value\n","    'max_depth': (8, 12), # Narrowing down based on best value\n","    'subsample_for_bin': (20000, 60000), # Narrowing down based on best value\n","    'min_split_gain': (0.0, 0.1), # Narrowing down based on best value\n","    'min_child_weight': (0.005, 0.02), # Narrowing down based on best value\n","    'min_child_samples': (10, 20), # Narrowing down based on best value\n","    'subsample': (0.8, 1.0), # Narrowing down based on best value\n","    'subsample_freq': (5, 10), # Keeping the same, as varied results\n","    'colsample_bytree': (0.5, 1.0),\n","    'reg_alpha': (0.0, 1.0),\n","    'reg_lambda': (0.0, 1.0)\n","    }\n","\n","    meta_model = LGBMRegressor()\n","\n","    meta_train_predictions_np = np.column_stack(meta_train_predictions)\n","    test_predictions_np = np.column_stack(test_predictions)\n","\n","    pipeline = ModelSelectionPipeline(meta_model, meta_param_grid, n_trials=15)\n","    pipeline.fit_and_predict(meta_train_predictions_np, y_meta_train, test_predictions_np)\n","\n","    # Calculate the mean squared error of the meta model on the validation data\n","    val_predictions_np = np.column_stack(val_predictions)\n","    meta_model_mse = mean_squared_error(y_val, pipeline.best_model.predict(val_predictions_np))\n","    print(f\"Meta model mean squared error: {meta_model_mse}\")\n","\n","    return pipeline.best_predictions, model_selections\n","\n"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import optuna\n","import numpy as np\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","from lightgbm import LGBMRegressor\n","\n","class ModelSelectionPipeline:\n","    def __init__(self, model, model_params, n_jobs=-1, n_trials=15):\n","        self.model = model\n","        self.model_params = model_params\n","        self.n_jobs = n_jobs\n","        self.best_model = None\n","        self.best_predictions = None\n","        self.n_trials=n_trials\n","\n","    def fit_and_predict(self, X_train, y_train, X_test):\n","\n","\n","\n","        print(f\"Evaluating model: {self.model.__class__.__name__}\")\n","        study = optuna.create_study(direction='minimize')\n","        study.optimize(lambda trial: self.objective(trial, X_train, y_train),  n_trials=self.n_trials)\n","        print(f\"Best trial for model {self.model.__class__.__name__}: score {study.best_value}, params {study.best_params}\")\n","\n","\n","        self.best_model = self.model.set_params(**study.best_params)\n","        self.best_model.fit(X_train, y_train)\n","        self.best_predictions = self.best_model.predict(X_test)\n","\n","    def objective(self, trial, X, y):\n","        params = self.model.get_params(deep=True)  # Start with the model's default parameters\n","        for param, bounds in self.model_params.items():\n","            if isinstance(bounds[0], int):\n","                params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","            elif isinstance(bounds[0], float):\n","                params[param] = trial.suggest_float(param, min(bounds), max(bounds))\n","            else:\n","                params[param] = trial.suggest_categorical(param, bounds)\n","        self.model.set_params(**params)\n","\n","        # Use MAE as the scoring metric\n","        scores = cross_val_score(self.model, X, y, cv=10, scoring='neg_mean_absolute_error', n_jobs=self.n_jobs)\n","\n","        return -scores.mean()  # Minimize the negative mean absolute error\n","\n","def best_preds_one(models, param_grids, X_train_combined, X_test_combined, y, n_trials=10):\n","    # Split the combined training data into train and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_train_combined, y, test_size=0.2, random_state=42)\n","\n","    test_predictions = []\n","    last_predictions = None\n","\n","    for model, param_grid in zip(models, param_grids):\n","        pipeline = ModelSelectionPipeline(model, param_grid, n_trials=n_trials)\n","        pipeline.fit_and_predict(X_train, y_train, X_test_combined)\n","\n","        pipeline_mse = mean_squared_error(y_val, pipeline.best_model.predict(X_val))\n","        print(f\"Model mean squared error: {pipeline_mse}\")\n","\n","\n","\n","        test_predictions.append(pipeline.best_model.predict(X_test_combined))\n","        print('------------------------------------------------------------------------')\n","\n","        last_predictions = pipeline.best_model.predict(X_test_combined)\n","\n","    return test_predictions, last_predictions\n"],"metadata":{"id":"LFxPl1B2YmFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnZXwE1IAMg7"},"outputs":[],"source":["models = [\n","    #linear_regression,\n","    #lasso, # changed from lasso_regression\n","    #ridge, # changed from ridge_regression\n","    #random_forest, # changed from random_forest_regressor\n","    #gradient_boosting, # changed from gradient_boosting_regressor\n","    #adaboost, # changed from adaboost_regressor\n","    #xgboost, # changed from xgboost_regressor\n","    lgbm\n","]\n","\n","param_grids = [\n","    #lr_params,\n","    #lasso_params,\n","    #ridge_params,\n","    #rf_params,\n","    #gbr_params,\n","    #ada_params,\n","    #xgboost_params, # changed from xgb_params\n","    lgbm_params\n","]\n","\n","# Reading data\n","folder_path = '/content/drive/MyDrive/datasets/'\n","dfTrain = pd.read_csv(folder_path + 'trainR.csv')\n","dfTest = pd.read_csv(folder_path + 'testR.csv')\n","\n","dfTrain.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)\n","\n","dfTest.drop(['ID_LAT_LON_YEAR_WEEK'],axis=1,inplace=True)\n","dfTrain =fill_nans_with_previous(dfTrain)\n","\n","dfTest =fill_nans_with_previous(dfTest)\n","\n"]},{"cell_type":"code","source":["dfTrain.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxrpST7RLYC0","executionInfo":{"status":"ok","timestamp":1691638859909,"user_tz":360,"elapsed":5,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"7bd08574-aac4-413e-c8f6-d784a0c08ea8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 79023 entries, 0 to 79022\n","Data columns (total 75 columns):\n"," #   Column                                                    Non-Null Count  Dtype  \n","---  ------                                                    --------------  -----  \n"," 0   latitude                                                  79023 non-null  float64\n"," 1   longitude                                                 79023 non-null  float64\n"," 2   year                                                      79023 non-null  int64  \n"," 3   week_no                                                   79023 non-null  int64  \n"," 4   SulphurDioxide_SO2_column_number_density                  79023 non-null  float64\n"," 5   SulphurDioxide_SO2_column_number_density_amf              79023 non-null  float64\n"," 6   SulphurDioxide_SO2_slant_column_number_density            79023 non-null  float64\n"," 7   SulphurDioxide_cloud_fraction                             79023 non-null  float64\n"," 8   SulphurDioxide_sensor_azimuth_angle                       79023 non-null  float64\n"," 9   SulphurDioxide_sensor_zenith_angle                        79023 non-null  float64\n"," 10  SulphurDioxide_solar_azimuth_angle                        79023 non-null  float64\n"," 11  SulphurDioxide_solar_zenith_angle                         79023 non-null  float64\n"," 12  SulphurDioxide_SO2_column_number_density_15km             79023 non-null  float64\n"," 13  CarbonMonoxide_CO_column_number_density                   79023 non-null  float64\n"," 14  CarbonMonoxide_H2O_column_number_density                  79023 non-null  float64\n"," 15  CarbonMonoxide_cloud_height                               79023 non-null  float64\n"," 16  CarbonMonoxide_sensor_altitude                            79023 non-null  float64\n"," 17  CarbonMonoxide_sensor_azimuth_angle                       79023 non-null  float64\n"," 18  CarbonMonoxide_sensor_zenith_angle                        79023 non-null  float64\n"," 19  CarbonMonoxide_solar_azimuth_angle                        79023 non-null  float64\n"," 20  CarbonMonoxide_solar_zenith_angle                         79023 non-null  float64\n"," 21  NitrogenDioxide_NO2_column_number_density                 79023 non-null  float64\n"," 22  NitrogenDioxide_tropospheric_NO2_column_number_density    79023 non-null  float64\n"," 23  NitrogenDioxide_stratospheric_NO2_column_number_density   79023 non-null  float64\n"," 24  NitrogenDioxide_NO2_slant_column_number_density           79023 non-null  float64\n"," 25  NitrogenDioxide_tropopause_pressure                       79023 non-null  float64\n"," 26  NitrogenDioxide_absorbing_aerosol_index                   79023 non-null  float64\n"," 27  NitrogenDioxide_cloud_fraction                            79023 non-null  float64\n"," 28  NitrogenDioxide_sensor_altitude                           79023 non-null  float64\n"," 29  NitrogenDioxide_sensor_azimuth_angle                      79023 non-null  float64\n"," 30  NitrogenDioxide_sensor_zenith_angle                       79023 non-null  float64\n"," 31  NitrogenDioxide_solar_azimuth_angle                       79023 non-null  float64\n"," 32  NitrogenDioxide_solar_zenith_angle                        79023 non-null  float64\n"," 33  Formaldehyde_tropospheric_HCHO_column_number_density      79023 non-null  float64\n"," 34  Formaldehyde_tropospheric_HCHO_column_number_density_amf  79023 non-null  float64\n"," 35  Formaldehyde_HCHO_slant_column_number_density             79023 non-null  float64\n"," 36  Formaldehyde_cloud_fraction                               79023 non-null  float64\n"," 37  Formaldehyde_solar_zenith_angle                           79023 non-null  float64\n"," 38  Formaldehyde_solar_azimuth_angle                          79023 non-null  float64\n"," 39  Formaldehyde_sensor_zenith_angle                          79023 non-null  float64\n"," 40  Formaldehyde_sensor_azimuth_angle                         79023 non-null  float64\n"," 41  UvAerosolIndex_absorbing_aerosol_index                    79023 non-null  float64\n"," 42  UvAerosolIndex_sensor_altitude                            79023 non-null  float64\n"," 43  UvAerosolIndex_sensor_azimuth_angle                       79023 non-null  float64\n"," 44  UvAerosolIndex_sensor_zenith_angle                        79023 non-null  float64\n"," 45  UvAerosolIndex_solar_azimuth_angle                        79023 non-null  float64\n"," 46  UvAerosolIndex_solar_zenith_angle                         79023 non-null  float64\n"," 47  Ozone_O3_column_number_density                            79023 non-null  float64\n"," 48  Ozone_O3_column_number_density_amf                        79023 non-null  float64\n"," 49  Ozone_O3_slant_column_number_density                      79023 non-null  float64\n"," 50  Ozone_O3_effective_temperature                            79023 non-null  float64\n"," 51  Ozone_cloud_fraction                                      79023 non-null  float64\n"," 52  Ozone_sensor_azimuth_angle                                79023 non-null  float64\n"," 53  Ozone_sensor_zenith_angle                                 79023 non-null  float64\n"," 54  Ozone_solar_azimuth_angle                                 79023 non-null  float64\n"," 55  Ozone_solar_zenith_angle                                  79023 non-null  float64\n"," 56  UvAerosolLayerHeight_aerosol_height                       79023 non-null  float64\n"," 57  UvAerosolLayerHeight_aerosol_pressure                     79023 non-null  float64\n"," 58  UvAerosolLayerHeight_aerosol_optical_depth                79023 non-null  float64\n"," 59  UvAerosolLayerHeight_sensor_zenith_angle                  79023 non-null  float64\n"," 60  UvAerosolLayerHeight_sensor_azimuth_angle                 79023 non-null  float64\n"," 61  UvAerosolLayerHeight_solar_azimuth_angle                  79023 non-null  float64\n"," 62  UvAerosolLayerHeight_solar_zenith_angle                   79023 non-null  float64\n"," 63  Cloud_cloud_fraction                                      79023 non-null  float64\n"," 64  Cloud_cloud_top_pressure                                  79023 non-null  float64\n"," 65  Cloud_cloud_top_height                                    79023 non-null  float64\n"," 66  Cloud_cloud_base_pressure                                 79023 non-null  float64\n"," 67  Cloud_cloud_base_height                                   79023 non-null  float64\n"," 68  Cloud_cloud_optical_depth                                 79023 non-null  float64\n"," 69  Cloud_surface_albedo                                      79023 non-null  float64\n"," 70  Cloud_sensor_azimuth_angle                                79023 non-null  float64\n"," 71  Cloud_sensor_zenith_angle                                 79023 non-null  float64\n"," 72  Cloud_solar_azimuth_angle                                 79023 non-null  float64\n"," 73  Cloud_solar_zenith_angle                                  79023 non-null  float64\n"," 74  emission                                                  79023 non-null  float64\n","dtypes: float64(73), int64(2)\n","memory usage: 45.2 MB\n"]}]},{"cell_type":"code","source":["\n","#lag days feature\n","def previous(df, column_num=3, num_days=1):\n","    # Copy the dataframe\n","    df_copy = df.copy()\n","\n","    for i in range(1, num_days + 1):\n","        # Shift the specified column 'i' days into the past\n","        shifted_series = df_copy.iloc[:, column_num].shift(i)\n","        # Create a new column with the shifted values\n","        df_copy.insert(i - 1, f'Price_{i}_ago', shifted_series)\n","\n","    # Remove the first 'num_days' rows from the copied dataframe\n","    df_copy = df_copy[num_days:]\n","\n","    # Fill NaN values with the most recent valid value\n","    df_copy.fillna(method='ffill', inplace=True)\n","\n","    return df_copy\n"],"metadata":{"id":"tLnyCjtADbvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p5a59q1KDi-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlN-bAFHAMz5"},"outputs":[],"source":["predictions, model_selected = best_preds_comb(models, param_grids,\n","                                              dfTrain.drop(['emission'], axis=1),\n","                                              dfTest,\n","                                              dfTrain['emission'])\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"M5KIjI7FDvEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjVvxCivAl3s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691693376753,"user_tz":360,"elapsed":1472480,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"8cea8172-6df4-4a19-8cec-d561b44df602"},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-10 18:25:05,675] A new study created in memory with name: no-name-73a95ec3-50bf-4755-a4a4-809befcabc87\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating model: LGBMRegressor\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-10 18:27:49,999] Trial 0 finished with value: 9.262423923325667 and parameters: {'n_estimators': 259, 'learning_rate': 0.2791486107843115, 'boosting_type': 'dart', 'num_leaves': 174, 'max_depth': 8, 'subsample_for_bin': 24391, 'min_split_gain': 0.044494203709832245, 'min_child_weight': 0.01843602150273977, 'min_child_samples': 12, 'subsample': 0.9071602230753402, 'subsample_freq': 7, 'colsample_bytree': 0.9257723875537283, 'reg_alpha': 0.409611674807905, 'reg_lambda': 0.38049000266969213}. Best is trial 0 with value: 9.262423923325667.\n","[I 2023-08-10 18:30:38,026] Trial 1 finished with value: 9.152497063813183 and parameters: {'n_estimators': 297, 'learning_rate': 0.4329765813769375, 'boosting_type': 'dart', 'num_leaves': 119, 'max_depth': 8, 'subsample_for_bin': 37984, 'min_split_gain': 0.09935134102521778, 'min_child_weight': 0.007647331362348591, 'min_child_samples': 19, 'subsample': 0.8183922569615208, 'subsample_freq': 7, 'colsample_bytree': 0.8464920537693446, 'reg_alpha': 0.5009414632090713, 'reg_lambda': 0.09850331591927582}. Best is trial 1 with value: 9.152497063813183.\n","[I 2023-08-10 18:33:09,738] Trial 2 finished with value: 9.946688483751943 and parameters: {'n_estimators': 256, 'learning_rate': 0.2539698960654046, 'boosting_type': 'dart', 'num_leaves': 178, 'max_depth': 9, 'subsample_for_bin': 43757, 'min_split_gain': 0.09226596501029069, 'min_child_weight': 0.00954694648104273, 'min_child_samples': 20, 'subsample': 0.9265941631656456, 'subsample_freq': 10, 'colsample_bytree': 0.5798498368987298, 'reg_alpha': 0.9889701745152973, 'reg_lambda': 0.2530513068801636}. Best is trial 1 with value: 9.152497063813183.\n","[I 2023-08-10 18:36:18,432] Trial 3 finished with value: 8.079920665587887 and parameters: {'n_estimators': 247, 'learning_rate': 0.19730969614567698, 'boosting_type': 'dart', 'num_leaves': 191, 'max_depth': 11, 'subsample_for_bin': 33144, 'min_split_gain': 0.04121327070053248, 'min_child_weight': 0.017472397016992762, 'min_child_samples': 17, 'subsample': 0.8688109073253736, 'subsample_freq': 9, 'colsample_bytree': 0.8586534242900914, 'reg_alpha': 0.6352581148086662, 'reg_lambda': 0.5514747694808775}. Best is trial 3 with value: 8.079920665587887.\n","[I 2023-08-10 18:37:22,435] Trial 4 finished with value: 12.692513759213252 and parameters: {'n_estimators': 265, 'learning_rate': 0.3423136562606689, 'boosting_type': 'gbdt', 'num_leaves': 190, 'max_depth': 8, 'subsample_for_bin': 44221, 'min_split_gain': 0.06813843038524169, 'min_child_weight': 0.012846077009851568, 'min_child_samples': 11, 'subsample': 0.8368876129318029, 'subsample_freq': 6, 'colsample_bytree': 0.7973000907985722, 'reg_alpha': 0.17968966904046899, 'reg_lambda': 0.577267126263793}. Best is trial 3 with value: 8.079920665587887.\n","[I 2023-08-10 18:39:18,535] Trial 5 finished with value: 9.062901469716097 and parameters: {'n_estimators': 298, 'learning_rate': 0.1926649175495332, 'boosting_type': 'gbdt', 'num_leaves': 187, 'max_depth': 12, 'subsample_for_bin': 50867, 'min_split_gain': 0.05754919512278116, 'min_child_weight': 0.018953761913218296, 'min_child_samples': 19, 'subsample': 0.9634198111091571, 'subsample_freq': 8, 'colsample_bytree': 0.9255858884252531, 'reg_alpha': 0.5937264520436919, 'reg_lambda': 0.5662647400396407}. Best is trial 3 with value: 8.079920665587887.\n","[I 2023-08-10 18:43:02,292] Trial 6 finished with value: 7.366383768936852 and parameters: {'n_estimators': 271, 'learning_rate': 0.4618486912974318, 'boosting_type': 'dart', 'num_leaves': 190, 'max_depth': 12, 'subsample_for_bin': 36786, 'min_split_gain': 0.050698282988378404, 'min_child_weight': 0.015661242771447002, 'min_child_samples': 17, 'subsample': 0.8578090482304607, 'subsample_freq': 8, 'colsample_bytree': 0.864798188564072, 'reg_alpha': 0.03273212593245682, 'reg_lambda': 0.579316577117079}. Best is trial 6 with value: 7.366383768936852.\n","[I 2023-08-10 18:45:46,068] Trial 7 finished with value: 8.507662005648958 and parameters: {'n_estimators': 288, 'learning_rate': 0.44485284670251446, 'boosting_type': 'dart', 'num_leaves': 117, 'max_depth': 9, 'subsample_for_bin': 40981, 'min_split_gain': 0.03756200519018926, 'min_child_weight': 0.01153123210031107, 'min_child_samples': 10, 'subsample': 0.9028404506690179, 'subsample_freq': 10, 'colsample_bytree': 0.6512036323669724, 'reg_alpha': 0.374318243180755, 'reg_lambda': 0.10637213352230242}. Best is trial 6 with value: 7.366383768936852.\n","[I 2023-08-10 18:48:10,266] Trial 8 finished with value: 10.507395904511785 and parameters: {'n_estimators': 272, 'learning_rate': 0.2336360876486427, 'boosting_type': 'dart', 'num_leaves': 144, 'max_depth': 8, 'subsample_for_bin': 22439, 'min_split_gain': 0.08444990013706623, 'min_child_weight': 0.010942581826278922, 'min_child_samples': 10, 'subsample': 0.819416245558939, 'subsample_freq': 10, 'colsample_bytree': 0.71250006213108, 'reg_alpha': 0.01186324908554633, 'reg_lambda': 0.5267844955530975}. Best is trial 6 with value: 7.366383768936852.\n","[I 2023-08-10 18:49:06,060] Trial 9 finished with value: 13.41796458857917 and parameters: {'n_estimators': 256, 'learning_rate': 0.4087287017750335, 'boosting_type': 'gbdt', 'num_leaves': 105, 'max_depth': 9, 'subsample_for_bin': 55669, 'min_split_gain': 0.0426338760384099, 'min_child_weight': 0.011976707601617969, 'min_child_samples': 17, 'subsample': 0.8635995501415329, 'subsample_freq': 8, 'colsample_bytree': 0.7682839430334357, 'reg_alpha': 0.3492755349877329, 'reg_lambda': 0.18383730038946672}. Best is trial 6 with value: 7.366383768936852.\n"]},{"output_type":"stream","name":"stdout","text":["Best trial for model LGBMRegressor: score 7.366383768936852, params {'n_estimators': 271, 'learning_rate': 0.4618486912974318, 'boosting_type': 'dart', 'num_leaves': 190, 'max_depth': 12, 'subsample_for_bin': 36786, 'min_split_gain': 0.050698282988378404, 'min_child_weight': 0.015661242771447002, 'min_child_samples': 17, 'subsample': 0.8578090482304607, 'subsample_freq': 8, 'colsample_bytree': 0.864798188564072, 'reg_alpha': 0.03273212593245682, 'reg_lambda': 0.579316577117079}\n","Model mean squared error: 335.6011782100024\n","------------------------------------------------------------------------\n"]}],"source":["predictions,lp = best_preds_one(models, param_grids,\n","\n","                                              dfTrain.drop(['emission'], axis=1),\n","                                              dfTest,\n","                                              dfTrain['emission'])\n"]},{"cell_type":"code","source":["len(predictions[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1LQmduqmVK1","executionInfo":{"status":"ok","timestamp":1691438814298,"user_tz":360,"elapsed":3,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"d3fa9cc8-d18d-412d-cfa2-fda3652b1320"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24353"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zzOmOQGMvlt"},"outputs":[],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'testR.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"ID_LAT_LON_YEAR_WEEK\": dfid[\"ID_LAT_LON_YEAR_WEEK\"],\n","        \"emission\": predictions\n","    })\n","\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)"]},{"cell_type":"code","source":["create_and_download_submission(predictions[0], 'submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"fDnnK-rtYqU2","executionInfo":{"status":"ok","timestamp":1691693541443,"user_tz":360,"elapsed":433,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"849dabcb-88f3-487b-cc54-8c6fc1bdc9b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b09d3a3a-b98a-40f9-aebc-a01278810e6e\", \"submission.csv\", 1056789)"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"x_ILsSdynCs_"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"toc_visible":true,"gpuType":"V100","authorship_tag":"ABX9TyP/JfvuEKo19ZpNMJk1RiLA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}