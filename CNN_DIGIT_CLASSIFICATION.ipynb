{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyND9gTCEm7Hlw+/o9M8SGMK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hciv2YOnd0cj"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sNwjfm0KzDM0"},"source":["#setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34412,"status":"ok","timestamp":1692146163816,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"2sPQJvvKn9wh","outputId":"5a0ebd24-6e35-4618-d417-585458bb8859"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install transformers\n","\n","# Imports\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","import pandas as pd\n","\n","\n","from scipy.sparse import hstack\n","from scipy.sparse import csr_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIRRawgSgUiP"},"outputs":[],"source":["from google.colab import files\n","\n","def create_and_download_submission(predictions, filename='submission.csv'):\n","    # Read the CSV file that contains the ID of each test sample\n","    dfid = pd.read_csv(folder_path + 'blyat.csv')\n","\n","    # Create a dataframe for the submission\n","    submission = pd.DataFrame({\n","        \"ImageId\": dfid[\"ImageId\"],\n","        \"Label\": predictions\n","    })\n","    submission.head()\n","    # Save the submission dataframe to a CSV file\n","    submission.to_csv(filename, index=False)\n","\n","    # Download the CSV file\n","    files.download(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXWODo9-0hdE"},"outputs":[],"source":["import warnings\n","# Filter warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8009,"status":"ok","timestamp":1692146171822,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"NgsGDmoPyK0V","outputId":"312c8ee0-a130-4442-c562-4e9b6ed85a59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.11.2-py3-none-any.whl (225 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes>=0.10.0 (from optuna)\n","  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.2 cmaes-0.10.0 colorlog-6.7.0 optuna-3.3.0\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a4vdrsFMBYj"},"outputs":[],"source":["import optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdXjWS97oUyW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import BertTokenizerFast, BertModel, AdamW\n","import numpy as np\n","from imblearn.over_sampling import RandomOverSampler\n","from collections import Counter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iGOehQE_V3o"},"outputs":[],"source":["torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wIIVdl_orcb"},"outputs":[],"source":["# Reading data\n","folder_path = '/content/drive/MyDrive/datasets/'\n","dfTrain = pd.read_csv(folder_path + 'train_cnn.csv')\n","dfTest = pd.read_csv(folder_path + 'test_cnn.csv')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVmI0xjXGv4i"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1692146181253,"user":{"displayName":"Irek","userId":"02915086061816371557"},"user_tz":360},"id":"tRnp27hgboe4","outputId":"5baa5d20-1ef9-4f4e-eb06-3e44ebfba49f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n","0           0       0       0       0       0       0       0       0       0   \n","1           0       0       0       0       0       0       0       0       0   \n","2           0       0       0       0       0       0       0       0       0   \n","3           0       0       0       0       0       0       0       0       0   \n","4           0       0       0       0       0       0       0       0       0   \n","...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","27995       0       0       0       0       0       0       0       0       0   \n","27996       0       0       0       0       0       0       0       0       0   \n","27997       0       0       0       0       0       0       0       0       0   \n","27998       0       0       0       0       0       0       0       0       0   \n","27999       0       0       0       0       0       0       0       0       0   \n","\n","       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n","0           0  ...         0         0         0         0         0   \n","1           0  ...         0         0         0         0         0   \n","2           0  ...         0         0         0         0         0   \n","3           0  ...         0         0         0         0         0   \n","4           0  ...         0         0         0         0         0   \n","...       ...  ...       ...       ...       ...       ...       ...   \n","27995       0  ...         0         0         0         0         0   \n","27996       0  ...         0         0         0         0         0   \n","27997       0  ...         0         0         0         0         0   \n","27998       0  ...         0         0         0         0         0   \n","27999       0  ...         0         0         0         0         0   \n","\n","       pixel779  pixel780  pixel781  pixel782  pixel783  \n","0             0         0         0         0         0  \n","1             0         0         0         0         0  \n","2             0         0         0         0         0  \n","3             0         0         0         0         0  \n","4             0         0         0         0         0  \n","...         ...       ...       ...       ...       ...  \n","27995         0         0         0         0         0  \n","27996         0         0         0         0         0  \n","27997         0         0         0         0         0  \n","27998         0         0         0         0         0  \n","27999         0         0         0         0         0  \n","\n","[28000 rows x 784 columns]"],"text/html":["\n","\n","  <div id=\"df-62ef9caa-4fc7-47e8-bf47-61ee4dff7976\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28000 rows × 784 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62ef9caa-4fc7-47e8-bf47-61ee4dff7976')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-e8a2613d-5ee8-445c-aa65-484b5f3e4c05\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8a2613d-5ee8-445c-aa65-484b5f3e4c05')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-e8a2613d-5ee8-445c-aa65-484b5f3e4c05 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-62ef9caa-4fc7-47e8-bf47-61ee4dff7976 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-62ef9caa-4fc7-47e8-bf47-61ee4dff7976');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}],"source":["dfTest"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def create_dataloader(df, batch_size, is_train=True):\n","    if is_train:\n","        # Extracting the labels for training data\n","        labels = torch.tensor(df['label'].values, dtype=torch.long)\n","    else:\n","        # For testing data, labels are not available\n","        labels = torch.zeros(len(df), dtype=torch.long)  # Placeholder; won't be used\n","\n","    # Extracting the image pixels\n","    images = torch.tensor(df.drop(columns=['label'] if is_train else []).values, dtype=torch.float32)\n","    images = images.view(-1, 1, 28, 28)  # Reshaping to match the expected input shape\n","\n","    # Creating a TensorDataset and DataLoader\n","    dataset = TensorDataset(images, labels)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n","\n","    return dataloader\n","\n"],"metadata":{"id":"5osDGgVHsuyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#REALLY SUPER POOPER COOL STUFF"],"metadata":{"id":"sg2W2IhwX7yd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class CNN_Arch(nn.Module):\n","    def __init__(self, num_classes=10, activation='relu', dropout=0.5):\n","        super(CNN_Arch, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n","\n","        # Max pooling layer\n","        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(64 * 5 * 5, 256)  # Updated input size\n","        self.fc2 = nn.Linear(256, num_classes)  # Output size is the number of classes (digits 0-9)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","        # Dropout layer with specified rate\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Convolutional layers with chosen activation function and max pooling\n","        x = self.max_pool(self.activation(self.conv1(x)))  # Shape: (32, 13, 13)\n","        x = self.max_pool(self.activation(self.conv2(x)))  # Shape: (64, 5, 5)\n","\n","        # Flatten the tensor\n","        x = torch.flatten(x, 1)  # Shape: (64 * 5 * 5)\n","\n","        # Fully connected layers with chosen activation function and dropout\n","        x = self.activation(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n"],"metadata":{"id":"VeZYUniesuda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN_Arch(nn.Module):\n","    def __init__(self, num_classes=10, activation='relu', dropout=0.5):\n","        super(CNN_Arch, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","\n","        # Max pooling layer\n","        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Batch normalization layers\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm2d(512)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(512 * 3 * 3, 1024)\n","        self.fc2 = nn.Linear(1024, num_classes)\n","\n","        # Choose activation function\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation == 'gelu':\n","            self.activation = nn.GELU()\n","\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.max_pool(self.activation(self.bn1(self.conv1(x))))\n","        x = self.max_pool(self.activation(self.bn2(self.conv2(x))))\n","        x = self.activation(self.bn3(self.conv3(x)))\n","        x = self.max_pool(self.activation(self.bn4(self.conv4(x))))\n","        x = self.activation(self.bn5(self.conv5(x)))\n","\n","        # Flatten the tensor\n","        x = torch.flatten(x, 1)\n","\n","        # Fully connected layers\n","        x = self.activation(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","class LSTM_Arch(nn.Module):\n","    def __init__(self, input_size,\n","                 hidden_sizes,  # List of hidden sizes\n","                 dropouts,      # List of dropouts\n","                 attention_size):\n","        super(LSTM_Arch, self).__init__()\n","        num_layers = len(hidden_sizes)  # Determine the number of layers from the hidden_sizes list\n","        self.lstms = nn.ModuleList([nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n","                                            hidden_sizes[i],\n","                                            num_layers=1,\n","                                            batch_first=True,\n","                                            dropout=0)  # Set dropout to 0 since num_layers is 1\n","                                    for i in range(num_layers)])\n","        self.attentions = nn.ModuleList([nn.Linear(hidden_sizes[i], attention_size) for i in range(num_layers)])\n","        self.attention_combines = nn.ModuleList([nn.Linear(attention_size, 1) for i in range(num_layers)])\n","        self.fc = nn.Linear(hidden_sizes[-1], 1)\n","\n","\n","\n","    def apply_attention(self, out, attention_layer, attention_combine_layer):\n","        attention_weights = F.softmax(attention_combine_layer(F.tanh(attention_layer(out))), dim=1)\n","        out = torch.bmm(attention_weights.transpose(1, 2), out)\n","        return out.squeeze(1)\n","\n","    def forward(self, x):\n","        for i in range(len(self.lstms)):3\n","            out, _ = self.lstms[i](x if i == 0 else out.unsqueeze(1))\n","            out = self.apply_attention(out, self.attentions[i], self.attention_combines[i])\n","        out = self.fc(out)\n","        return out"],"metadata":{"id":"HPcaHdowhfOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, test_dataloader,device):# Get the device from the model\n","    model.eval() # Set the model to evaluation mode\n","    predictions = []\n","\n","    with torch.no_grad(): # No need to compute gradients during prediction\n","        for images, _ in test_dataloader: # Unpack images and ignore labels\n","            images = images.to(device) # Move images to the device\n","            outputs = model(images) # Get model predictions\n","            _, predicted_labels = torch.max(outputs, 1) # Get the index of the max value\n","            predictions.extend(predicted_labels.cpu().numpy()) # Store predictions\n","\n","    return predictions\n","import random"],"metadata":{"id":"BRoQUSLO3niC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","def train(model, optimizer, train_dataloader, device, loss_function):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_train = []  # Store true labels\n","    predicted_labels_train = []  # Store predicted labels\n","\n","    for images, labels in train_dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","        model.zero_grad()\n","        predictions = model(images)\n","        loss = loss_function(predictions, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        _, predicted_labels = torch.max(predictions, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        true_labels_train.extend(labels.cpu().numpy())  # Add true labels to list\n","        predicted_labels_train.extend(predicted_labels.cpu().numpy())  # Add predicted labels to list\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    accuracy = correct_predictions / len(train_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_train, predicted_labels_train\n","\n","def evaluate(model, val_dataloader, device, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_eval = []  # Store true labels\n","    predicted_labels_eval = []  # Store predicted labels\n","\n","    with torch.no_grad():\n","        for images, labels in val_dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            predictions = model(images)\n","            loss = loss_function(predictions, labels)\n","            total_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted_labels = torch.max(predictions, 1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            true_labels_eval.extend(labels.cpu().numpy())  # Add true labels to list\n","            predicted_labels_eval.extend(predicted_labels.cpu().numpy())  # Add predicted labels to list\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    accuracy = correct_predictions / len(val_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_eval, predicted_labels_eval\n","\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","def objective(trial, dfTrain, device, layers=3):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [3e-5, 1e-2],\n","    \"weight_decay\": [1e-6, 1e-2],\n","    \"batch_size\": [40, 50],\n","    \"epochs\": [5,10],\n","    \"beta1\": [0.8, 0.99],\n","    \"beta2\": [0.9, 0.9999],\n","    \"factor\": [0.05, 0.5],\n","    \"scheduler_patience\": [1, 2],\n","    \"threshold\": [1e-5, 1e-3],\n","    \"cooldown\": [0, 1],\n","    \"min_lr\": [0, 1e-3],\n","    \"dropout\": [0.1, 0.5],  # Only one definition of \"dropout\" is needed\n","    \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'],\n","    'test_size':[0.35,0.45],\n","    'train_size':[0.5,0.75],\n","    'random_state':[42,1024],\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    train_data, val_data_temp = train_test_split(dfTrain, test_size=1- params[\"train_size\"], random_state=params[\"random_state\"])\n","    y_train,y_val_temp=train_test_split(dfTrain['label'], test_size=1- params[\"train_size\"], random_state=params[\"random_state\"])\n","\n","    test_data, val_data = train_test_split(val_data_temp, test_size=1- params[\"test_size\"], random_state=params[\"random_state\"])\n","    y_test, val_y=train_test_split(y_val_temp, test_size=1-params[\"test_size\"], random_state=params[\"random_state\"])\n","\n","\n","\n","\n","    train_dataloader = create_dataloader(train_data, batch_size=params[\"batch_size\"])\n","    val_dataloader = create_dataloader(val_data, batch_size=params[\"batch_size\"])\n","    test_dataloader = create_dataloader(test_data, batch_size=params[\"batch_size\"])\n","\n","\n","\n","\n","\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    val_labels,train_labels = train_test_split(dfTrain['label'], test_size=params[\"test_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['label'])\n","    #all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)\n","\n","    best_local_metric = 0\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        if test_dataloader is not None:\n","            test_preds = predict(model, test_dataloader,device)\n","            print(\"Testing Classification Report:\")\n","            test_accuracy=accuracy_score(y_test, test_preds)\n","            print('-' * 107); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% / {100 * test_accuracy:.4f}% |'); print('-' * 107);\n","            print()\n","        else:\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100);\n","        #metric = valid_loss\n","        metric=train_accuracy+val_accuracy+test_accuracy\n","        if metric > best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric > best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print();print('~'*26);print(f'|Best Value Found {best_local_metric:.5f}|');print('~'*26); print(f'Params = {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54); print()\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader,device)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(y_test, test_preds))\n","                    print('~' * 54)\n","                    print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna(dfTrain, device, layers=3):\n","    global best_state_dict, best_params\n","\n","    study = optuna.create_study(direction=\"maximize\")  # Maximize the accuracy\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    # Load the best model using the best state dictionary\n","    best_model = CNN_Arch(\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n"],"metadata":{"id":"DnKPPIxuda1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"MqNg2bO7sY06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","def objective(trial, dfTrain, device, layers=3):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [3e-5, 1e-2],\n","    \"weight_decay\": [1e-6, 1e-2],\n","    \"batch_size\": [40, 50],\n","    \"epochs\": [5,10],\n","    \"beta1\": [0.8, 0.99],\n","    \"beta2\": [0.9, 0.9999],\n","    \"factor\": [0.05, 0.5],\n","    \"scheduler_patience\": [1, 2],\n","    \"threshold\": [1e-5, 1e-3],\n","    \"cooldown\": [0, 1],\n","    \"min_lr\": [0, 1e-3],\n","    \"dropout\": [0.1, 0.5],  # Only one definition of \"dropout\" is needed\n","    \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'],\n","    'test_size':[0.35,0.45],\n","    'train_size':[0.5,0.75],\n","    'random_state':[42,1024],\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    train_data, val_data_temp = train_test_split(dfTrain, test_size=1- params[\"train_size\"], random_state=params[\"random_state\"])\n","    y_train,y_val_temp=train_test_split(dfTrain['label'], test_size=1- params[\"train_size\"], random_state=params[\"random_state\"])\n","\n","    test_data, val_data = train_test_split(val_data_temp, test_size=1- params[\"test_size\"], random_state=params[\"random_state\"])\n","    y_test, val_y=train_test_split(y_val_temp, test_size=1-params[\"test_size\"], random_state=params[\"random_state\"])\n","\n","\n","\n","\n","    train_dataloader = create_dataloader(train_data, batch_size=params[\"batch_size\"])\n","    val_dataloader = create_dataloader(val_data, batch_size=params[\"batch_size\"])\n","    test_dataloader = create_dataloader(test_data, batch_size=params[\"batch_size\"])\n","\n","\n","\n","\n","\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    val_labels,train_labels = train_test_split(dfTrain['label'], test_size=params[\"test_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['label'])\n","    #all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","\n","    #loss_function = nn.NLLLoss(weight=weights)\n","    loss_function = nn.CrossEntropyLoss(weight=weights)\n","\n","    best_local_metric = 0\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        if test_dataloader is not None:\n","            test_preds = predict(model, test_dataloader,device)\n","            print(\"Testing Classification Report:\")\n","            test_accuracy=accuracy_score(y_test, test_preds)\n","            print('-' * 107); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% / {100 * test_accuracy:.4f}% |'); print('-' * 107);\n","            print()\n","        else:\n","            print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100);\n","        #metric = valid_loss\n","        metric=train_accuracy+val_accuracy+test_accuracy\n","        if metric > best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric > best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print();print('~'*26);print(f'|Best Value Found {best_local_metric:.5f}|');print('~'*26); print(f'Params = {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54); print()\n","                if test_dataloader is not None:\n","                    test_preds = predict(model, test_dataloader,device)\n","                    print(\"Testing Classification Report:\")\n","                    print(classification_report(y_test, test_preds))\n","                    print('~' * 54)\n","                    print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","def tune_model_with_optuna(dfTrain, device, layers=3):\n","    global best_state_dict, best_params\n","\n","    study = optuna.create_study(direction=\"maximize\")  # Maximize the accuracy\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    # Load the best model using the best state dictionary\n","    best_model = CNN_Arch(\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n"],"metadata":{"id":"hJo0nbn-sPf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuned_model=tune_model_with_optuna(dfTrain, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"foyjp3hxsBE6","executionInfo":{"status":"error","timestamp":1692146372263,"user_tz":360,"elapsed":117730,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"c25335b6-2dbf-460e-8bff-5c338d842e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-16 00:37:33,341] A new study created in memory with name: no-name-7cf95a9d-773e-436a-be9e-e52c30658fd4\n"]},{"output_type":"stream","name":"stdout","text":["lr = 0.0004136875361759556\n","weight_decay = 0.003996303491483556\n","batch_size = 46\n","epochs = 6\n","beta1 = 0.9487914163580693\n","beta2 = 0.9457495961583618\n","factor = 0.1340385681811893\n","scheduler_patience = 2\n","threshold = 5.594129250116556e-05\n","cooldown = 1\n","min_lr = 0\n","dropout = 0.2618875587975504\n","activation = tanh\n","test_size = 0.44978099873399513\n","train_size = 0.5968457154065806\n","random_state = 230\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 1 | Loss Train/Valid: 0.211408 / 0.077088 | Accuracy Train/Valid: 93.541% / 97.7997% / 10.1103% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.01451|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.96      0.96      2483\n","           1       0.97      0.97      0.97      2778\n","           2       0.92      0.93      0.92      2461\n","           3       0.93      0.91      0.92      2646\n","           4       0.93      0.94      0.93      2452\n","           5       0.93      0.93      0.93      2266\n","           6       0.94      0.96      0.95      2467\n","           7       0.95      0.94      0.95      2647\n","           8       0.92      0.92      0.92      2372\n","           9       0.89      0.90      0.90      2495\n","\n","    accuracy                           0.94     25067\n","   macro avg       0.93      0.93      0.93     25067\n","weighted avg       0.94      0.94      0.94     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       922\n","           1       1.00      0.99      0.99      1050\n","           2       0.94      0.99      0.96       961\n","           3       0.99      0.96      0.97       914\n","           4       0.97      0.98      0.97       901\n","           5       0.98      0.99      0.99       824\n","           6       0.99      0.98      0.98       905\n","           7       0.98      0.98      0.98       984\n","           8       0.98      0.97      0.98       935\n","           9       0.98      0.95      0.96       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.08      0.08      0.08       727\n","           1       0.12      0.12      0.12       856\n","           2       0.10      0.10      0.10       755\n","           3       0.11      0.11      0.11       791\n","           4       0.09      0.09      0.09       719\n","           5       0.10      0.10      0.10       705\n","           6       0.10      0.10      0.10       765\n","           7       0.11      0.11      0.11       770\n","           8       0.10      0.10      0.10       756\n","           9       0.11      0.11      0.11       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 2 | Loss Train/Valid: 0.074762 / 0.067613 | Accuracy Train/Valid: 97.594% / 98.0358% / 9.6376% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.05268|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.98      0.98      2483\n","           1       0.99      0.99      0.99      2778\n","           2       0.97      0.97      0.97      2461\n","           3       0.98      0.97      0.97      2646\n","           4       0.98      0.98      0.98      2452\n","           5       0.97      0.98      0.97      2266\n","           6       0.98      0.98      0.98      2467\n","           7       0.97      0.97      0.97      2647\n","           8       0.97      0.97      0.97      2372\n","           9       0.96      0.97      0.97      2495\n","\n","    accuracy                           0.98     25067\n","   macro avg       0.98      0.98      0.98     25067\n","weighted avg       0.98      0.98      0.98     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.99       922\n","           1       0.98      1.00      0.99      1050\n","           2       0.99      0.98      0.98       961\n","           3       1.00      0.94      0.97       914\n","           4       0.97      0.98      0.98       901\n","           5       0.97      0.99      0.98       824\n","           6       0.98      0.99      0.99       905\n","           7       0.98      0.98      0.98       984\n","           8       0.97      0.99      0.98       935\n","           9       0.99      0.95      0.97       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.11      0.11      0.11       856\n","           2       0.11      0.11      0.11       755\n","           3       0.11      0.11      0.11       791\n","           4       0.09      0.09      0.09       719\n","           5       0.10      0.10      0.10       705\n","           6       0.10      0.11      0.10       765\n","           7       0.11      0.11      0.11       770\n","           8       0.09      0.09      0.09       756\n","           9       0.10      0.10      0.10       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 3 | Loss Train/Valid: 0.050707 / 0.056175 | Accuracy Train/Valid: 98.380% / 98.4115% / 9.5457% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.06338|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99      2483\n","           1       0.99      0.99      0.99      2778\n","           2       0.98      0.98      0.98      2461\n","           3       0.99      0.99      0.99      2646\n","           4       0.98      0.98      0.98      2452\n","           5       0.99      0.99      0.99      2266\n","           6       0.98      0.99      0.99      2467\n","           7       0.98      0.98      0.98      2647\n","           8       0.98      0.98      0.98      2372\n","           9       0.98      0.98      0.98      2495\n","\n","    accuracy                           0.98     25067\n","   macro avg       0.98      0.98      0.98     25067\n","weighted avg       0.98      0.98      0.98     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      1.00      1.00       922\n","           1       1.00      0.99      0.99      1050\n","           2       0.98      0.98      0.98       961\n","           3       0.99      0.97      0.98       914\n","           4       0.99      0.97      0.98       901\n","           5       0.97      0.99      0.98       824\n","           6       0.99      0.99      0.99       905\n","           7       0.97      0.99      0.98       984\n","           8       0.99      0.98      0.98       935\n","           9       0.96      0.98      0.97       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.11      0.11      0.11       856\n","           2       0.12      0.12      0.12       755\n","           3       0.11      0.11      0.11       791\n","           4       0.09      0.09      0.09       719\n","           5       0.09      0.10      0.09       705\n","           6       0.09      0.09      0.09       765\n","           7       0.12      0.12      0.12       770\n","           8       0.10      0.10      0.10       756\n","           9       0.09      0.09      0.09       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 4 | Loss Train/Valid: 0.039144 / 0.057854 | Accuracy Train/Valid: 98.759% / 98.3900% / 10.3860% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.07535|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99      2483\n","           1       0.99      0.99      0.99      2778\n","           2       0.99      0.99      0.99      2461\n","           3       0.99      0.99      0.99      2646\n","           4       0.99      0.99      0.99      2452\n","           5       0.99      0.99      0.99      2266\n","           6       0.99      0.99      0.99      2467\n","           7       0.99      0.99      0.99      2647\n","           8       0.98      0.98      0.98      2372\n","           9       0.98      0.98      0.98      2495\n","\n","    accuracy                           0.99     25067\n","   macro avg       0.99      0.99      0.99     25067\n","weighted avg       0.99      0.99      0.99     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99       922\n","           1       0.99      0.99      0.99      1050\n","           2       0.99      0.96      0.98       961\n","           3       0.98      0.98      0.98       914\n","           4       0.98      0.99      0.99       901\n","           5       0.97      0.99      0.98       824\n","           6       0.99      0.99      0.99       905\n","           7       0.98      0.99      0.98       984\n","           8       0.99      0.97      0.98       935\n","           9       0.98      0.98      0.98       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.10      0.10      0.10       727\n","           1       0.11      0.11      0.11       856\n","           2       0.10      0.10      0.10       755\n","           3       0.13      0.14      0.13       791\n","           4       0.09      0.09      0.09       719\n","           5       0.10      0.10      0.10       705\n","           6       0.08      0.08      0.08       765\n","           7       0.11      0.11      0.11       770\n","           8       0.10      0.10      0.10       756\n","           9       0.10      0.09      0.10       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 5 | Loss Train/Valid: 0.026285 / 0.052521 | Accuracy Train/Valid: 99.134% / 98.6154% / 9.5982% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 6 | Loss Train/Valid: 0.019282 / 0.070668 | Accuracy Train/Valid: 99.366% / 98.2398% / 10.1103% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.07716|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      1.00      1.00      2778\n","           2       0.99      0.99      0.99      2461\n","           3       0.99      0.99      0.99      2646\n","           4       0.99      0.99      0.99      2452\n","           5       1.00      0.99      1.00      2266\n","           6       0.99      0.99      0.99      2467\n","           7       0.99      0.99      0.99      2647\n","           8       0.99      0.99      0.99      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           0.99     25067\n","   macro avg       0.99      0.99      0.99     25067\n","weighted avg       0.99      0.99      0.99     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       922\n","           1       0.98      0.99      0.99      1050\n","           2       0.97      0.99      0.98       961\n","           3       1.00      0.97      0.99       914\n","           4       0.97      0.99      0.98       901\n","           5       0.97      0.99      0.98       824\n","           6       0.99      0.98      0.99       905\n","           7       0.98      0.99      0.98       984\n","           8       1.00      0.96      0.98       935\n","           9       0.97      0.97      0.97       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.11      0.11      0.11       856\n","           2       0.10      0.10      0.10       755\n","           3       0.09      0.09      0.09       791\n","           4       0.09      0.09      0.09       719\n","           5       0.08      0.08      0.08       705\n","           6       0.11      0.11      0.11       765\n","           7       0.13      0.13      0.13       770\n","           8       0.11      0.11      0.11       756\n","           9       0.08      0.08      0.08       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 7 | Loss Train/Valid: 0.019391 / 0.068440 | Accuracy Train/Valid: 99.398% / 98.4759% / 10.0578% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.07931|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      0.99      1.00      2778\n","           2       0.99      1.00      0.99      2461\n","           3       0.99      0.99      0.99      2646\n","           4       1.00      0.99      1.00      2452\n","           5       0.99      1.00      0.99      2266\n","           6       1.00      1.00      1.00      2467\n","           7       0.99      0.99      0.99      2647\n","           8       0.99      0.99      0.99      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           0.99     25067\n","   macro avg       0.99      0.99      0.99     25067\n","weighted avg       0.99      0.99      0.99     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.99       922\n","           1       1.00      0.99      0.99      1050\n","           2       0.99      0.96      0.98       961\n","           3       0.98      0.98      0.98       914\n","           4       0.98      0.99      0.99       901\n","           5       0.99      0.99      0.99       824\n","           6       0.99      0.99      0.99       905\n","           7       0.99      0.98      0.99       984\n","           8       0.95      1.00      0.98       935\n","           9       0.99      0.97      0.98       921\n","\n","    accuracy                           0.98      9317\n","   macro avg       0.98      0.98      0.98      9317\n","weighted avg       0.98      0.98      0.98      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.11      0.11      0.11       856\n","           2       0.11      0.11      0.11       755\n","           3       0.10      0.10      0.10       791\n","           4       0.10      0.10      0.10       719\n","           5       0.09      0.09      0.09       705\n","           6       0.08      0.08      0.08       765\n","           7       0.10      0.10      0.10       770\n","           8       0.10      0.11      0.11       756\n","           9       0.10      0.10      0.10       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 8 | Loss Train/Valid: 0.019078 / 0.066494 | Accuracy Train/Valid: 99.358% / 98.4866% / 10.0315% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 9 | Loss Train/Valid: 0.015933 / 0.053866 | Accuracy Train/Valid: 99.485% / 98.6476% / 9.8346% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.07968|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      1.00      1.00      2778\n","           2       1.00      1.00      1.00      2461\n","           3       1.00      1.00      1.00      2646\n","           4       0.99      0.99      0.99      2452\n","           5       0.99      0.99      0.99      2266\n","           6       1.00      1.00      1.00      2467\n","           7       1.00      1.00      1.00      2647\n","           8       0.99      0.99      0.99      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           0.99     25067\n","   macro avg       0.99      0.99      0.99     25067\n","weighted avg       0.99      0.99      0.99     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       922\n","           1       0.99      0.99      0.99      1050\n","           2       0.97      0.99      0.98       961\n","           3       1.00      0.98      0.99       914\n","           4       0.99      0.98      0.99       901\n","           5       0.98      0.99      0.99       824\n","           6       0.98      0.99      0.99       905\n","           7       1.00      0.97      0.98       984\n","           8       0.99      0.99      0.99       935\n","           9       0.97      0.99      0.98       921\n","\n","    accuracy                           0.99      9317\n","   macro avg       0.99      0.99      0.99      9317\n","weighted avg       0.99      0.99      0.99      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.12      0.12      0.12       856\n","           2       0.09      0.09      0.09       755\n","           3       0.11      0.11      0.11       791\n","           4       0.08      0.08      0.08       719\n","           5       0.12      0.12      0.12       705\n","           6       0.11      0.11      0.11       765\n","           7       0.09      0.09      0.09       770\n","           8       0.11      0.11      0.11       756\n","           9       0.10      0.10      0.10       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 10 | Loss Train/Valid: 0.013430 / 0.069924 | Accuracy Train/Valid: 99.569% / 98.5832% / 10.1759% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.08328|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      1.00      1.00      2778\n","           2       0.99      1.00      0.99      2461\n","           3       1.00      1.00      1.00      2646\n","           4       1.00      1.00      1.00      2452\n","           5       0.99      1.00      1.00      2266\n","           6       1.00      1.00      1.00      2467\n","           7       0.99      0.99      0.99      2647\n","           8       1.00      1.00      1.00      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           1.00     25067\n","   macro avg       1.00      1.00      1.00     25067\n","weighted avg       1.00      1.00      1.00     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      1.00      0.99       922\n","           1       1.00      0.98      0.99      1050\n","           2       0.98      0.99      0.98       961\n","           3       1.00      0.96      0.98       914\n","           4       0.99      0.99      0.99       901\n","           5       0.98      0.99      0.99       824\n","           6       0.99      0.99      0.99       905\n","           7       0.99      0.98      0.99       984\n","           8       0.97      1.00      0.98       935\n","           9       0.98      0.99      0.98       921\n","\n","    accuracy                           0.99      9317\n","   macro avg       0.99      0.99      0.99      9317\n","weighted avg       0.99      0.99      0.99      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.08      0.08      0.08       727\n","           1       0.12      0.11      0.12       856\n","           2       0.12      0.13      0.13       755\n","           3       0.08      0.08      0.08       791\n","           4       0.09      0.09      0.09       719\n","           5       0.08      0.08      0.08       705\n","           6       0.08      0.08      0.08       765\n","           7       0.09      0.09      0.09       770\n","           8       0.09      0.09      0.09       756\n","           9       0.10      0.11      0.11       772\n","\n","    accuracy                           0.09      7616\n","   macro avg       0.09      0.09      0.09      7616\n","weighted avg       0.09      0.09      0.09      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 11 | Loss Train/Valid: 0.013506 / 0.061213 | Accuracy Train/Valid: 99.537% / 98.6906% / 10.1628% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.08391|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      1.00      1.00      2778\n","           2       1.00      1.00      1.00      2461\n","           3       1.00      1.00      1.00      2646\n","           4       1.00      0.99      0.99      2452\n","           5       1.00      0.99      0.99      2266\n","           6       1.00      1.00      1.00      2467\n","           7       0.99      1.00      1.00      2647\n","           8       0.99      1.00      0.99      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           1.00     25067\n","   macro avg       1.00      1.00      1.00     25067\n","weighted avg       1.00      1.00      1.00     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99       922\n","           1       1.00      0.99      1.00      1050\n","           2       0.98      0.99      0.98       961\n","           3       1.00      0.98      0.99       914\n","           4       0.99      0.98      0.98       901\n","           5       0.98      0.99      0.99       824\n","           6       0.99      0.99      0.99       905\n","           7       0.98      0.99      0.99       984\n","           8       0.99      0.99      0.99       935\n","           9       0.98      0.97      0.97       921\n","\n","    accuracy                           0.99      9317\n","   macro avg       0.99      0.99      0.99      9317\n","weighted avg       0.99      0.99      0.99      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.09      0.09       727\n","           1       0.10      0.10      0.10       856\n","           2       0.11      0.11      0.11       755\n","           3       0.10      0.10      0.10       791\n","           4       0.10      0.10      0.10       719\n","           5       0.10      0.10      0.10       705\n","           6       0.09      0.09      0.09       765\n","           7       0.11      0.11      0.11       770\n","           8       0.09      0.09      0.09       756\n","           9       0.11      0.10      0.11       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 12 | Loss Train/Valid: 0.010069 / 0.101507 | Accuracy Train/Valid: 99.669% / 97.9178% / 9.8739% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 13 | Loss Train/Valid: 0.009053 / 0.075822 | Accuracy Train/Valid: 99.681% / 98.5510% / 10.3335% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","|Best Value Found 2.08565|\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Params = {'lr': 0.0004136875361759556, 'weight_decay': 0.003996303491483556, 'batch_size': 46, 'epochs': 6, 'beta1': 0.9487914163580693, 'beta2': 0.9457495961583618, 'factor': 0.1340385681811893, 'scheduler_patience': 2, 'threshold': 5.594129250116556e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.2618875587975504, 'activation': 'tanh', 'test_size': 0.44978099873399513, 'train_size': 0.5968457154065806, 'random_state': 230}\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2483\n","           1       1.00      1.00      1.00      2778\n","           2       1.00      1.00      1.00      2461\n","           3       1.00      1.00      1.00      2646\n","           4       0.99      0.99      0.99      2452\n","           5       1.00      1.00      1.00      2266\n","           6       1.00      1.00      1.00      2467\n","           7       1.00      1.00      1.00      2647\n","           8       1.00      0.99      1.00      2372\n","           9       0.99      0.99      0.99      2495\n","\n","    accuracy                           1.00     25067\n","   macro avg       1.00      1.00      1.00     25067\n","weighted avg       1.00      1.00      1.00     25067\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99       922\n","           1       0.99      0.99      0.99      1050\n","           2       0.99      0.98      0.98       961\n","           3       0.99      0.98      0.99       914\n","           4       0.97      0.99      0.98       901\n","           5       0.99      0.99      0.99       824\n","           6       0.98      0.99      0.99       905\n","           7       0.98      0.99      0.99       984\n","           8       0.99      0.99      0.99       935\n","           9       0.99      0.95      0.97       921\n","\n","    accuracy                           0.99      9317\n","   macro avg       0.99      0.99      0.99      9317\n","weighted avg       0.99      0.99      0.99      9317\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.10      0.10      0.10       727\n","           1       0.10      0.10      0.10       856\n","           2       0.10      0.10      0.10       755\n","           3       0.10      0.10      0.10       791\n","           4       0.09      0.09      0.09       719\n","           5       0.09      0.09      0.09       705\n","           6       0.09      0.10      0.10       765\n","           7       0.11      0.11      0.11       770\n","           8       0.10      0.10      0.10       756\n","           9       0.11      0.10      0.10       772\n","\n","    accuracy                           0.10      7616\n","   macro avg       0.10      0.10      0.10      7616\n","weighted avg       0.10      0.10      0.10      7616\n","\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 14 | Loss Train/Valid: 0.010541 / 0.070062 | Accuracy Train/Valid: 99.641% / 98.5188% / 10.3729% |\n","-----------------------------------------------------------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-16 00:39:04,109] Trial 0 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 15 | Loss Train/Valid: 0.010103 / 0.063587 | Accuracy Train/Valid: 99.677% / 98.8301% / 9.9527% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","\n","<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","OH NO WE BE OVERFITTING\n","<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","lr = 0.0067811437740502595\n","weight_decay = 5.812072677383952e-05\n","batch_size = 48\n","epochs = 9\n","beta1 = 0.8659390254855196\n","beta2 = 0.9053996439021338\n","factor = 0.21542906076757362\n","scheduler_patience = 2\n","threshold = 4.917690321746687e-05\n","cooldown = 1\n","min_lr = 0\n","dropout = 0.20391941254048238\n","activation = gelu\n","test_size = 0.43134099081493166\n","train_size = 0.5269146977989723\n","random_state = 601\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 1 | Loss Train/Valid: 0.734833 / 0.229525 | Accuracy Train/Valid: 83.434% / 94.1593% / 10.3151% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 2 | Loss Train/Valid: 0.202844 / 0.167283 | Accuracy Train/Valid: 95.576% / 96.6549% / 9.9300% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 3 | Loss Train/Valid: 0.157163 / 0.121590 | Accuracy Train/Valid: 96.566% / 97.6018% / 9.6616% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 4 | Loss Train/Valid: 0.135923 / 0.196414 | Accuracy Train/Valid: 97.379% / 95.6814% / 10.0000% |\n","-----------------------------------------------------------------------------------------------------------\n","\n","Testing Classification Report:\n","-----------------------------------------------------------------------------------------------------------\n","| Epoch 5 | Loss Train/Valid: 0.130078 / 0.133682 | Accuracy Train/Valid: 97.551% / 97.7788% / 9.9067% |\n","-----------------------------------------------------------------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["[W 2023-08-16 00:39:30,925] Trial 1 failed with parameters: {'lr': 0.0067811437740502595, 'weight_decay': 5.812072677383952e-05, 'batch_size': 48, 'epochs': 9, 'beta1': 0.8659390254855196, 'beta2': 0.9053996439021338, 'factor': 0.21542906076757362, 'scheduler_patience': 2, 'threshold': 4.917690321746687e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.20391941254048238, 'activation': 'gelu', 'test_size': 0.43134099081493166, 'train_size': 0.5269146977989723, 'random_state': 601} because of the following error: KeyboardInterrupt().\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-20-d4f471ba35b9>\", line 123, in <lambda>\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","  File \"<ipython-input-20-d4f471ba35b9>\", line 82, in objective\n","    train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","  File \"<ipython-input-17-351fa29d8dd4>\", line 16, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","[W 2023-08-16 00:39:30,927] Trial 1 failed with value None.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-1139cd98d138>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtune_model_with_optuna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-d4f471ba35b9>\u001b[0m in \u001b[0;36mtune_model_with_optuna\u001b[0;34m(dfTrain, device, layers)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Maximize the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-d4f471ba35b9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Maximize the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-d4f471ba35b9>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, dfTrain, device, layers)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mno_improvement_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-351fa29d8dd4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, device, loss_function)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["###fdfsf"],"metadata":{"id":"hkQfdPV6r_37"}},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import AdamW\n","import torch.nn.functional as F\n","\n","\n","from math import sqrt\n","\n","import copy\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","def objective(trial, dfTrain, device, layers=3):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [3e-5, 1e-2],\n","    \"weight_decay\": [1e-6, 1e-2],\n","    \"batch_size\": [40, 50],\n","    \"epochs\": [5,10],\n","    \"beta1\": [0.8, 0.99],\n","    \"beta2\": [0.9, 0.9999],\n","    \"factor\": [0.05, 0.5],\n","    \"scheduler_patience\": [1, 2],\n","    \"threshold\": [1e-5, 1e-3],\n","    \"cooldown\": [0, 1],\n","    \"min_lr\": [0, 1e-3],\n","    \"dropout\": [0.1, 0.5],  # Only one definition of \"dropout\" is needed\n","    \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'],\n","    'test_size':[0.15,0.45],\n","    'random_state':[42,1024],\n","}\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","\n","    train_data, val_data = train_test_split(dfTrain, test_size=params[\"test_size\"], random_state=params[\"random_state\"])\n","    y_train,y_val=train_test_split(dfTrain['label'], test_size=params[\"test_size\"], random_state=params[\"random_state\"])\n","    #train_data, val_data = train_test_split(dfTrain, test_size=0.3, random_state=52)\n","    #y_train,y_val=train_test_split(dfTrain['label'], test_size=0.3, random_state=52)\n","\n","    # Create data loaders for training and validation data\n","    train_dataloader = create_dataloader(train_data, batch_size=params[\"batch_size\"])\n","    val_dataloader = create_dataloader(val_data, batch_size=params[\"batch_size\"])  # Using the same function for validation data\n","\n","    # Instantiate the model using the new CNN_Arch class\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","\n","    model = model.to(device)\n","\n","\n","    optimizer = AdamW(model.parameters(),\n","                      lr=params[\"lr\"],\n","                      betas=(params[\"beta1\"], params[\"beta2\"]),\n","                      #eps=params[\"eps_optimizer\"],\n","                      weight_decay=params[\"weight_decay\"])\n","\n","    scheduler = ReduceLROnPlateau(optimizer,\n","                                  mode='min',\n","                                  factor=params[\"factor\"],\n","                                  patience=params[\"scheduler_patience\"],\n","                                  verbose=False,\n","                                  threshold=params[\"threshold\"],\n","                                  threshold_mode='rel',\n","                                  cooldown=params[\"cooldown\"],\n","                                  min_lr=params[\"min_lr\"],\n","                                  #eps=params[\"scheduler_eps\"]\n","                                  )\n","\n","    loss_function = nn.CrossEntropyLoss()\n","    best_local_metric=0\n","\n","    separator = '=' * 180\n","    valid_losses=[]\n","\n","\n","\n","    for epoch in range(params[\"epochs\"]):\n","        print('-'*14)\n","        print(f'|Epoch {epoch + 1} / {params[\"epochs\"]}|')\n","        print('-'*31)\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","\n","        print(f'|Loss Train/Valid:   {train_loss:.4f}  /  {valid_loss:.4f}  |')\n","        print(f'|Accuracy Train/Valid:   {100*train_accuracy:.4f}%  /  {100*val_accuracy:.4f}%  |')\n","        print('-'*47)\n","\n","        if val_accuracy >    best_local_metric:\n","            best_local_metric=val_accuracy\n","            if best_local_metric > best_valid_metric:\n","                best_valid_metric = val_accuracy\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params = params\n","                print()\n","                print('^'*100)\n","                print('MODEL HAVE BEEN SAVED BEAWARE')\n","                print('^'*100)\n","                print()\n","        valid_losses.append(valid_loss)\n","\n","    print(\"Training Classification Report:\")\n","    print(classification_report(true_labels_train, predicted_labels_train))\n","    print(\"Validation Classification Report:\")\n","    print(classification_report(true_labels_eval, predicted_labels_eval))\n","\n","    print('*' * 180)\n","    print('                                                                                    TRIAL=GUD')\n","    print('*' * 180)\n","    print()\n","\n","\n","    torch.save(model.state_dict(), f'saved_weights_trial_{trial.number}.pt')\n","    avg_valid_loss = sum(valid_losses) / len(valid_losses)\n","\n","    print(('-----------------------'))\n","\n","\n","\n","    return val_accuracy\n","\n","\n","\n","def tune_model_with_optuna(dfTrain, device, layers=3):\n","    global best_state_dict, best_params\n","\n","    study = optuna.create_study(direction=\"maximize\")  # Maximize the accuracy\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    # Load the best model using the best state dictionary\n","    best_model = CNN_Arch(\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n"],"metadata":{"id":"3-u8Uf5swbL5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#here"],"metadata":{"id":"GgD8llE3zcVg"}},{"cell_type":"code","source":["best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","\n","def tune_model_with_optuna(dfTrain, device, layers=3):\n","    global best_state_dict, best_params\n","\n","    study = optuna.create_study(direction=\"minimize\")  # Maximize the accuracy\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    # Load the best model using the best state dictionary\n","    best_model = CNN_Arch(\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n","tuned_model=tune_model_with_optuna(dfTrain, device)"],"metadata":{"id":"W_6Bzm5vwdOa","executionInfo":{"status":"ok","timestamp":1691976449653,"user_tz":360,"elapsed":219939,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"538b8568-7177-4d32-c3c8-d67c2e101637"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:23:49,466] A new study created in memory with name: no-name-ec35dfdc-fcf3-4c4b-af84-8b1994e4cb48\n"]},{"output_type":"stream","name":"stdout","text":["lr = 0.007435501294031588\n","weight_decay = 1.2268231483339016e-05\n","batch_size = 44\n","epochs = 7\n","beta1 = 0.8569527533160558\n","beta2 = 0.9121194410267074\n","factor = 0.3389174191734056\n","scheduler_patience = 2\n","threshold = 0.00010591442586341901\n","cooldown = 1\n","min_lr = 0\n","dropout = 0.34710101063456267\n","activation = gelu\n","test_size = 0.32379792894039017\n","random_state = 898\n","--------------\n","|Epoch 1 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.6597  /  0.2628  |\n","|Accuracy Train/Valid:   85.3134%  /  94.3603%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 2 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.2467  /  0.4395  |\n","|Accuracy Train/Valid:   95.1127%  /  93.8676%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 3 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.2350  /  0.1408  |\n","|Accuracy Train/Valid:   95.9789%  /  97.4485%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 4 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.2151  /  0.2537  |\n","|Accuracy Train/Valid:   96.7782%  /  97.3971%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 5 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.1987  /  0.2035  |\n","|Accuracy Train/Valid:   97.1232%  /  97.8971%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 6 / 7|\n","-------------------------------\n","|Loss Train/Valid:   0.2028  /  0.2922  |\n","|Accuracy Train/Valid:   97.4965%  /  98.0147%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 7 / 7|\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:24:32,420] Trial 0 finished with value: 0.9847794117647058 and parameters: {'lr': 0.007435501294031588, 'weight_decay': 1.2268231483339016e-05, 'batch_size': 44, 'epochs': 7, 'beta1': 0.8569527533160558, 'beta2': 0.9121194410267074, 'factor': 0.3389174191734056, 'scheduler_patience': 2, 'threshold': 0.00010591442586341901, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.34710101063456267, 'activation': 'gelu', 'test_size': 0.32379792894039017, 'random_state': 898}. Best is trial 0 with value: 0.9847794117647058.\n"]},{"output_type":"stream","name":"stdout","text":["|Loss Train/Valid:   0.2355  /  0.1204  |\n","|Accuracy Train/Valid:   97.8380%  /  98.4779%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.98      2801\n","           1       0.99      0.98      0.99      3165\n","           2       0.97      0.98      0.97      2784\n","           3       0.98      0.98      0.98      2966\n","           4       0.98      0.98      0.98      2805\n","           5       0.98      0.98      0.98      2541\n","           6       0.98      0.98      0.98      2788\n","           7       0.98      0.98      0.98      2979\n","           8       0.97      0.97      0.97      2732\n","           9       0.96      0.97      0.97      2839\n","\n","    accuracy                           0.98     28400\n","   macro avg       0.98      0.98      0.98     28400\n","weighted avg       0.98      0.98      0.98     28400\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.97      0.99      1331\n","           1       0.98      0.99      0.98      1519\n","           2       0.99      0.99      0.99      1393\n","           3       0.99      0.99      0.99      1385\n","           4       0.99      0.98      0.98      1267\n","           5       0.98      0.99      0.98      1254\n","           6       0.99      0.98      0.99      1349\n","           7       0.97      0.99      0.98      1422\n","           8       0.98      0.99      0.98      1331\n","           9       0.98      0.97      0.98      1349\n","\n","    accuracy                           0.98     13600\n","   macro avg       0.98      0.98      0.98     13600\n","weighted avg       0.98      0.98      0.98     13600\n","\n","************************************************************************************************************************************************************************************\n","                                                                                    TRIAL=GUD\n","************************************************************************************************************************************************************************************\n","\n","-----------------------\n","lr = 0.0017810330702433556\n","weight_decay = 6.271757591985645e-06\n","batch_size = 42\n","epochs = 6\n","beta1 = 0.8166998638814977\n","beta2 = 0.929306647983458\n","factor = 0.2297691508626524\n","scheduler_patience = 1\n","threshold = 4.425069712027412e-05\n","cooldown = 1\n","min_lr = 0\n","dropout = 0.10957918637430152\n","activation = leaky_relu\n","test_size = 0.1830523619624917\n","random_state = 273\n","--------------\n","|Epoch 1 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.2495  /  0.1147  |\n","|Accuracy Train/Valid:   93.7192%  /  97.3208%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 2 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.1145  /  0.0928  |\n","|Accuracy Train/Valid:   97.5431%  /  97.5940%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 3 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0784  /  0.0536  |\n","|Accuracy Train/Valid:   98.1872%  /  98.6604%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 4 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0581  /  0.0714  |\n","|Accuracy Train/Valid:   98.7293%  /  98.4523%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 5 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0526  /  0.0648  |\n","|Accuracy Train/Valid:   98.8692%  /  98.2312%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 6 / 6|\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:25:15,384] Trial 1 finished with value: 0.9834828976459877 and parameters: {'lr': 0.0017810330702433556, 'weight_decay': 6.271757591985645e-06, 'batch_size': 42, 'epochs': 6, 'beta1': 0.8166998638814977, 'beta2': 0.929306647983458, 'factor': 0.2297691508626524, 'scheduler_patience': 1, 'threshold': 4.425069712027412e-05, 'cooldown': 1, 'min_lr': 0, 'dropout': 0.10957918637430152, 'activation': 'leaky_relu', 'test_size': 0.1830523619624917, 'random_state': 273}. Best is trial 0 with value: 0.9847794117647058.\n"]},{"output_type":"stream","name":"stdout","text":["|Loss Train/Valid:   0.0402  /  0.0769  |\n","|Accuracy Train/Valid:   99.0382%  /  98.3483%  |\n","-----------------------------------------------\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99      3391\n","           1       0.99      0.99      0.99      3781\n","           2       0.99      0.99      0.99      3404\n","           3       0.99      0.99      0.99      3546\n","           4       0.99      0.99      0.99      3315\n","           5       0.99      0.99      0.99      3109\n","           6       0.99      0.99      0.99      3374\n","           7       0.99      0.99      0.99      3597\n","           8       0.99      0.99      0.99      3376\n","           9       0.99      0.99      0.99      3418\n","\n","    accuracy                           0.99     34311\n","   macro avg       0.99      0.99      0.99     34311\n","weighted avg       0.99      0.99      0.99     34311\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      1.00      0.99       741\n","           1       0.99      0.99      0.99       903\n","           2       0.98      0.99      0.99       773\n","           3       1.00      0.99      0.99       805\n","           4       1.00      0.94      0.97       757\n","           5       0.99      0.99      0.99       686\n","           6       1.00      0.99      0.99       763\n","           7       1.00      0.97      0.98       804\n","           8       0.94      1.00      0.97       687\n","           9       0.96      0.98      0.97       770\n","\n","    accuracy                           0.98      7689\n","   macro avg       0.98      0.98      0.98      7689\n","weighted avg       0.98      0.98      0.98      7689\n","\n","************************************************************************************************************************************************************************************\n","                                                                                    TRIAL=GUD\n","************************************************************************************************************************************************************************************\n","\n","-----------------------\n","lr = 0.00019984026250965007\n","weight_decay = 0.0001775234115817902\n","batch_size = 44\n","epochs = 8\n","beta1 = 0.9857522578246692\n","beta2 = 0.9071524088460314\n","factor = 0.09000597115331258\n","scheduler_patience = 1\n","threshold = 0.0003318670188463854\n","cooldown = 0\n","min_lr = 0\n","dropout = 0.41462293866273514\n","activation = sigmoid\n","test_size = 0.4105785240038028\n","random_state = 666\n","--------------\n","|Epoch 1 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.7684  /  0.4172  |\n","|Accuracy Train/Valid:   76.2230%  /  88.4024%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 2 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.5686  /  2.3846  |\n","|Accuracy Train/Valid:   81.2482%  /  9.0693%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 3 / 8|\n","-------------------------------\n","|Loss Train/Valid:   1.3983  /  0.9545  |\n","|Accuracy Train/Valid:   50.0990%  /  68.9301%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 4 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.8435  /  0.6680  |\n","|Accuracy Train/Valid:   71.1452%  /  78.1734%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 5 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.5746  /  0.4221  |\n","|Accuracy Train/Valid:   80.7271%  /  87.6312%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 6 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.4026  /  0.3564  |\n","|Accuracy Train/Valid:   87.4652%  /  90.1305%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 7 / 8|\n","-------------------------------\n","|Loss Train/Valid:   0.3111  /  0.2455  |\n","|Accuracy Train/Valid:   90.6282%  /  92.7863%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 8 / 8|\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:26:00,506] Trial 2 finished with value: 0.9438677877645695 and parameters: {'lr': 0.00019984026250965007, 'weight_decay': 0.0001775234115817902, 'batch_size': 44, 'epochs': 8, 'beta1': 0.9857522578246692, 'beta2': 0.9071524088460314, 'factor': 0.09000597115331258, 'scheduler_patience': 1, 'threshold': 0.0003318670188463854, 'cooldown': 0, 'min_lr': 0, 'dropout': 0.41462293866273514, 'activation': 'sigmoid', 'test_size': 0.4105785240038028, 'random_state': 666}. Best is trial 0 with value: 0.9847794117647058.\n"]},{"output_type":"stream","name":"stdout","text":["|Loss Train/Valid:   0.2446  /  0.1914  |\n","|Accuracy Train/Valid:   92.7247%  /  94.3868%  |\n","-----------------------------------------------\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.96      0.96      2400\n","           1       0.98      0.97      0.97      2782\n","           2       0.93      0.92      0.93      2465\n","           3       0.91      0.90      0.91      2568\n","           4       0.94      0.93      0.94      2425\n","           5       0.91      0.90      0.90      2231\n","           6       0.95      0.96      0.95      2417\n","           7       0.95      0.94      0.94      2614\n","           8       0.88      0.88      0.88      2355\n","           9       0.88      0.89      0.88      2498\n","\n","    accuracy                           0.93     24755\n","   macro avg       0.93      0.93      0.93     24755\n","weighted avg       0.93      0.93      0.93     24755\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.99      0.96      1732\n","           1       0.99      0.97      0.98      1902\n","           2       0.95      0.94      0.95      1712\n","           3       0.97      0.87      0.92      1783\n","           4       0.94      0.97      0.95      1647\n","           5       0.93      0.94      0.93      1564\n","           6       0.97      0.95      0.96      1720\n","           7       0.94      0.96      0.95      1787\n","           8       0.89      0.94      0.92      1708\n","           9       0.93      0.90      0.92      1690\n","\n","    accuracy                           0.94     17245\n","   macro avg       0.94      0.94      0.94     17245\n","weighted avg       0.94      0.94      0.94     17245\n","\n","************************************************************************************************************************************************************************************\n","                                                                                    TRIAL=GUD\n","************************************************************************************************************************************************************************************\n","\n","-----------------------\n","lr = 3.204201922485777e-05\n","weight_decay = 0.006116774282464299\n","batch_size = 47\n","epochs = 6\n","beta1 = 0.8895549087061183\n","beta2 = 0.9369990853374751\n","factor = 0.11762974515228268\n","scheduler_patience = 2\n","threshold = 3.196173824327435e-05\n","cooldown = 0\n","min_lr = 0\n","dropout = 0.22840716750332216\n","activation = leaky_relu\n","test_size = 0.40560353073372113\n","random_state = 338\n","--------------\n","|Epoch 1 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.2523  /  0.0682  |\n","|Accuracy Train/Valid:   93.8712%  /  97.8692%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 2 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0410  /  0.0457  |\n","|Accuracy Train/Valid:   98.7422%  /  98.5325%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 3 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0179  /  0.0473  |\n","|Accuracy Train/Valid:   99.4752%  /  98.4797%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 4 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0072  /  0.0484  |\n","|Accuracy Train/Valid:   99.8197%  /  98.5677%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 5 / 6|\n","-------------------------------\n","|Loss Train/Valid:   0.0043  /  0.0463  |\n","|Accuracy Train/Valid:   99.8798%  /  98.8025%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","--------------\n","|Epoch 6 / 6|\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:26:32,957] Trial 3 finished with value: 0.9880840572904438 and parameters: {'lr': 3.204201922485777e-05, 'weight_decay': 0.006116774282464299, 'batch_size': 47, 'epochs': 6, 'beta1': 0.8895549087061183, 'beta2': 0.9369990853374751, 'factor': 0.11762974515228268, 'scheduler_patience': 2, 'threshold': 3.196173824327435e-05, 'cooldown': 0, 'min_lr': 0, 'dropout': 0.22840716750332216, 'activation': 'leaky_relu', 'test_size': 0.40560353073372113, 'random_state': 338}. Best is trial 3 with value: 0.9880840572904438.\n"]},{"output_type":"stream","name":"stdout","text":["|Loss Train/Valid:   0.0029  /  0.0463  |\n","|Accuracy Train/Valid:   99.9239%  /  98.8084%  |\n","-----------------------------------------------\n","\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","MODEL HAVE BEEN SAVED BEAWARE\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      2478\n","           1       1.00      1.00      1.00      2772\n","           2       1.00      1.00      1.00      2475\n","           3       1.00      1.00      1.00      2598\n","           4       1.00      1.00      1.00      2411\n","           5       1.00      1.00      1.00      2250\n","           6       1.00      1.00      1.00      2452\n","           7       1.00      1.00      1.00      2625\n","           8       1.00      1.00      1.00      2453\n","           9       1.00      1.00      1.00      2450\n","\n","    accuracy                           1.00     24964\n","   macro avg       1.00      1.00      1.00     24964\n","weighted avg       1.00      1.00      1.00     24964\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99      1654\n","           1       1.00      0.99      0.99      1912\n","           2       0.99      0.99      0.99      1702\n","           3       0.99      0.99      0.99      1753\n","           4       0.99      0.98      0.99      1661\n","           5       0.99      0.98      0.99      1545\n","           6       0.99      0.99      0.99      1685\n","           7       0.99      0.99      0.99      1776\n","           8       0.98      0.98      0.98      1610\n","           9       0.96      0.99      0.98      1738\n","\n","    accuracy                           0.99     17036\n","   macro avg       0.99      0.99      0.99     17036\n","weighted avg       0.99      0.99      0.99     17036\n","\n","************************************************************************************************************************************************************************************\n","                                                                                    TRIAL=GUD\n","************************************************************************************************************************************************************************************\n","\n","-----------------------\n","lr = 0.00839890469525455\n","weight_decay = 2.574719927822453e-05\n","batch_size = 45\n","epochs = 10\n","beta1 = 0.8451051998105569\n","beta2 = 0.9068682328203357\n","factor = 0.12517644716009996\n","scheduler_patience = 2\n","threshold = 7.536735450984983e-05\n","cooldown = 0\n","min_lr = 0\n","dropout = 0.2555453316397311\n","activation = sigmoid\n","test_size = 0.394345133470221\n","random_state = 1010\n","--------------\n","|Epoch 1 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7487  /  2.8200  |\n","|Accuracy Train/Valid:   10.3235%  /  9.8412%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 2 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7622  /  2.8575  |\n","|Accuracy Train/Valid:   9.9894%  /  9.9318%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 3 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7326  /  2.9407  |\n","|Accuracy Train/Valid:   10.3393%  /  9.8412%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 4 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7266  /  2.4779  |\n","|Accuracy Train/Valid:   10.1191%  /  10.5416%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 5 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7370  /  2.8199  |\n","|Accuracy Train/Valid:   10.2371%  /  10.2940%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 6 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7294  /  2.6105  |\n","|Accuracy Train/Valid:   9.7968%  /  10.5416%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 7 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7075  /  2.6181  |\n","|Accuracy Train/Valid:   10.1938%  /  10.5416%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 8 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7255  /  2.5967  |\n","|Accuracy Train/Valid:   10.1230%  /  10.2940%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 9 / 10|\n","-------------------------------\n","|Loss Train/Valid:   2.7189  /  2.6546  |\n","|Accuracy Train/Valid:   10.0759%  /  9.8171%  |\n","-----------------------------------------------\n","--------------\n","|Epoch 10 / 10|\n","-------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["[I 2023-08-14 01:27:28,793] Trial 4 finished with value: 0.09817062126426372 and parameters: {'lr': 0.00839890469525455, 'weight_decay': 2.574719927822453e-05, 'batch_size': 45, 'epochs': 10, 'beta1': 0.8451051998105569, 'beta2': 0.9068682328203357, 'factor': 0.12517644716009996, 'scheduler_patience': 2, 'threshold': 7.536735450984983e-05, 'cooldown': 0, 'min_lr': 0, 'dropout': 0.2555453316397311, 'activation': 'sigmoid', 'test_size': 0.394345133470221, 'random_state': 1010}. Best is trial 3 with value: 0.9880840572904438.\n"]},{"output_type":"stream","name":"stdout","text":["|Loss Train/Valid:   2.7214  /  2.6575  |\n","|Accuracy Train/Valid:   9.9855%  /  9.8171%  |\n","-----------------------------------------------\n","Training Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.10      0.10      0.10      2535\n","           1       0.12      0.13      0.12      2874\n","           2       0.11      0.11      0.11      2617\n","           3       0.10      0.10      0.10      2605\n","           4       0.09      0.09      0.09      2446\n","           5       0.09      0.08      0.08      2297\n","           6       0.09      0.09      0.09      2432\n","           7       0.10      0.11      0.11      2655\n","           8       0.10      0.09      0.09      2433\n","           9       0.10      0.10      0.10      2543\n","\n","    accuracy                           0.10     25437\n","   macro avg       0.10      0.10      0.10     25437\n","weighted avg       0.10      0.10      0.10     25437\n","\n","Validation Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1597\n","           1       0.00      0.00      0.00      1810\n","           2       0.00      0.00      0.00      1560\n","           3       0.00      0.00      0.00      1746\n","           4       0.10      1.00      0.18      1626\n","           5       0.00      0.00      0.00      1498\n","           6       0.00      0.00      0.00      1705\n","           7       0.00      0.00      0.00      1746\n","           8       0.00      0.00      0.00      1630\n","           9       0.00      0.00      0.00      1645\n","\n","    accuracy                           0.10     16563\n","   macro avg       0.01      0.10      0.02     16563\n","weighted avg       0.01      0.10      0.02     16563\n","\n","************************************************************************************************************************************************************************************\n","                                                                                    TRIAL=GUD\n","************************************************************************************************************************************************************************************\n","\n","-----------------------\n","Best trial found with value: 0.988\n","Best parameters:\n","    lr: 3.204201922485777e-05\n","    weight_decay: 0.006116774282464299\n","    batch_size: 47\n","    epochs: 6\n","    beta1: 0.8895549087061183\n","    beta2: 0.9369990853374751\n","    factor: 0.11762974515228268\n","    scheduler_patience: 2\n","    threshold: 3.196173824327435e-05\n","    cooldown: 0\n","    min_lr: 0\n","    dropout: 0.22840716750332216\n","    activation: leaky_relu\n","    test_size: 0.40560353073372113\n","    random_state: 338\n"]}]},{"cell_type":"code","source":["\n","test_dataloader=create_dataloader(dfTest, batch_size=best_params['batch_size'], is_train=False)\n","tuned_model=tuned_model.to(device)\n","test_predictions = predict(tuned_model, test_dataloader,device)\n","create_and_download_submission(test_predictions, 'submission.csv')"],"metadata":{"id":"M_0mXbkI_2U7","executionInfo":{"status":"ok","timestamp":1691975079956,"user_tz":360,"elapsed":1653,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"81baa4f2-b30e-4206-bdab-a801196c8034"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_39ee4da3-fa3d-4cc4-b35b-3c04a211f5fe\", \"submission.csv\", 212908)"]},"metadata":{}}]},{"cell_type":"code","source":["test_predictions = predict(tuned_model, test_dataloader,device)\n","create_and_download_submission(test_predictions, 'submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"8RWtzeRH1DEH","executionInfo":{"status":"ok","timestamp":1691974030254,"user_tz":360,"elapsed":1977,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"820f3c19-ddcf-4d56-9356-11b6336da08e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1f5bbff6-7911-4c4c-b9ab-c37721c8bc9a\", \"submission.csv\", 212908)"]},"metadata":{}}]},{"cell_type":"code","source":["test_predictions[0]"],"metadata":{"id":"1m4mnmnF2R4u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691971040375,"user_tz":360,"elapsed":7,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"97854eca-a934-46d2-8aed-4068a965fc24"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":164}]},{"cell_type":"code","source":["test_predictions"],"metadata":{"id":"PH-qeEi74Ppi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691971040375,"user_tz":360,"elapsed":492,"user":{"displayName":"Irek","userId":"02915086061816371557"}},"outputId":"b5a0c500-7c10-4f1f-accb-70cf4c2e9f45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2,\n"," 0,\n"," 9,\n"," 9,\n"," 3,\n"," 7,\n"," 0,\n"," 3,\n"," 0,\n"," 3,\n"," 5,\n"," 7,\n"," 4,\n"," 0,\n"," 4,\n"," 3,\n"," 3,\n"," 1,\n"," 9,\n"," 0,\n"," 9,\n"," 1,\n"," 1,\n"," 5,\n"," 7,\n"," 4,\n"," 2,\n"," 7,\n"," 4,\n"," 7,\n"," 7,\n"," 5,\n"," 4,\n"," 2,\n"," 6,\n"," 2,\n"," 5,\n"," 5,\n"," 1,\n"," 6,\n"," 7,\n"," 7,\n"," 4,\n"," 9,\n"," 8,\n"," 7,\n"," 8,\n"," 2,\n"," 6,\n"," 7,\n"," 6,\n"," 8,\n"," 8,\n"," 3,\n"," 8,\n"," 2,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 4,\n"," 1,\n"," 7,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 9,\n"," 0,\n"," 1,\n"," 6,\n"," 5,\n"," 8,\n"," 8,\n"," 2,\n"," 8,\n"," 9,\n"," 9,\n"," 2,\n"," 3,\n"," 5,\n"," 4,\n"," 1,\n"," 0,\n"," 9,\n"," 2,\n"," 4,\n"," 3,\n"," 6,\n"," 7,\n"," 2,\n"," 0,\n"," 6,\n"," 6,\n"," 1,\n"," 4,\n"," 3,\n"," 9,\n"," 7,\n"," 4,\n"," 0,\n"," 9,\n"," 2,\n"," 0,\n"," 7,\n"," 3,\n"," 0,\n"," 5,\n"," 0,\n"," 8,\n"," 0,\n"," 0,\n"," 4,\n"," 7,\n"," 1,\n"," 7,\n"," 1,\n"," 1,\n"," 3,\n"," 3,\n"," 3,\n"," 7,\n"," 2,\n"," 8,\n"," 6,\n"," 3,\n"," 8,\n"," 7,\n"," 7,\n"," 4,\n"," 3,\n"," 5,\n"," 6,\n"," 0,\n"," 0,\n"," 0,\n"," 3,\n"," 1,\n"," 3,\n"," 6,\n"," 4,\n"," 3,\n"," 4,\n"," 5,\n"," 5,\n"," 8,\n"," 7,\n"," 7,\n"," 2,\n"," 8,\n"," 4,\n"," 3,\n"," 5,\n"," 6,\n"," 5,\n"," 3,\n"," 7,\n"," 5,\n"," 7,\n"," 8,\n"," 3,\n"," 0,\n"," 4,\n"," 5,\n"," 1,\n"," 3,\n"," 7,\n"," 6,\n"," 3,\n"," 0,\n"," 2,\n"," 7,\n"," 8,\n"," 6,\n"," 1,\n"," 3,\n"," 7,\n"," 4,\n"," 1,\n"," 2,\n"," 4,\n"," 8,\n"," 5,\n"," 2,\n"," 4,\n"," 9,\n"," 2,\n"," 1,\n"," 6,\n"," 0,\n"," 6,\n"," 1,\n"," 4,\n"," 9,\n"," 6,\n"," 0,\n"," 9,\n"," 7,\n"," 6,\n"," 9,\n"," 1,\n"," 9,\n"," 0,\n"," 9,\n"," 9,\n"," 0,\n"," 8,\n"," 4,\n"," 6,\n"," 2,\n"," 0,\n"," 9,\n"," 3,\n"," 6,\n"," 3,\n"," 2,\n"," 1,\n"," 6,\n"," 3,\n"," 4,\n"," 2,\n"," 3,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 4,\n"," 6,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 9,\n"," 1,\n"," 7,\n"," 3,\n"," 2,\n"," 3,\n"," 8,\n"," 6,\n"," 8,\n"," 6,\n"," 2,\n"," 8,\n"," 5,\n"," 5,\n"," 4,\n"," 8,\n"," 3,\n"," 5,\n"," 9,\n"," 7,\n"," 1,\n"," 3,\n"," 8,\n"," 4,\n"," 5,\n"," 1,\n"," 4,\n"," 5,\n"," 6,\n"," 3,\n"," 3,\n"," 5,\n"," 7,\n"," 0,\n"," 6,\n"," 8,\n"," 3,\n"," 1,\n"," 6,\n"," 0,\n"," 6,\n"," 3,\n"," 9,\n"," 3,\n"," 1,\n"," 5,\n"," 8,\n"," 4,\n"," 0,\n"," 9,\n"," 2,\n"," 0,\n"," 5,\n"," 3,\n"," 7,\n"," 1,\n"," 9,\n"," 9,\n"," 5,\n"," 7,\n"," 7,\n"," 9,\n"," 9,\n"," 6,\n"," 3,\n"," 0,\n"," 3,\n"," 3,\n"," 6,\n"," 9,\n"," 8,\n"," 2,\n"," 6,\n"," 3,\n"," 7,\n"," 1,\n"," 4,\n"," 5,\n"," 8,\n"," 5,\n"," 9,\n"," 0,\n"," 0,\n"," 3,\n"," 8,\n"," 4,\n"," 1,\n"," 8,\n"," 4,\n"," 1,\n"," 1,\n"," 9,\n"," 8,\n"," 4,\n"," 5,\n"," 1,\n"," 5,\n"," 3,\n"," 6,\n"," 3,\n"," 1,\n"," 3,\n"," 0,\n"," 9,\n"," 0,\n"," 0,\n"," 6,\n"," 0,\n"," 6,\n"," 3,\n"," 1,\n"," 8,\n"," 6,\n"," 0,\n"," 6,\n"," 5,\n"," 2,\n"," 2,\n"," 6,\n"," 7,\n"," 7,\n"," 2,\n"," 5,\n"," 8,\n"," 3,\n"," 9,\n"," 2,\n"," 7,\n"," 8,\n"," 6,\n"," 3,\n"," 8,\n"," 4,\n"," 2,\n"," 3,\n"," 8,\n"," 1,\n"," 6,\n"," 4,\n"," 8,\n"," 7,\n"," 9,\n"," 7,\n"," 6,\n"," 9,\n"," 5,\n"," 3,\n"," 7,\n"," 6,\n"," 5,\n"," 5,\n"," 4,\n"," 2,\n"," 6,\n"," 2,\n"," 1,\n"," 3,\n"," 7,\n"," 1,\n"," 7,\n"," 9,\n"," 9,\n"," 6,\n"," 1,\n"," 1,\n"," 1,\n"," 7,\n"," 3,\n"," 9,\n"," 7,\n"," 6,\n"," 1,\n"," 1,\n"," 1,\n"," 9,\n"," 3,\n"," 8,\n"," 5,\n"," 5,\n"," 0,\n"," 4,\n"," 1,\n"," 2,\n"," 3,\n"," 1,\n"," 1,\n"," 3,\n"," 5,\n"," 9,\n"," 6,\n"," 6,\n"," 5,\n"," 3,\n"," 1,\n"," 4,\n"," 7,\n"," 4,\n"," 7,\n"," 4,\n"," 8,\n"," 5,\n"," 2,\n"," 6,\n"," 1,\n"," 3,\n"," 9,\n"," 5,\n"," 0,\n"," 8,\n"," 4,\n"," 7,\n"," 4,\n"," 4,\n"," 4,\n"," 1,\n"," 5,\n"," 3,\n"," 9,\n"," 5,\n"," 7,\n"," 6,\n"," 9,\n"," 5,\n"," 9,\n"," 2,\n"," 3,\n"," 5,\n"," 6,\n"," 1,\n"," 7,\n"," 5,\n"," 0,\n"," 5,\n"," 1,\n"," 7,\n"," 4,\n"," 4,\n"," 1,\n"," 1,\n"," 4,\n"," 9,\n"," 5,\n"," 6,\n"," 0,\n"," 1,\n"," 3,\n"," 1,\n"," 0,\n"," 4,\n"," 8,\n"," 1,\n"," 2,\n"," 7,\n"," 9,\n"," 4,\n"," 8,\n"," 3,\n"," 7,\n"," 7,\n"," 4,\n"," 2,\n"," 4,\n"," 6,\n"," 7,\n"," 6,\n"," 3,\n"," 2,\n"," 0,\n"," 6,\n"," 5,\n"," 9,\n"," 4,\n"," 1,\n"," 8,\n"," 3,\n"," 3,\n"," 0,\n"," 2,\n"," 7,\n"," 5,\n"," 8,\n"," 7,\n"," 5,\n"," 3,\n"," 5,\n"," 7,\n"," 4,\n"," 3,\n"," 6,\n"," 9,\n"," 0,\n"," 7,\n"," 7,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 7,\n"," 0,\n"," 5,\n"," 3,\n"," 8,\n"," 3,\n"," 5,\n"," 6,\n"," 5,\n"," 7,\n"," 3,\n"," 0,\n"," 2,\n"," 8,\n"," 2,\n"," 0,\n"," 3,\n"," 0,\n"," 9,\n"," 2,\n"," 1,\n"," 1,\n"," 3,\n"," 0,\n"," 5,\n"," 0,\n"," 0,\n"," 7,\n"," 5,\n"," 6,\n"," 2,\n"," 0,\n"," 3,\n"," 8,\n"," 1,\n"," 6,\n"," 5,\n"," 4,\n"," 1,\n"," 1,\n"," 4,\n"," 6,\n"," 5,\n"," 3,\n"," 6,\n"," 0,\n"," 4,\n"," 8,\n"," 2,\n"," 4,\n"," 2,\n"," 5,\n"," 1,\n"," 7,\n"," 6,\n"," 9,\n"," 1,\n"," 7,\n"," 3,\n"," 8,\n"," 0,\n"," 8,\n"," 8,\n"," 4,\n"," 5,\n"," 3,\n"," 6,\n"," 6,\n"," 6,\n"," 0,\n"," 3,\n"," 5,\n"," 1,\n"," 7,\n"," 1,\n"," 6,\n"," 2,\n"," 8,\n"," 5,\n"," 6,\n"," 4,\n"," 7,\n"," 4,\n"," 3,\n"," 3,\n"," 2,\n"," 4,\n"," 7,\n"," 0,\n"," 0,\n"," 9,\n"," 8,\n"," 5,\n"," 9,\n"," 4,\n"," 0,\n"," 8,\n"," 8,\n"," 3,\n"," 6,\n"," 2,\n"," 6,\n"," 1,\n"," 8,\n"," 6,\n"," 1,\n"," 4,\n"," 7,\n"," 7,\n"," 8,\n"," 3,\n"," 0,\n"," 9,\n"," 9,\n"," 6,\n"," 7,\n"," 7,\n"," 4,\n"," 4,\n"," 1,\n"," 8,\n"," 4,\n"," 8,\n"," 0,\n"," 2,\n"," 8,\n"," 2,\n"," 4,\n"," 3,\n"," 3,\n"," 7,\n"," 2,\n"," 3,\n"," 4,\n"," 0,\n"," 4,\n"," 8,\n"," 1,\n"," 3,\n"," 5,\n"," 6,\n"," 3,\n"," 9,\n"," 4,\n"," 3,\n"," 8,\n"," 7,\n"," 7,\n"," 2,\n"," 6,\n"," 0,\n"," 6,\n"," 9,\n"," 8,\n"," 8,\n"," 1,\n"," 3,\n"," 4,\n"," 6,\n"," 9,\n"," 9,\n"," 2,\n"," 6,\n"," 0,\n"," 1,\n"," 8,\n"," 4,\n"," 3,\n"," 9,\n"," 8,\n"," 8,\n"," 4,\n"," 0,\n"," 5,\n"," 0,\n"," 6,\n"," 0,\n"," 4,\n"," 4,\n"," 6,\n"," 5,\n"," 1,\n"," 8,\n"," 1,\n"," 5,\n"," 3,\n"," 6,\n"," 2,\n"," 3,\n"," 7,\n"," 8,\n"," 9,\n"," 3,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 6,\n"," 4,\n"," 7,\n"," 5,\n"," 7,\n"," 1,\n"," 3,\n"," 2,\n"," 7,\n"," 7,\n"," 1,\n"," 5,\n"," 1,\n"," 5,\n"," 4,\n"," 4,\n"," 3,\n"," 4,\n"," 3,\n"," 9,\n"," 0,\n"," 7,\n"," 8,\n"," 6,\n"," 4,\n"," 9,\n"," 4,\n"," 4,\n"," 1,\n"," 4,\n"," 7,\n"," 1,\n"," 1,\n"," 8,\n"," 7,\n"," 0,\n"," 4,\n"," 0,\n"," 4,\n"," 0,\n"," 0,\n"," 5,\n"," 1,\n"," 8,\n"," 6,\n"," 5,\n"," 0,\n"," 1,\n"," 5,\n"," 3,\n"," 4,\n"," 6,\n"," 3,\n"," 1,\n"," 1,\n"," 6,\n"," 9,\n"," 8,\n"," 3,\n"," 5,\n"," 5,\n"," 4,\n"," 8,\n"," 8,\n"," 5,\n"," 0,\n"," 4,\n"," 0,\n"," 4,\n"," 3,\n"," 1,\n"," 6,\n"," 9,\n"," 9,\n"," 1,\n"," 1,\n"," 3,\n"," 3,\n"," 1,\n"," 4,\n"," 9,\n"," 6,\n"," 9,\n"," 1,\n"," 5,\n"," 4,\n"," 2,\n"," 3,\n"," 2,\n"," 4,\n"," 0,\n"," 9,\n"," 7,\n"," 4,\n"," 3,\n"," 0,\n"," 5,\n"," 0,\n"," 1,\n"," 9,\n"," 0,\n"," 4,\n"," 5,\n"," 2,\n"," 8,\n"," 0,\n"," 5,\n"," 9,\n"," 3,\n"," 9,\n"," 6,\n"," 1,\n"," 5,\n"," 5,\n"," 1,\n"," 9,\n"," 0,\n"," 8,\n"," 4,\n"," 6,\n"," 7,\n"," 2,\n"," 8,\n"," 5,\n"," 8,\n"," 9,\n"," 7,\n"," 7,\n"," 2,\n"," 8,\n"," 1,\n"," 3,\n"," 4,\n"," 5,\n"," 0,\n"," 4,\n"," 1,\n"," 4,\n"," 2,\n"," 3,\n"," 6,\n"," 9,\n"," 2,\n"," 3,\n"," 4,\n"," 5,\n"," 4,\n"," 2,\n"," 3,\n"," 3,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 9,\n"," 1,\n"," 1,\n"," 2,\n"," 7,\n"," 1,\n"," 5,\n"," 4,\n"," 9,\n"," 1,\n"," 7,\n"," 6,\n"," 0,\n"," 4,\n"," 2,\n"," 9,\n"," 4,\n"," 1,\n"," 1,\n"," 5,\n"," 3,\n"," 5,\n"," 7,\n"," 4,\n"," 7,\n"," 8,\n"," 3,\n"," 2,\n"," 7,\n"," 2,\n"," 0,\n"," 4,\n"," 7,\n"," 1,\n"," 6,\n"," 4,\n"," 6,\n"," 1,\n"," 5,\n"," 7,\n"," 3,\n"," 5,\n"," 9,\n"," 4,\n"," 7,\n"," 9,\n"," 6,\n"," 6,\n"," 3,\n"," 3,\n"," 2,\n"," 1,\n"," 4,\n"," 5,\n"," 3,\n"," 7,\n"," 7,\n"," 9,\n"," 5,\n"," 6,\n"," 3,\n"," 6,\n"," 1,\n"," 0,\n"," 9,\n"," 3,\n"," 2,\n"," 9,\n"," 2,\n"," 6,\n"," 7,\n"," 5,\n"," 2,\n"," 3,\n"," 2,\n"," 8,\n"," 3,\n"," 0,\n"," 2,\n"," 7,\n"," 9,\n"," 4,\n"," 0,\n"," 9,\n"," 5,\n"," 1,\n"," 8,\n"," 8,\n"," 5,\n"," 3,\n"," 2,\n"," 9,\n"," 6,\n"," 7,\n"," 0,\n"," 8,\n"," 0,\n"," 7,\n"," 4,\n"," 5,\n"," 8,\n"," 7,\n"," 9,\n"," 7,\n"," 7,\n"," 0,\n"," 5,\n"," 3,\n"," 2,\n"," 1,\n"," 9,\n"," 0,\n"," 6,\n"," 8,\n"," 3,\n"," 6,\n"," 2,\n"," 2,\n"," 9,\n"," ...]"]},"metadata":{},"execution_count":163}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","def train(model, optimizer, train_dataloader, device, loss_function):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_train = []  # Store true labels\n","    predicted_labels_train = []  # Store predicted labels\n","\n","    for images, labels in train_dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","        model.zero_grad()\n","        predictions = model(images)\n","        loss = loss_function(predictions, labels)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        _, predicted_labels = torch.max(predictions, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        true_labels_train.extend(labels.cpu().numpy())  # Add true labels to list\n","        predicted_labels_train.extend(predicted_labels.cpu().numpy())  # Add predicted labels to list\n","\n","    avg_loss = total_loss / len(train_dataloader)\n","    accuracy = correct_predictions / len(train_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_train, predicted_labels_train\n","\n","def evaluate(model, val_dataloader, device, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    true_labels_eval = []  # Store true labels\n","    predicted_labels_eval = []  # Store predicted labels\n","\n","    with torch.no_grad():\n","        for images, labels in val_dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            predictions = model(images)\n","            loss = loss_function(predictions, labels)\n","            total_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted_labels = torch.max(predictions, 1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            true_labels_eval.extend(labels.cpu().numpy())  # Add true labels to list\n","            predicted_labels_eval.extend(predicted_labels.cpu().numpy())  # Add predicted labels to list\n","\n","    avg_loss = total_loss / len(val_dataloader)\n","    accuracy = correct_predictions / len(val_dataloader.dataset)\n","\n","    return avg_loss, accuracy, true_labels_eval, predicted_labels_eval\n","\n","best_state_dict = None\n","best_params = None\n","best_valid_metric = 0\n","def objective(trial, dfTrain, device, layers=3):\n","    global best_state_dict, best_params,best_valid_metric\n","\n","    model_params = {\n","    \"lr\": [3e-5, 1e-2],\n","    \"weight_decay\": [1e-6, 1e-2],\n","    \"batch_size\": [40, 50],\n","    \"epochs\": [5,10],\n","    \"beta1\": [0.8, 0.99],\n","    \"beta2\": [0.9, 0.9999],\n","    \"factor\": [0.05, 0.5],\n","    \"scheduler_patience\": [1, 2],\n","    \"threshold\": [1e-5, 1e-3],\n","    \"cooldown\": [0, 1],\n","    \"min_lr\": [0, 1e-3],\n","    \"dropout\": [0.1, 0.5],  # Only one definition of \"dropout\" is needed\n","    \"activation\": ['leaky_relu', 'sigmoid', 'tanh', 'gelu'],\n","    'test_size':[0.15,0.45],\n","    'random_state':[42,1024],\n","    }\n","    params = {}\n","    for param, bounds in model_params.items():\n","        if isinstance(bounds[0], int):\n","            params[param] = trial.suggest_int(param, min(bounds), max(bounds))\n","        elif isinstance(bounds[0], bool) or isinstance(bounds[0], str): # Handle strings\n","            params[param] = trial.suggest_categorical(param, bounds)\n","        elif isinstance(bounds[0], float):\n","            params[param] = trial.suggest_float(param, min(bounds), max(bounds), log=True)\n","\n","    for param, value in params.items():\n","        print(f'{param} = {value}')\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    train_data, val_data = train_test_split(dfTrain, test_size=params[\"test_size\"], random_state=params[\"random_state\"])\n","    y_train,y_val=train_test_split(dfTrain['label'], test_size=params[\"test_size\"], random_state=params[\"random_state\"])\n","    train_dataloader = create_dataloader(train_data, batch_size=params[\"batch_size\"])\n","    val_dataloader = create_dataloader(val_data, batch_size=params[\"batch_size\"])  # Using the same function for validation data\n","\n","    model = CNN_Arch( activation=params[\"activation\"],dropout=params[\"dropout\"])\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], params[\"beta2\"]), weight_decay=params[\"weight_decay\"])\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=params[\"factor\"], patience=params[\"scheduler_patience\"], verbose=False, threshold=params[\"threshold\"], threshold_mode='rel', cooldown=params[\"cooldown\"], min_lr=params[\"min_lr\"])\n","\n","\n","    train_labels, val_labels = train_test_split(dfTrain['label'], test_size=params[\"test_size\"],random_state=params[\"random_state\"]+random.randint(0,300),stratify=dfTrain['target'])\n","    #all_labels = np.concatenate([train_labels, val_labels])\n","    all_labels=train_labels\n","    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(all_labels), y=all_labels)\n","    weights = torch.tensor(class_weights, dtype=torch.float)\n","    weights = weights.to(device)\n","\n","\n","\n","\n","    best_local_metric = 12312310\n","    valid_losses = []\n","    no_improvement_counter = 0\n","    for epoch in range(1000000):\n","        train_loss, train_accuracy, true_labels_train, predicted_labels_train = train(model, optimizer, train_dataloader, device, loss_function)\n","        valid_loss, val_accuracy, true_labels_eval, predicted_labels_eval = evaluate(model, val_dataloader, device, loss_function)\n","        print('-' * 100); print(f'| Epoch {epoch + 1} | Loss Train/Valid: {train_loss:.6f} / {valid_loss:.6f} |', end=''); print(f' Accuracy Train/Valid: {100 * train_accuracy:.3f}% / {100 * val_accuracy:.4f}% |'); print('-' * 100);\n","        metric = valid_loss\n","        if metric < best_local_metric:\n","            best_local_metric = metric\n","            no_improvement_counter = 0\n","            if metric < best_valid_metric:\n","                best_valid_metric = metric\n","                best_state_dict = copy.deepcopy(model.state_dict())\n","                best_params=params\n","                print();print('~'*26);print(f'|Best Value Found {best_local_metric:.5f}|');print('~'*26); print(f'Params = {best_params}'); print('~' * 54)\n","                print(\"Training Classification Report:\"); print(classification_report(true_labels_train, predicted_labels_train))\n","                print(\"Validation Classification Report:\"); print(classification_report(true_labels_eval, predicted_labels_eval))\n","                print('~' * 54); print()\n","        else:\n","            no_improvement_counter += 1\n","            if no_improvement_counter >= 2:\n","                print(); print('<'*50); print('OH NO WE BE OVERFITTING'); print('<'*50); print()\n","                raise optuna.TrialPruned()\n","    return metric\n","\n","\n","\n","def tune_model_with_optuna(dfTrain, device, layers=3):\n","    global best_state_dict, best_params\n","\n","    study = optuna.create_study(direction=\"minimize\")  # Maximize the accuracy\n","    study.optimize(lambda trial: objective(trial, dfTrain, device, layers=layers), n_trials=5)\n","    trial = study.best_trial\n","\n","    if trial.state == optuna.trial.TrialState.COMPLETE:\n","        print(f'Best trial found with value: {trial.value:.3f}')\n","        print('Best parameters:')\n","        for key, value in trial.params.items():\n","            print(f'    {key}: {value}')\n","    else:\n","        print('No completed trials.')\n","\n","    # Load the best model using the best state dictionary\n","    best_model = CNN_Arch(\n","        dropout=best_params[\"dropout\"] ,\n","        activation=best_params[\"activation\"],# Specify other parameters as needed\n","    )\n","\n","    best_model.load_state_dict(best_state_dict)\n","    best_model = best_model.to(device)\n","\n","    return best_model\n","\n"],"metadata":{"id":"cbsCfqYXrsUt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X02typ5gVSqD"},"execution_count":null,"outputs":[]}]}